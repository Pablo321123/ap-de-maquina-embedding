{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta prática iremos apresentar o uso de embeddings. Para isso, você deve primeiro instalar as dependencias usando `pip install -r requirements.txt` (ou `pip3`, dependendo da forma que seu python está instalado)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente, você deverá baixar os repositórios em português e inglês e salvá-los na pasta `embedding_data` seguindo as seguintes instruções: \n",
    "\n",
    "- [No respositório da USP](http://www.nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc) baixe [este arquivo (Glove 100 dimensões)](http://143.107.183.175:22980/download.php?file=embeddings/glove/glove_s100.zip). Ele possui  um pouco mais de 600 mil palavras retiradas de textos de páginas Web tais como a Wikipedia e canais de notícias [(Hartmann et al., 2017)](https://arxiv.org/abs/1708.06025). Descomprima e renomeie o arquivo txt para `glove.pt.100.txt`.\n",
    "\n",
    "- No [repositório de Stanford](https://nlp.stanford.edu/projects/glove/), baixe [este arquivo](http://nlp.stanford.edu/data/glove.6B.zip) use o arquivo . Este arquivo compreende ~400 mil palavras de textos extraidos da Wikipédia e [GigaWord](https://catalog.ldc.upenn.edu/LDC2011T07) [(Pennington et al., 2015)](https://nlp.stanford.edu/pubs/glove.pdf). Descomprima e salve o arquivo com embeddings de 100 dimensões (nome `glove.6B.100d.txt`) na pasta `embedding_data` renomeando esse arquivo para `glove.en.100.txt`.\n",
    "\n",
    "Como você pode perceber, esta prática demandará um espaço livre em disco de aproximadamente 3GB. Os arquivos estão no seguinte formato: em cada linha, uma palavra e N valores representando o valor em cada uma das N dimensões do embedding desta palavra. Por exemplo, caso as palavras `casa`, `redondel` e `rei` sejam representadas por um embedding de 4 dimensões, uma possível representação seria:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "casa 0.12 0.1 0.5 -0.4\n",
    "redondel 0.2 0.1 -0.4 0.5\n",
    "rei 0.1 0.5 -0.1 0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função `get_embedding`, do arquivo `embeddings/utils.py` é responsável por ler esse arquivo e gerar um dicionário em que a chave é a palavra e o valor é sua representação por meio de embeddings. Para a  representação acima, a saída desta função seria seria: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dict_embedding_ex = {\n",
    "                        \"casa\":np.array([0.12,0.1,0.5,-0.4]),\n",
    "                        \"redondel\":np.array([0.2,0.1,-0.4,0.5]),\n",
    "                        \"rei\":np.array([0.1,0.5,-0.1,0.1]),\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa função, também é salvo o objeto criado usando [pickle](https://docs.python.org/3/library/pickle.html), assim, a próxima vez que seja lido o embedding, a leitura será mais rápida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 1 - obtenção do embedding**: Complete a função `get_embedding` obtendo a palavra e o vetor de embeddings com a dimensão `embeddings_size` substituindo os `None` apropriadamente. O dataset possui algumas incosistencias que você deve considerar ao modificar essas linhas: no dataset em português, a maioria das palavras compostas são separadas por hífen, porém, foi verificado que umas palavras foi separado por espaço. Por caso disso, você deve considerar que as `embeddings_size` últimas posições são os valores de cada dimensão, separados por espaço e, as demais, são a palavra. Sugiro \"brincar\" abaixo com o uso de [índice negativo](https://www.geeksforgeeks.org/python-negative-index-of-element-in-list/) entenda também o [método join](https://www.geeksforgeeks.org/join-function-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'pé de moleque 0.1 -0.5 0.5 0.1 -0.5': [ 0.1 -0.5  0.5  0.1 -0.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "linha = \"pé de moleque 0.1 -0.5 0.5 0.1 -0.5\"\n",
    "embedding_size = 5\n",
    "arr_line = linha.strip().split()\n",
    "\n",
    "word = \" \".join(arr_line[:])\n",
    "\n",
    "# colocamos float16 para economizar memória\n",
    "embedding = np.array(arr_line[3:], dtype=np.float16)\n",
    "print(f\"'{word}': {embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute o teste unitário abaixo para verificar o funcionamento do `get_embeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: rei\n",
      "Palavras ignoradas: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.011s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m embeddings.embedding_tests TestEmbeddings.test_get_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute os embeddings em português e ingles. Não se preocupe com as palavras ignoradas: foram algumas inconsistências no dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028\n",
      "10000: persecution 0.79327 0.36709 -0.48275 -0.32871 -0.064043 1.3279 -0.86922 -0.91222 0.22627 0.33839 -0.33918 0.46198 0.4819 0.18628 0.13482 0.05749 -0.31759 -0.45847 -1.275 0.23817 0.34383 0.24047 0.16118 0.8461 -0.020052 -0.14709 0.67272 -0.046894 0.42801 -0.64108 -0.058007 -0.34103 -0.16484 0.15837 -0.23235 -0.23329 0.89868 -0.17708 -0.2795 -0.43088 -1.9371 0.41807 0.19409 0.0056234 0.29662 -1.1352 1.2659 0.0045242 0.47534 -0.1274 0.4673 -0.032414 -0.45122 0.81415 -0.08585 -0.1864 0.48751 -0.60543 0.74305 -0.5143 -0.35297 -0.19083 0.0075049 -0.7089 1.1692 -0.42422 0.03897 -0.61022 -0.12374 0.018167 -0.3486 -0.44683 -0.28754 0.30437 0.15596 -0.025862 -0.61326 0.0054372 -0.80021 0.8991 0.41528 0.40377 -0.021326 1.0061 -1.137 0.20465 -0.75233 -0.25128 0.04837 -0.50917 -0.23438 -0.51117 0.84584 0.62746 0.93226 -1.1124 -0.0020958\n",
      "20000: baths 0.19361 0.77835 -0.63637 0.4011 -0.80984 0.60587 -0.049379 0.61942 -0.35005 -0.73075 0.29206 -0.13326 0.14234 1.4927 0.038686 -0.51051 -0.56125 -0.26137 -0.81463 -0.55346 -0.41485 0.55106 -0.77326 0.22891 -0.71337 -0.29861 0.22984 0.54849 -0.63144 -0.24795 0.13418 -0.19278 -0.48616 0.41281 0.23853 0.98553 -0.045648 0.24489 -0.2661 -0.43768 0.64638 -0.24739 -0.63798 -0.84792 0.88075 0.75128 -0.076646 -0.064563 1.1866 -0.082361 -0.72173 -0.39497 0.1744 0.42181 -0.77287 -1.2962 0.43995 -0.36313 -0.10083 0.08113 -0.17672 0.33276 0.85662 0.98821 0.047326 0.81553 0.46147 -1.4436 -0.33294 -1.3258 -0.39753 0.071827 0.94192 0.38201 -0.44131 -0.0096991 0.62609 0.23091 -0.26173 -0.11505 0.035679 -0.13504 0.73291 0.90446 0.53031 0.74786 -0.087916 0.085088 0.4729 -0.38313 0.30923 -0.80738 0.41327 -0.36631 -0.095886 -0.30429 -0.28829\n",
      "30000: mortally -0.44706 -0.14309 -0.89121 0.23156 -0.38413 1.0456 0.56096 -0.2727 0.55126 0.017513 0.35854 1.0084 0.40966 -1.003 0.56211 -0.26906 -0.19743 -0.80396 -0.86625 0.41227 0.69954 -0.77319 0.42462 0.047808 0.53844 0.72136 -0.79553 0.11251 0.89835 0.45541 0.71869 0.31343 -0.61877 -0.59697 -0.12446 -0.17067 -1.0913 -0.017694 0.68361 1.1964 0.35526 0.055263 0.44015 -0.47002 0.39336 0.39839 -0.37402 0.98916 -0.6981 -0.21064 -0.44319 -0.76443 -0.31606 0.82774 -0.3748 0.028216 0.43854 -1.0261 -0.56773 0.4714 0.24924 0.78989 0.29339 0.63187 -0.7875 -0.98934 0.27836 0.16201 0.11845 0.71556 -0.22276 -0.9741 0.41906 0.7765 0.45019 0.0076196 -0.12958 0.5277 -0.090841 -0.49497 -0.14577 0.55818 -0.089367 0.32771 -0.55268 -0.70972 -0.80992 0.20449 -0.31109 0.19595 -0.22711 -0.085183 0.19958 0.75822 -0.18851 0.33264 0.40005\n",
      "40000: 1667 0.39444 -0.6746 -0.27264 0.39532 -0.38024 0.1509 0.71751 -0.70989 0.21976 -0.63909 0.25419 -0.27753 -0.18392 0.24652 0.80551 -0.15211 -0.12273 -0.36276 -0.062178 -0.16213 -0.48331 -0.48618 0.7814 -0.046758 0.57997 -0.71189 -0.53564 -0.0069881 0.24182 -0.28956 -0.42493 -0.50768 0.19585 -0.30965 -0.93979 0.077654 -0.01848 -0.063651 0.31305 0.034993 0.318 -0.14884 0.47067 -0.72759 0.1371 -0.74743 0.36341 -0.85247 -0.0089813 0.27025 -0.49552 0.4409 0.40908 0.14149 -0.89864 -0.11142 0.2169 -0.95417 -1.0356 -0.42705 -0.053882 0.39926 -0.27163 0.078868 0.031792 -0.28377 -0.8245 -0.20276 0.065733 -0.52967 0.4006 0.042663 0.99184 0.023222 -0.63024 -0.81826 0.43098 0.54075 -0.76779 -0.23405 -0.24934 -0.65918 0.099703 0.7619 0.46974 0.11807 0.69707 -0.8289 -0.49636 0.32239 0.086129 -0.012369 0.57565 0.63727 -0.1158 0.4583 -0.10951\n",
      "50000: bec 0.17865 -0.69992 0.34185 0.13009 -0.25456 0.07195 0.17694 0.37281 0.76649 -0.39858 0.55652 0.17582 -1.2284 0.62613 -0.52843 -0.46453 -0.047966 -0.40044 0.77988 0.31123 -0.10865 -0.78296 0.28396 0.23278 0.60613 -0.51023 0.61492 0.038553 -0.53071 0.35207 -0.87084 -0.38816 0.14315 1.1361 -0.3666 0.65744 0.045711 1.0979 -0.0025841 -0.68796 -0.063621 0.24938 0.24533 -0.61844 0.19504 0.87456 -0.46225 0.084439 0.49152 0.30219 -0.60536 0.20837 -0.89575 -0.16517 -0.40801 0.10569 0.00031951 0.11742 -0.88028 -0.2539 0.16214 -0.50744 -0.045147 0.37566 -0.37146 0.74913 -0.099645 0.11784 0.4224 -0.44371 0.044811 -0.0053917 -0.20023 -0.32234 -0.64823 -0.33831 -0.29074 -0.84457 -0.53169 -0.48206 -0.081932 -0.50102 -0.15976 0.55565 -0.17511 0.51657 -0.27359 0.58922 -0.36195 0.92312 -0.36282 -0.46019 -0.16794 0.51888 0.53584 0.022216 -0.59242\n",
      "60000: baek -0.27535 0.24531 0.36129 0.17927 -0.58862 0.018252 0.37768 -0.22595 -0.016943 0.2865 0.11962 1.1704 0.32672 0.4426 0.85163 0.23344 0.13275 -0.20693 -0.8699 -0.58189 0.21569 -0.61754 -0.038573 0.17747 -1.0619 0.5844 -0.65437 0.61203 -0.47917 -0.49082 -0.56953 -0.14873 0.33061 1.0082 0.51675 -0.52833 -0.048025 0.36062 -0.61679 -0.71534 -0.29747 0.40103 -0.62595 0.33876 0.65654 -0.034211 0.29326 -0.53764 -0.57305 0.44636 -0.097445 -0.54014 0.55607 -0.53241 0.18672 0.24677 -0.06459 -0.2927 -0.39487 -0.090398 0.067719 -0.37376 0.28401 0.11183 -0.8733 0.59041 0.77796 0.52677 -0.33984 0.98705 0.22309 0.40846 -0.43786 -1.1593 -0.32952 0.10423 -0.64864 0.20029 0.40687 -0.25694 0.23251 -0.2881 0.22227 0.16766 -0.089702 0.24961 -0.29282 0.28995 0.57015 -0.061305 -0.22759 0.5931 0.4115 -0.39097 0.24253 -0.15244 -0.029267\n",
      "70000: b/w -0.081021 -0.24374 0.16524 -0.12923 -0.20033 0.45365 0.019782 0.11297 0.35419 -0.08906 0.56429 0.11645 0.054524 0.35766 0.19253 0.55838 0.053474 0.5975 0.19721 0.056773 0.62213 0.0057346 -0.69742 -0.30561 -0.019735 0.82005 0.27002 -0.20663 0.42973 0.43662 0.19805 0.014384 0.66599 0.36858 -0.19809 0.02191 -0.11875 0.1981 1.0467 -0.6322 0.58067 0.31569 -0.45414 0.52827 0.026791 -0.66439 -0.1054 -0.045638 -0.43459 0.064405 -0.12456 0.47467 0.34123 0.35278 -0.12608 0.16045 0.6016 0.45386 -0.19314 -0.26507 -0.31031 0.37903 -1.1166 -0.21237 0.34549 0.80085 0.10614 -0.56278 -0.14649 -0.38959 -0.06917 0.39566 0.46095 0.018064 0.28011 0.6976 -0.12585 0.17175 0.14175 -0.65689 -0.8933 -0.3136 0.066392 0.51353 0.059439 0.16289 0.23694 -0.66059 0.033279 0.32525 -0.56483 -0.056862 0.09462 -0.071204 0.3708 -0.03993 0.077736\n",
      "80000: klinghoffer 0.93592 -0.83256 0.30664 -0.51925 0.046467 0.32427 1.08 -0.1225 0.45542 -0.44381 -1.0243 0.40211 0.16575 0.35558 0.90129 0.7251 -0.016332 0.32883 -0.12312 0.014342 0.056407 0.43961 0.93385 0.57257 0.65552 0.69214 -0.58995 -1.2333 0.85075 0.52174 0.18521 0.66849 -0.039912 1.2872 -0.87095 0.15615 0.99704 0.46977 0.2092 0.35262 -0.90761 -0.12454 -0.12348 0.3742 0.78428 0.32948 -0.72969 -0.049783 -0.72192 0.43929 -0.88021 0.23244 -0.48265 0.37422 -0.37394 0.78228 0.56372 0.60393 -0.38157 0.42083 0.17501 -0.030493 0.99422 0.1283 -0.42494 -0.45316 -0.060246 -0.78236 -1.3716 -0.35105 -0.71617 -1.0175 0.22968 1.1878 0.19198 0.99946 0.87781 0.15911 0.019981 -0.37608 -0.42802 0.72478 0.8679 -0.060474 1.0468 -0.71259 -0.22719 -0.7949 -0.79956 -0.92237 0.64 -0.24044 0.38982 -0.76124 0.095311 -0.10848 -0.26379\n",
      "90000: azarov -0.84779 -0.81158 0.31192 0.8444 0.37085 -0.86742 -0.71074 -0.39262 0.6561 0.71714 -0.44991 0.13117 0.15397 0.34202 0.7948 -0.4006 -0.34574 -0.27266 0.50279 -0.11107 -0.56629 -0.44139 -0.068648 0.41496 -0.70783 0.036723 0.3605 0.87929 0.43616 0.63794 -0.014098 0.52091 0.32128 -0.19314 -0.19829 0.38367 -0.16686 0.12444 -0.29447 -0.79226 0.48984 1.0102 0.54931 0.045012 0.35113 -0.39933 -0.20285 -0.52905 0.027485 0.64211 -1.0283 -0.2317 0.76791 0.53849 -1.003 0.55818 0.32899 0.27884 -0.81686 0.86433 -0.28436 -0.43628 0.08832 0.40966 0.34761 0.6756 -0.2764 -0.43146 -0.32233 -0.63641 0.19826 0.10337 0.4297 0.055899 -0.20341 0.27978 -0.67 0.63719 -0.29731 -0.55312 -0.020314 -0.16197 -0.25664 -1.1987 0.061021 -0.25289 -0.43387 0.80729 -0.028647 -0.048138 0.93256 -0.24103 -1.0984 -0.021581 -0.19767 -0.16438 0.34961\n",
      "100000: capron 0.25789 0.21751 -0.25583 -0.093133 -0.40987 -0.1333 0.26102 -0.17168 -0.21675 -0.49027 -0.086235 0.78573 -0.78807 0.14047 0.1713 0.39518 0.14244 0.35465 -0.3075 -0.47633 0.22469 0.20186 0.42005 -0.4107 0.31823 -0.12058 -0.30217 0.34309 -0.094585 0.83448 -0.2694 -0.066043 -0.37557 0.15318 0.083103 -0.18062 0.40928 0.12017 0.38465 -0.30535 0.055415 0.24156 -0.36549 0.14358 0.031094 0.1598 0.63389 -0.35917 0.0026699 0.37569 0.15659 -0.64842 0.45747 -0.8328 0.065468 0.44769 -0.15419 -0.72917 -1.3181 -0.39947 0.54467 -0.65713 0.22812 0.43905 -0.043266 -0.5093 0.019246 0.00061177 0.02387 0.50717 -0.32615 0.61067 0.40208 -0.21907 -0.11038 0.11072 0.16268 0.13132 -0.052999 0.68595 0.3393 -0.17538 0.54534 -0.67313 0.62824 -0.021203 -0.3295 -0.14773 -0.45971 0.10368 -0.21519 -0.17429 0.11772 0.29828 0.12491 -0.51382 0.21954\n",
      "110000: perpetua 0.08593 -0.74345 -0.74758 -0.41369 0.12876 0.74621 0.22945 0.23218 -0.11248 -0.094056 -0.15041 0.84485 -0.7237 0.15459 0.19086 -0.56527 0.15813 -0.24205 0.13934 0.44543 -0.52201 -0.50294 -0.81421 -0.21743 0.64909 0.69812 0.43802 0.070801 -0.13523 -0.47338 -0.13752 -0.53631 -0.27563 0.080151 0.5055 -0.60856 0.44252 0.27134 0.42643 -0.07752 -0.068353 0.5373 -0.35526 1.2618 -0.34595 -0.21431 -0.068782 0.31514 0.39092 0.57879 -0.14439 0.69687 -0.69424 -0.10267 -0.14025 0.22605 -0.041908 0.19514 -0.72758 -0.57083 -0.50138 -0.66429 -0.53427 0.29919 -0.18928 0.24398 -0.039063 -0.60909 0.3958 -0.23234 -0.21996 0.41779 0.43276 -0.53833 0.18852 -0.88696 0.6558 -0.27145 -0.2688 0.15222 -0.16741 -0.10156 0.25411 -0.19026 1.176 -0.78545 0.3345 -0.061179 0.18448 0.0060414 0.071122 -0.28132 0.22906 -0.044027 -0.11052 -0.19164 0.17654\n",
      "120000: biratnagar -0.31704 -0.25 0.39288 0.1766 -0.031244 0.46051 -0.99225 0.73277 0.27843 0.19924 -0.59865 -0.29429 0.06398 -0.017308 -0.10676 -0.24878 0.25086 -0.89783 0.66488 -0.19437 0.51668 0.61698 -0.38708 0.74033 -0.31574 -0.44352 0.75221 0.035775 0.12503 0.041681 -0.69479 -0.51008 0.27242 0.17792 -0.38468 0.18828 -0.12949 0.026388 -0.26186 -0.37123 0.099944 0.0062021 0.37243 -0.50242 -0.77188 0.41125 1.2488 -0.079565 0.62152 0.28608 -0.51326 0.36958 -0.39022 0.033359 -0.23492 1.2317 -0.86333 -0.53414 -0.37116 -0.321 -0.068214 0.073897 -0.25373 0.089365 -0.6871 -0.16667 0.024601 -0.3481 -0.27241 -0.0047164 -0.53037 -0.07671 -0.14335 0.10766 -0.05256 0.40831 0.38218 0.08151 0.23604 0.54997 -0.46647 0.54634 -0.49369 -0.32489 0.67775 1.0147 -0.31165 0.32195 -0.27775 0.086096 0.99659 0.021062 0.073932 0.39693 -0.13036 -0.031886 0.14625\n",
      "130000: 12.74 0.81429 0.35959 0.021583 0.41194 -0.086166 0.21861 -0.083905 -0.19764 0.52731 0.45393 0.49963 0.20048 -0.90781 -0.47227 -0.13635 -0.047514 -0.26844 -0.13909 0.045127 -0.10716 0.42083 0.18198 0.14461 -0.055848 0.48382 -0.19337 0.54453 0.4434 0.44726 -0.48448 -0.040326 -0.41646 0.52639 -0.31737 -0.45518 0.22377 0.14061 -0.33417 -0.060146 0.75466 0.12336 0.0095106 -0.25673 0.16652 -0.5513 0.34892 -0.087335 0.46037 -0.35848 -0.8468 0.083813 -0.019716 0.47783 -0.44977 0.19079 0.41083 0.29553 -0.044898 -0.10182 -0.21201 -0.17757 -0.43383 0.17693 0.30094 -1.1161 0.044527 0.10093 0.035273 0.38188 0.25591 -0.32183 0.080572 -0.18975 0.063349 -0.26482 -0.049469 -0.31227 -0.25384 0.59683 0.23449 -0.84462 0.12052 -0.052911 0.31087 0.41472 0.64994 0.40985 0.713 0.34881 0.45153 -0.32228 0.11685 0.57675 -0.43894 -0.11849 0.29834 -0.034284\n",
      "140000: yaffa -0.80148 -0.5237 -0.69207 0.11659 -0.09254 -0.68151 -0.53433 -0.37007 0.73419 0.80028 -0.22135 -0.31862 -0.81663 0.13145 0.6164 0.32362 -0.54966 -0.36047 0.51811 -0.36199 -0.37585 1.0997 -0.13512 -0.24785 -0.30338 0.50663 0.099961 -0.71448 0.0089579 0.63482 -0.55671 -0.52852 0.55632 0.33298 -0.16187 -0.383 0.16287 0.10435 -0.34098 -0.22521 0.12443 0.11484 -0.57652 1.1325 0.39744 -0.059465 -0.77863 0.67869 0.17955 -0.67999 0.094384 -0.048083 -0.087005 -0.28933 -0.019391 0.49125 -0.65289 0.2704 -0.68568 0.7534 -0.21989 -0.28923 0.78065 -0.18514 0.089915 0.22317 0.27453 0.4743 -0.32424 -0.18985 0.38547 -0.19483 0.059571 -0.25796 -0.18385 0.70594 0.1456 -0.05469 0.079464 -0.01267 -0.56141 -0.36207 0.36564 -0.19723 0.78918 -0.056306 0.30887 0.52079 0.2633 0.37811 0.31585 -0.053145 -0.036721 -0.39142 0.098659 -0.090346 -0.073302\n",
      "150000: cryogenics 0.27179 0.083877 0.21512 0.47804 0.015122 -0.13914 0.66967 0.098341 0.040555 0.18118 -0.33982 -0.087979 -0.59389 -0.45029 -0.14964 0.24803 -0.056612 0.21834 0.35094 -0.38947 -0.81632 0.15912 0.015749 -0.43036 -0.68171 0.15941 -0.15403 0.084428 -0.56304 0.12817 -0.28663 -0.07754 -0.79207 0.083044 -0.12726 0.30251 -0.11196 -0.033942 0.20347 -0.52079 0.15387 0.038931 -0.40063 -0.29857 0.05724 -0.32701 0.22831 0.66416 0.029789 0.28713 0.019023 -0.0079954 -0.066571 -0.21325 -0.039775 0.58264 -0.12688 0.085971 -0.43755 -0.068182 -0.25347 -0.010152 0.19519 0.22659 -0.080673 0.026839 0.21789 -0.46621 -0.11913 0.30445 -0.13788 0.20443 0.45562 0.22398 -0.27323 0.084955 0.038373 -0.14217 0.18743 0.16454 0.098459 0.2619 0.01989 0.03732 0.057054 0.34981 0.34753 0.3201 -0.12133 0.34241 0.3414 0.22305 0.10869 -0.24577 0.63789 0.042749 0.24613\n",
      "160000: ef1 0.23341 -0.33829 0.13232 0.78787 -0.47635 -0.0011824 0.66496 -0.31293 0.38876 0.42675 0.00090531 -0.6205 0.39688 -0.16347 0.62481 0.23682 -0.729 0.012095 0.35781 0.62507 0.40459 -0.33319 -0.29625 0.080255 0.49025 -0.27005 0.096392 0.98197 -0.60669 0.48281 0.73684 -0.23073 -0.14244 1.0493 0.14603 -0.44737 -0.037023 -0.57098 0.35625 1.2719 0.74632 0.55955 -0.7517 -0.18806 0.84908 0.27661 0.61194 0.0079779 -0.0050331 -0.94612 0.1122 -0.27173 1.2246 -0.14545 0.44596 1.0475 0.23805 -0.14957 -0.31622 0.45701 0.20075 0.58711 0.49779 0.37225 -0.089971 -0.66357 -0.27573 0.070081 0.010752 0.27803 0.18427 -0.048578 1.1642 -0.44237 0.58458 0.6669 0.75884 -0.07667 0.22545 -0.60504 0.032923 -0.081218 0.41227 0.025159 0.76301 -0.44372 0.24039 0.77948 0.43176 0.044424 -0.24386 0.29744 -1.1289 -0.34231 0.35818 0.40439 -0.16825\n",
      "170000: franchetti 0.19546 -0.6544 -0.37343 0.50862 -0.036893 0.25539 -0.13704 -0.61698 -0.20434 -0.001251 0.021527 0.1306 -0.1216 -0.57869 0.039425 0.53673 0.10318 0.30764 -0.012106 0.2711 -0.265 -0.45464 0.08067 0.13009 -0.35699 -0.034601 0.20728 0.45166 0.42866 -0.0889 -0.16472 0.26504 -0.38946 0.25654 0.64075 0.11165 0.20292 -0.63376 0.3115 0.58844 0.19186 0.215 -0.15585 0.53515 -0.1176 -0.035542 0.12221 -0.23218 0.18911 0.21745 0.14445 -0.32836 -0.088244 -0.26328 -0.4028 0.61192 -0.31171 0.17876 -0.58305 -0.02739 0.24595 -0.15336 -0.1585 -0.1879 -0.13905 -0.33092 0.042317 -0.73507 0.21035 0.047386 0.32872 0.5141 0.27868 -0.080253 0.43679 0.091954 -0.491 -0.11865 0.1718 0.44057 -0.085888 -0.030209 0.16439 -0.17966 0.41601 -0.16978 -0.36814 -0.070726 -0.45014 0.22868 0.50058 0.11221 0.41001 0.1697 0.17183 0.18326 0.082544\n",
      "180000: blintzes 0.04556 -0.016971 -0.067331 0.028431 0.072395 -0.1953 0.60855 -0.092935 0.39621 -0.50966 -0.074968 0.011838 -0.07211 0.4711 -0.012217 0.38802 0.041819 0.20251 -0.35343 0.35532 -0.46398 0.4548 -0.14503 -0.11646 -0.062951 0.00063022 -0.067617 0.16906 -0.12925 -0.31419 0.28767 0.20121 -0.2518 -0.9207 -0.023039 0.15597 0.11189 -0.61309 0.005494 -0.48486 0.90846 -0.16918 -0.22934 -0.14612 -0.061567 -0.43768 0.7199 0.15764 -0.21855 -0.13106 -0.12845 -0.1656 -0.43896 -0.12421 -0.28421 0.94487 0.72442 0.35241 -0.74275 0.084627 0.37538 -0.044688 0.58287 -0.30451 -0.38876 -0.24965 0.13603 0.25918 -0.21388 -0.88804 0.13332 0.41189 0.35218 0.04243 0.040419 0.14265 -0.032337 -0.14748 0.73958 0.51037 0.16225 0.31375 0.58919 0.02891 0.39664 -0.24476 0.24934 0.069522 0.029075 -0.28625 -0.2812 0.18733 -0.11935 -0.44554 -0.18426 -0.24839 -0.11126\n",
      "190000: birthstones 0.93672 0.14259 -0.47747 -0.14521 0.21637 -0.85152 0.37421 -0.46957 -0.51753 -0.4739 -0.92435 0.029792 0.14399 -0.19658 -0.3445 -0.18007 0.19039 0.10468 0.029722 0.35354 -0.13122 -0.072902 0.76218 -0.73286 0.20897 -0.044275 -0.35928 0.045978 -0.41576 -0.53527 0.16652 -0.59976 0.41028 -0.23341 -0.087413 -0.45861 -0.11026 0.059448 -0.27966 -0.067302 0.10224 0.68195 1.0395 0.76854 0.2531 -0.42481 -0.24358 -0.04643 -0.41331 0.16232 0.35769 -0.26937 -0.066158 -0.27206 0.47766 0.38921 0.82925 -0.76349 -0.13925 0.65078 -0.18525 0.16675 0.5102 -0.91787 0.20373 -0.73488 0.058389 0.15913 -0.77262 0.05817 0.54901 -0.015755 0.16711 0.57786 -0.92483 -0.12989 0.31897 0.97166 -0.27545 -0.44709 -0.64308 0.36122 0.83133 0.026839 0.015841 -0.46571 -0.2732 0.38908 -1.588 0.57203 -0.23875 0.36734 0.2615 -0.90072 0.032048 -0.12274 -0.46059\n",
      "200000: naadam 0.067651 0.45258 -0.061108 0.11451 0.28562 -0.17799 -0.077092 0.6227 -0.37069 -0.64525 -0.81898 -0.066169 0.51924 -0.29064 -0.68168 0.1327 0.12402 -0.051656 0.62165 -0.63272 -0.17144 -0.40623 -0.56162 -0.35693 -0.075551 0.45212 0.16962 0.44751 -0.056488 -0.4203 -0.89346 -0.43361 0.30866 -0.065358 0.36763 0.1116 0.032518 0.059214 -0.32578 -0.39313 0.37028 0.42177 -0.28051 0.40403 -0.40784 -0.15146 0.4209 0.0070703 0.1303 -0.14024 -0.69227 -0.33959 0.00093201 -0.79677 -0.34639 0.4275 0.30266 0.52753 -0.0631 0.34907 -0.60095 0.076366 -0.51455 -0.12812 -0.10618 0.68099 -0.6807 -1.0177 0.32406 0.19649 -0.04767 0.1067 0.50801 0.92492 -0.68506 0.44065 -0.27071 -0.18915 0.03613 -0.085901 -0.48905 -0.23506 -0.23959 -0.19935 0.56551 0.38916 -0.59707 0.23625 0.62235 0.62444 0.010283 0.084386 -0.11383 0.19066 0.33071 -0.30791 0.16487\n",
      "210000: concertation 0.20561 0.033854 -0.21247 0.42397 0.30531 0.32942 0.061176 -0.13238 0.019422 -0.23068 -0.064923 0.11934 0.82861 0.016775 0.23178 0.1742 -0.072716 -0.32252 0.34455 -0.64869 -0.31307 0.28035 -0.30377 -0.82529 -0.10927 0.00071058 0.26377 0.52447 0.076858 -0.3214 -0.4326 -0.82995 0.18807 -0.082672 0.53811 0.084854 0.10848 -0.52562 -0.21861 -0.034082 0.2885 -0.035228 -0.070593 0.13567 -0.058057 -0.18769 -0.48696 0.34858 -0.13759 0.83343 -0.06835 0.0035885 -0.64587 -0.33565 0.056597 -0.29768 0.26185 -0.35049 -0.52455 0.40708 -0.27431 -0.64874 -0.098645 0.21799 -0.30774 0.60798 -0.29275 -0.33832 -0.30692 -0.65513 0.56134 0.71606 -0.028471 -0.38228 0.24709 0.4659 -0.090316 -0.0032138 0.56593 0.45958 -0.96908 0.27076 0.319 0.10297 0.40687 0.50861 -0.24145 0.70433 -0.24962 -0.18934 0.001032 -0.048913 -0.15238 0.12569 0.86523 0.37359 0.64095\n",
      "220000: lesticus -0.052488 0.31827 -0.094453 0.20367 -0.30606 -0.030857 -0.12325 0.10118 0.21865 -0.0816 0.011343 -0.082322 0.023396 -0.075119 0.12817 -0.044331 0.043467 -0.19313 0.11919 0.24068 -0.15674 0.077316 0.014641 -0.134 -0.018699 0.41629 -0.066348 0.18937 -0.3438 0.1358 -0.046974 0.11082 -0.24861 0.080265 0.32723 0.019188 -0.19766 -0.088653 0.23456 0.050306 0.090531 -0.17696 0.29309 -0.096094 0.1409 0.12413 -0.071504 0.24346 -0.22753 0.30087 -0.059623 -0.099433 -0.12235 -0.22498 0.11019 0.20698 0.13733 0.25376 0.041228 -0.036424 0.23872 0.14778 0.048842 0.00023955 0.18386 0.25887 -0.14877 -0.26454 -0.23546 -0.11695 -0.00079969 0.074469 0.066833 0.25224 0.04053 -0.15891 0.20658 0.070364 0.057815 0.40338 -0.044064 0.036281 0.16601 -0.23311 0.18605 -0.24546 0.37673 0.029108 0.44027 0.43764 -0.14429 0.15165 -0.14549 0.031631 0.21436 -0.028576 -0.018651\n",
      "230000: containerboard -0.17579 -0.022457 -0.51813 -0.32371 0.37057 -0.41954 0.1064 0.18239 0.20129 0.41013 0.037098 -0.33482 -0.64571 0.28475 -0.24872 0.075919 0.012652 2.9898e-05 0.90067 -0.47527 -0.22856 0.041328 0.13516 -0.30285 0.23634 0.31174 0.29047 0.1128 -0.7762 -0.22666 0.11734 -0.028045 0.075194 -0.19199 -0.41804 0.39299 0.019185 -0.33317 -0.10573 -0.51236 0.64173 0.0015254 0.0095014 0.42707 -0.30376 0.35719 -0.15325 -0.041839 0.12516 0.14165 0.16145 0.00056862 -0.41296 -0.20097 0.099196 1.0641 -0.29094 -0.12725 0.14843 -0.99135 0.17907 -0.41932 0.17165 0.43727 -0.4274 -0.45706 -0.20623 -0.0058816 -0.22896 -0.74935 0.031526 0.40738 0.41326 0.4543 -0.12072 0.52145 0.13587 0.55708 0.056813 0.42271 -0.47422 0.56472 0.539 0.56514 0.21634 -0.42309 0.11655 -0.056845 0.084175 0.48163 0.29681 0.26341 0.085208 -0.4314 0.13536 -0.15137 -0.015635\n",
      "240000: boydston -0.095683 -0.22324 -0.22916 0.20322 -0.28577 -0.35978 -0.18099 -0.030605 0.16026 -0.16467 -0.22644 0.081273 -0.35178 0.09431 -0.12558 0.28959 -0.25096 0.085697 0.054156 0.25243 -0.16699 0.0064939 0.13709 -0.16378 -0.32935 -0.040939 -0.4206 0.055919 -0.038707 -0.096223 0.083931 0.11885 -0.063413 -0.047065 0.087938 -0.20514 0.051731 -0.23642 0.2637 0.053512 0.088645 0.34096 -0.17013 0.38911 -0.5337 -0.065592 0.12014 -0.16836 -0.0050649 0.23979 0.20623 -0.12758 -0.11421 -0.50314 0.20337 0.52459 0.042389 0.31446 -0.64691 -0.094127 0.39956 -0.24115 0.53051 -0.052236 -0.4711 -0.12344 0.20117 0.097907 -0.15481 -0.12797 0.16825 0.087192 0.1679 0.62105 0.11902 -0.075662 0.40098 -0.0071636 0.035088 -0.033955 -0.22431 -0.091346 0.026658 -0.43191 0.46551 -0.24759 0.31539 -0.14636 -0.30259 0.36335 0.31123 0.13108 0.3982 -0.094336 0.15284 0.093485 0.22807\n",
      "250000: afterellen.com -0.32387 -0.9943 -0.20289 0.72868 0.40009 0.51742 -0.26246 -0.38053 0.5078 0.53257 0.24738 -0.08104 -0.050665 -0.69436 -0.17054 0.31068 -0.017479 -0.49279 0.473 -0.16014 -0.54113 -0.33412 -0.33767 -0.044322 -0.37426 0.017896 0.11 0.66906 0.03001 0.228 0.043734 0.37817 0.65776 0.43201 0.19028 0.53361 -0.51893 -0.053162 -0.2544 0.67453 0.15435 0.2839 -0.030743 0.29371 0.2518 -0.12674 -0.60324 0.4137 0.0085271 0.5533 -0.066501 0.21175 -0.36714 -0.076638 -0.38603 0.41268 0.45078 -0.28169 -0.41464 0.38545 -0.38574 -0.45541 0.1335 0.19421 -0.26655 0.052574 -0.12248 0.152 0.23767 0.19339 0.37111 0.55566 0.030618 0.74444 -0.28672 0.71879 0.91985 -0.28975 0.47522 0.15291 -0.31997 -0.32457 0.62431 -0.24673 0.33889 -1.027 -0.40787 0.21679 0.16159 0.57051 0.26342 0.39023 0.68627 -0.40346 0.090415 0.20814 -0.50732\n",
      "260000: acuff-rose 0.35392 -0.32482 -0.27675 0.58744 0.38678 0.26705 0.45684 -0.39241 -0.2863 -0.11773 -0.61268 -0.059122 -0.80368 -0.44424 -0.2839 -0.19619 0.014528 0.021449 0.086353 0.52743 -0.50589 -0.53227 0.64627 -0.35298 -0.71143 0.45728 0.35428 0.68271 0.23023 0.70274 0.0028966 0.11778 -0.26683 -0.053551 0.10917 -0.013814 -0.14235 -0.0085331 -0.13137 -0.65853 0.08819 0.59434 -0.71428 0.0062467 -0.50373 0.16853 -0.17129 0.22121 0.68182 0.2743 0.62576 0.043255 -0.36381 -1.1013 -0.26082 0.29469 -0.057577 0.17584 -0.018102 0.11301 0.055899 -0.76428 -0.20227 -0.37762 -0.66641 -0.12102 0.42822 0.082213 0.069251 0.336 0.17604 0.58469 0.33841 0.44256 -0.21186 0.11493 0.37661 0.057312 0.080476 0.10319 -0.61412 -0.43987 0.52695 0.077245 0.56361 -0.32413 0.076334 -0.24507 -0.23391 0.31156 0.051327 -0.36312 -0.2295 -0.084224 0.08908 -0.21329 0.15436\n",
      "270000: close-fitting -0.18776 0.29161 -0.45049 0.27683 0.35919 0.20029 0.059598 0.37716 -0.035997 0.72048 -0.37677 -0.05556 0.17623 0.00051129 0.1209 0.0027851 -0.38397 -0.028916 0.47268 -0.31426 -0.13436 0.070907 -0.24429 -0.41154 0.076006 0.20795 -0.26589 -0.12109 -0.22009 -0.14041 0.17574 -0.41319 0.013683 -0.63661 0.32188 0.20262 -0.0034537 -0.22238 0.30313 0.21439 0.77972 0.32008 0.49455 0.30681 -0.15141 -0.058505 0.59025 0.71334 -0.30032 0.19316 0.069507 0.13727 -0.09266 -0.38373 0.53326 0.59892 0.1752 0.35276 -0.28987 -0.48056 -0.017284 -0.10798 0.030122 -0.19291 -0.21442 -0.24127 0.0039292 -0.14568 0.0016037 -0.21219 -0.25579 0.55712 0.23633 0.5979 -0.41801 0.26129 0.12999 -0.29571 0.51702 0.035257 -0.06769 -0.071129 0.28956 -0.23713 0.30119 -0.074736 0.41244 0.057207 -0.073528 0.34066 -0.050827 0.019827 0.18442 -0.58357 0.44548 -0.21948 0.10452\n",
      "280000: packbot 0.39977 0.21278 0.30418 0.199 -0.46841 -0.0062027 -0.20157 -0.20025 0.49313 0.064453 -0.049247 0.47871 0.18596 -0.47792 -0.65792 0.60003 0.15987 0.11462 0.77218 -0.17036 0.22969 -0.22358 0.096905 -0.18397 -0.15873 -0.08177 -0.35138 0.3654 -0.23934 -0.15141 -0.037131 0.013724 -0.069388 -0.46414 0.18752 0.072294 0.094694 -0.42301 0.2237 -0.18419 0.63649 0.45228 0.072802 -0.046235 -0.19788 0.3331 -0.158 0.39304 0.11696 0.66112 -0.24156 0.41114 0.024182 -0.18038 0.56117 1.0453 0.16768 0.10006 -0.43278 0.091324 0.32139 -0.094915 0.72387 0.13587 -0.037802 0.28514 -0.35057 -0.35811 -0.5993 0.40734 0.56745 -0.15218 0.12518 0.47971 -0.14759 -0.3497 0.28744 0.24004 0.53122 0.39433 0.013154 -0.11773 0.57401 -0.079729 0.048651 0.41358 0.38828 0.17378 0.074792 -0.23202 0.54968 0.46018 -0.0024691 -0.08781 0.19005 -0.77004 -0.0076627\n",
      "290000: comptel 0.53551 0.15861 -0.02185 0.41383 -0.0012728 -0.45381 0.33628 0.043651 0.15083 -0.73497 -0.15826 -0.63383 0.033605 -0.098161 0.18731 0.56205 -0.087372 0.18402 -0.035716 -0.11634 -0.59741 0.21681 -0.75565 -0.68181 -0.042567 -0.0047546 -0.24197 0.039952 -0.18221 -0.044384 -0.2209 0.16336 0.10987 -0.079618 0.030656 0.12003 0.12413 -0.31548 0.17223 -0.027989 0.68542 -0.028724 0.13517 0.1045 -0.33876 -0.21945 0.12205 0.16662 -0.43848 -0.046104 -0.21884 0.52617 -0.48359 -0.56632 0.44328 0.99471 0.60036 0.39324 -0.34637 0.18364 -0.16614 -0.075212 -0.036775 -0.81942 -0.60305 0.087585 0.29608 -0.10398 0.31222 0.011241 0.034909 0.13137 0.031547 0.26183 0.057415 -0.61681 0.2762 -0.11505 -0.19271 -0.39201 -0.37031 -0.26891 0.47937 0.13841 0.43946 -0.1318 0.0049078 0.47702 0.026095 0.024479 0.29756 -0.25686 0.55301 -0.15128 0.67296 -0.52707 0.24545\n",
      "300000: tanke -0.0076017 -0.91656 -0.46305 0.092254 0.14661 -0.31531 0.19378 0.18798 0.2498 -0.33432 -0.0073541 0.046903 -0.32156 0.50438 0.056309 -0.36864 -0.13587 0.18673 0.10032 0.12296 -0.14489 0.14346 -0.37561 -0.22455 -0.12523 0.47993 0.63618 0.301 0.11076 0.20484 -0.12046 -0.6782 -0.031457 -0.093299 0.27319 0.14711 0.13002 -0.19153 0.18169 0.11588 0.088411 0.63364 0.042805 0.35928 0.027514 -0.19297 -0.046434 0.16196 0.010562 0.47396 0.075233 0.27252 -0.16257 -0.56256 0.12982 0.35643 0.48666 0.19425 -1.0962 -0.036469 0.018865 -0.69793 0.12931 -0.39118 -0.35563 0.207 -0.19058 -0.3764 -0.3346 0.16312 0.19852 0.27188 -0.031165 0.44726 -0.1016 0.043668 0.15963 0.091841 0.6432 0.045873 -0.47045 -0.23536 0.054299 -0.4203 0.54585 0.1995 -0.57888 -0.26864 -0.28643 0.18537 0.062789 -0.11676 0.22026 -0.34283 0.24021 0.29291 0.049536\n",
      "310000: saraju -0.071412 0.08355 0.32427 0.80219 0.64144 0.05391 -0.17368 0.65809 0.48883 0.35445 -0.78813 -0.53816 0.0015355 -0.42472 -0.092583 -0.081556 -0.34988 -0.18347 -0.0054173 0.1889 0.16043 0.60937 0.44596 -0.35879 -1.0402 0.085966 0.07543 0.21351 -0.61498 0.57552 0.060666 -0.56023 0.36649 -0.13831 0.14594 -0.45199 -0.1664 -0.43903 0.72765 0.22315 0.39696 -0.43919 0.11731 -0.21594 -0.19597 -0.17426 0.58225 0.0027591 0.016924 0.23923 -0.32102 -0.13193 -0.29054 -0.77111 0.19243 0.58919 0.091745 -0.12969 -0.39982 -0.51467 -0.054215 -0.77193 0.68504 -0.35545 0.00092065 -0.20084 -0.16407 0.096771 -0.26309 0.25206 0.21372 0.080732 0.11218 0.19664 -0.036212 0.049138 0.32062 0.26338 1.0155 -0.11292 -0.53211 0.33385 0.0043159 -0.57909 0.79581 0.1787 -0.24009 -0.0077358 0.14272 0.82853 0.44359 -0.38914 0.15251 -0.22288 0.18255 0.73486 0.79434\n",
      "320000: rouiba 0.096197 -0.08665 0.04063 0.55965 0.057081 0.02907 -0.18187 -0.033039 -0.24751 -0.05021 0.11897 -0.022732 0.20544 -0.18259 0.17744 0.075073 -0.59835 -0.046684 -0.00834 -0.14666 0.095674 0.36108 0.24973 -0.1309 -0.34697 0.21493 -0.085562 0.44759 -0.26579 0.2924 -0.36866 -0.85774 0.47391 -0.1517 -0.26812 -0.23099 0.35655 -0.48061 -0.047793 0.26528 0.26091 -0.1606 -0.046824 -0.13274 -0.50451 0.39011 0.2361 0.28917 0.0016917 0.51223 -0.32806 0.066901 -0.32492 -0.98092 -0.12877 0.50095 -0.11011 -0.48106 -0.56264 -0.42864 -0.025081 0.18901 0.095652 -0.28872 -0.43037 0.26823 -0.40832 -0.63122 -0.44366 0.0013408 0.13988 0.050693 -0.16981 0.57015 0.026238 0.053076 0.30671 0.081255 0.61202 -0.10435 -0.1011 0.41063 0.094192 -0.063913 0.61082 0.33532 0.071574 -0.0051555 0.087887 0.43715 0.35135 0.31124 0.48559 0.07972 0.0076495 0.46683 0.20198\n",
      "330000: discomfit -0.15599 0.033059 -0.045736 0.31338 -0.22631 -0.0043026 -0.12421 -0.63846 0.56654 0.026745 -0.29391 0.2338 0.14267 -0.74956 0.046132 -0.15418 -0.28701 0.37874 0.20229 -0.089532 -0.43909 -0.33244 -0.31092 -0.35571 -0.12882 -0.29569 -0.032961 -0.020796 0.11095 -0.20814 0.48459 -0.18497 0.17492 -0.0027187 -0.29865 -0.005104 0.070042 -0.34097 -0.47115 -0.049314 0.465 0.18558 -0.046338 -0.011255 -0.16393 0.20413 0.36404 0.37056 0.15012 -0.20085 -0.049888 0.31974 -0.41183 -0.54265 0.38552 0.60873 0.23728 -0.23122 -0.42134 -0.39776 -0.1654 0.10528 0.42356 -0.046329 -0.18857 0.4286 -0.10403 -0.25846 -0.54152 -0.24339 0.17022 0.062257 -0.094321 0.52526 -0.33014 -0.084249 0.018827 -0.0048921 0.71703 0.0069143 -0.77355 -0.32169 -0.090985 -0.23656 0.14976 0.058168 0.33917 0.40954 -0.33673 0.28565 -0.13365 0.1853 -0.1735 -0.39103 0.41732 -0.25873 0.29097\n",
      "340000: numurkah 0.076335 -0.48177 -0.24149 0.47322 -0.25941 0.079653 0.13705 -0.085171 0.30149 -0.24108 0.22084 -0.11981 -0.19559 -0.25641 0.13144 -0.030652 0.15228 -0.1106 0.2388 -0.070121 -0.24917 0.15812 0.05414 -0.078638 -0.44054 -0.018876 0.12278 0.065662 -0.057862 0.03051 0.088205 -0.33619 0.16194 0.067156 -0.0019973 -0.27819 -0.02838 -0.26143 -0.19423 -0.1225 0.42338 0.16644 -0.046458 0.009327 -0.24148 0.23499 0.22172 0.4554 0.094187 0.51217 -0.264 0.35146 -0.23341 -0.28875 0.020078 0.95775 -0.29801 -0.3153 -0.68335 -0.22703 0.2753 -0.30056 -0.059171 0.058923 -0.46933 0.32637 -0.34484 -0.13566 -0.17289 -0.11497 0.23315 -0.29121 0.20513 0.39248 -0.2841 0.13655 0.36109 0.36835 0.60843 0.22079 -0.51556 -0.12748 0.20748 -0.11306 0.79226 -0.12687 -0.1841 0.33684 0.0070715 0.057627 -0.092475 0.17471 -0.091302 0.072614 0.22294 -0.16793 0.23545\n",
      "350000: hla-a 0.031391 -0.050161 -0.082944 0.38878 0.1738 0.30447 0.4858 -0.33718 0.15761 0.018224 -0.19319 -0.32857 -0.15068 -0.42734 -0.14 -0.0098475 -0.25022 0.4429 0.6303 0.21971 -0.32162 0.12598 0.0099228 -0.56093 -0.1628 0.11204 0.31083 0.35544 -0.33649 0.26748 0.44576 -0.059134 0.35948 -0.10748 0.52516 -0.32496 -0.036112 0.29814 0.047233 0.0044441 0.34504 0.49406 0.0060015 0.21061 -0.29494 0.50476 -0.023796 0.54973 -0.33724 0.49728 0.027097 0.31801 -0.42551 -0.32877 0.25338 0.7424 -0.22832 0.29861 -0.34721 -0.25709 0.42851 -0.265 0.43999 0.05572 -0.10601 -0.010538 -0.4042 0.043886 -0.35076 0.046947 -0.11567 0.38713 -0.044151 0.20777 -0.129 0.10299 -0.13806 -0.4752 0.36955 -0.22013 -0.58399 -0.27215 0.11437 -0.34799 0.11649 0.20621 -0.41072 0.12881 0.25421 0.41852 -0.33327 0.034756 -0.10168 -0.42495 0.19202 -0.18549 0.035505\n",
      "360000: 90125 0.21922 -0.43855 0.010213 0.064248 -0.24035 -0.24785 0.43025 -0.19884 0.61969 -0.08168 0.080359 0.4361 -0.076933 -0.083666 -0.20155 -0.12549 -0.20612 0.22378 0.11384 0.15375 0.3226 -0.13741 -0.52822 -0.42005 -0.26194 0.53978 0.056819 0.16771 0.25985 0.29494 0.61871 -0.45837 0.13618 -0.039174 0.31845 -0.013487 0.44762 -0.54543 -0.36734 -0.41106 0.278 0.48187 0.15101 0.59854 -0.080198 -0.11467 -0.11515 0.17488 -0.39403 0.47245 -0.11979 0.23458 -0.17546 -0.5524 -0.079758 0.51376 0.53591 0.41671 -0.39263 -0.50083 0.19066 -0.28588 -0.71516 -0.64882 -0.037117 0.05373 -0.48828 -0.39712 -0.20965 -0.2379 0.27184 0.71653 0.038642 0.52173 -0.48854 0.35831 0.046338 0.5796 0.16751 -0.025992 -0.455 -0.18522 0.28633 0.12838 0.63393 -0.10117 -0.08688 -0.3696 -0.46761 0.41482 -0.35258 0.1688 0.17587 0.098452 0.40537 -0.28936 -0.033086\n",
      "370000: zipkin 0.40199 -0.086073 -0.48024 -0.11565 0.02011 0.30963 0.19435 -0.13951 -0.098383 -0.38174 0.13641 0.35578 -0.17208 -0.36498 -0.28103 0.27389 0.099599 0.44211 0.5836 -0.13189 -0.36361 0.049308 0.0086353 -0.33793 -0.46666 -0.21271 -0.18006 0.28329 0.070682 0.031501 0.022662 -0.029588 -0.16106 0.19797 -0.43724 0.05005 0.22196 -0.43146 0.081992 0.1062 -0.030463 0.1925 0.13956 -0.06601 -0.074844 0.26782 0.0094542 0.24434 0.16704 0.19442 0.033621 0.10809 -0.11156 -0.56133 0.42513 0.75267 0.1367 -0.024745 -0.2468 -0.28788 0.27131 -0.097655 0.27625 -0.15506 -0.063394 -0.36538 0.0060131 -0.12018 -0.70132 -0.30129 0.10127 0.24009 0.042289 0.26366 0.085478 0.12594 -0.18107 0.088584 0.64416 0.45198 -0.03242 0.32854 0.033886 -0.1887 0.70717 0.27418 0.11116 -0.17092 -0.3261 0.26707 -0.0085236 -0.043047 0.40734 -0.60712 0.068523 -0.32209 0.20867\n",
      "380000: lombarde 0.17181 -0.38677 -0.19093 0.25784 -0.14552 -0.085949 0.00084307 -0.14172 0.62479 -0.21237 0.27659 0.54207 0.028671 0.23243 -0.080071 0.066366 -0.023302 0.040742 0.17559 -0.05777 -0.041533 -0.04002 -0.60738 0.13284 -0.87946 0.30869 0.14094 0.85061 -0.17688 -0.13589 -0.45246 -0.30593 0.26907 -0.026722 0.39823 -0.30519 -0.13545 -0.64646 -0.015508 0.029418 0.33137 -0.14465 -0.0079425 -0.18729 -0.064249 0.062656 -0.13318 0.68184 0.21231 0.60913 0.00015834 0.3757 -0.27528 -0.38073 -0.013302 0.73253 0.51432 0.47519 -0.56666 -0.52877 0.30256 -0.34474 0.27188 -0.10483 -0.14169 0.26036 -0.73677 -0.99287 -0.26116 -0.34765 0.094392 0.53348 0.46213 0.49718 0.36549 -0.095853 -0.062255 -0.13906 0.69987 -0.0090784 -0.4687 -0.045936 0.089142 -0.72693 0.51778 0.56614 0.12124 0.079259 0.58474 0.57933 0.57491 0.16733 -0.19154 -0.19623 -0.17046 0.44264 -0.053798\n",
      "390000: 1.137 0.49522 0.17979 -0.15346 0.5246 -0.021729 0.10892 0.13724 -0.29523 0.071893 0.1089 -0.034811 0.41227 -0.523 -0.30983 -0.26139 -0.073651 -0.047225 -0.036328 0.17519 0.069244 -0.15686 -0.11197 -0.36088 -0.35464 -0.057412 -0.21619 0.45981 0.68138 0.16192 0.017543 0.1678 -0.44128 0.27802 -0.39053 -0.20165 0.18663 0.0025345 -0.55338 -0.13277 0.35181 0.44936 0.24425 0.030123 0.082591 -0.10368 0.16057 -0.34081 0.32926 -0.27812 0.032259 0.27518 -0.11913 0.21492 -0.58495 0.11969 0.27652 0.10691 -0.2442 -0.46086 -0.48381 0.4153 -0.29248 -0.17763 -0.041351 -0.76273 0.35444 -0.42056 -0.046353 -0.19216 -0.13472 0.032999 0.45614 -0.11411 0.2661 -0.46228 -0.099707 -0.13143 0.051958 0.44664 0.23042 -0.4967 0.16803 0.12475 0.13319 0.66758 0.1956 0.33675 0.03329 -0.26781 0.36442 -0.02643 0.58293 0.14939 -0.72751 0.32645 -0.053761 -0.015863\n",
      "Palavras ignoradas: 0\n",
      "10000: distribuída 0.279184 -0.059367 0.884441 -0.621538 -0.134511 -0.184927 -0.125765 0.402987 -0.422638 0.523744 0.042716 -0.381731 0.121640 -0.301497 0.832516 -0.367990 -0.616317 -0.008846 0.796937 -0.134521 0.414595 -0.140344 0.212067 -1.090750 -0.642239 0.744789 0.067219 0.235801 0.171474 0.396434 -0.578489 -0.343412 0.262512 -0.857107 -0.682621 -0.498741 0.105770 0.111963 -0.744005 -0.337165 0.354616 -0.472595 1.097979 -0.332028 -0.338311 -0.199299 -0.452992 -0.476396 -0.147330 -0.205088 0.132998 0.058852 0.216672 -0.204573 -0.199280 -0.390836 0.031583 -0.120305 -0.026678 -0.036252 0.034537 -1.087442 0.561698 0.436112 0.189916 0.196905 -0.284908 0.335488 -0.086327 0.159021 0.201809 0.561149 0.129761 -0.129541 0.381498 0.451839 -0.116486 0.626156 0.176794 0.656397 0.529451 -0.467564 -0.233912 0.379477 0.601768 0.188032 0.469145 -0.320838 0.341106 0.050658 -0.261275 -0.079436 1.023572 -0.029866 -0.750076 0.560593 -0.094370\n",
      "20000: selena -0.739634 0.117485 -0.573358 -0.069093 -0.601702 -0.067250 -0.401919 -0.109372 0.333540 0.192756 0.192939 -0.685713 0.119372 -0.745820 -0.319630 -0.491912 0.444359 0.775031 -0.289532 0.747291 -0.522381 -0.072315 0.926318 0.539221 -0.808793 -0.423081 0.032677 -0.192956 -0.062458 0.123115 0.322935 0.209115 0.305189 0.888357 -0.039908 0.339819 -0.146124 -0.049158 -0.397604 0.217958 0.674600 -0.168621 0.267859 -0.317864 0.290828 0.206206 0.041330 0.291369 0.548082 0.485030 0.213011 -0.410299 0.447085 0.104954 -0.170926 -0.343844 -0.366484 -0.137717 -0.412370 0.037650 -0.076479 0.305098 0.662369 -0.163098 0.173374 0.525436 0.264224 0.424487 0.704475 0.276430 0.681088 0.319666 -0.825969 -1.081229 -0.851012 0.229864 0.566231 0.121955 -0.643924 -0.194908 -0.809387 -0.522516 0.348205 0.331812 0.818036 0.630504 0.089637 -0.001945 -0.657667 0.277024 0.312361 -1.054257 0.618177 0.234891 0.149873 0.001705 -0.254412\n",
      "30000: sailor -0.110311 -0.696703 -0.839099 0.127139 -0.826695 0.022835 -0.296882 -0.296856 0.303250 -0.446877 0.393064 -0.182003 0.800164 -0.023978 -0.327403 0.392617 1.007905 -0.325129 -0.045455 -0.502281 0.049998 0.727869 -0.021863 0.486534 0.069884 0.064563 0.032128 -0.851287 -1.269793 0.290697 0.110702 -0.516969 0.761018 -0.030895 -0.661740 0.532263 -0.834598 -0.368417 0.043770 0.116539 -0.239473 -0.609377 0.027580 0.203228 -0.503002 -0.382863 -0.280147 0.463413 -0.207571 -0.338570 -0.197084 0.511028 0.424370 1.229698 -0.110507 -0.040898 -0.137777 0.121899 -0.890328 0.333164 -0.176483 -0.130794 0.792957 -0.552513 -0.453679 0.221032 -0.165448 -0.532383 0.169914 0.186405 1.219083 0.565580 0.289620 -0.885446 -1.297709 0.044854 0.600751 -0.118277 0.525891 0.313110 -0.399728 -0.849605 0.605894 0.686271 -0.325228 -0.926831 0.534315 0.013065 0.536478 -0.769385 0.246413 0.132455 -0.197874 -0.048163 -0.841392 -0.470229 -0.464369\n",
      "40000: aguaceiros 0.490698 0.190373 0.355682 0.537600 -0.422971 -1.110973 -0.396923 0.219353 -0.020729 0.002961 -0.204815 0.407919 -0.104587 0.339118 0.388135 -0.359273 -0.948074 0.673725 0.699179 0.909420 -0.799313 0.263386 -1.007713 -0.013213 -0.188447 -0.349349 0.648499 0.496168 -0.377433 -0.513642 0.512070 -1.453058 -0.100673 0.691610 0.571850 -0.060878 -0.531227 -0.579589 -0.777317 -0.441139 -0.362296 -0.285118 0.052324 -0.055990 0.591919 -0.866515 0.292498 0.385726 -0.838227 0.786291 0.732363 1.062602 0.524218 -0.167562 -0.401484 -0.645844 -0.931991 -0.388398 -0.703693 0.040628 0.540857 -0.454842 0.789798 0.659249 0.242043 -0.462100 0.809225 0.898811 -0.562189 -1.173907 -0.626282 -0.250821 -0.093983 0.188887 0.527573 1.011514 -1.519213 1.113552 -1.228164 1.015509 1.804401 0.293936 -1.090180 -0.131966 -1.963118 -0.149156 -0.595349 -1.069631 0.430220 1.437047 -0.947361 0.421434 0.982109 -0.043874 -0.237912 0.354434 0.098199\n",
      "50000: retrô 0.124150 0.317331 0.085425 -0.155943 -0.385795 0.016121 -0.527326 0.620884 -0.176347 -0.197971 0.487490 -0.111004 0.299141 -0.573217 -0.262806 0.245609 -0.196259 0.367349 -0.156169 0.923736 -0.388866 0.577562 0.566426 0.742496 0.040560 0.333013 -0.423608 -1.386732 -0.080769 0.910841 0.427817 -0.107931 -0.343947 -0.407712 -0.458752 -0.063789 -0.783957 0.631704 0.254265 0.772351 0.358376 0.274169 0.100999 0.391876 0.290991 -0.422412 0.583953 -0.002566 0.107249 0.265291 0.559465 -0.240169 -0.280371 0.037210 0.030142 -0.080554 0.860162 0.140216 -0.061753 0.559311 0.029677 -0.366652 -0.188170 -1.337240 0.986189 -0.496286 0.345303 0.128744 0.159846 -0.749723 -0.072016 0.072941 0.491952 -1.087229 -0.279957 0.533394 -0.164376 -0.067869 -0.151409 -0.261770 -0.130999 -0.052833 -0.585072 -0.539915 -0.276547 0.409495 -0.109494 -0.264972 -0.200038 0.024134 -0.147968 -0.277052 0.255373 0.256217 -0.420835 0.245416 0.520673\n",
      "60000: indesmentível -0.497376 -0.143558 -0.091989 0.352247 0.461443 -0.173494 0.256803 0.510938 -0.621769 0.305854 0.568021 0.023805 -0.733780 -0.707288 -0.317769 0.184260 0.083318 -0.321908 0.492476 -0.375662 0.245257 -0.129613 0.049970 0.315397 0.083033 0.426858 0.109447 -0.208940 -0.404069 -0.429558 -0.247464 0.172176 -0.544378 0.070968 0.909858 -0.888211 0.207209 0.520487 0.259934 0.686803 -0.438552 -0.353034 0.703599 0.123261 -0.409645 -1.214578 0.116380 -0.581463 -0.430124 0.089843 0.514863 -0.462147 -0.028873 0.350411 -0.267865 -0.709814 -0.800469 0.136601 0.862114 0.451798 0.377967 -0.147509 -0.302341 -0.764807 0.158078 0.690933 0.297249 -0.032645 -0.103391 0.014651 -0.415785 -0.394202 -0.147127 1.056885 0.117255 0.113785 0.147501 -1.133766 0.150047 -0.263347 -0.450691 -0.083236 -0.592924 -0.030658 0.321049 0.096249 0.068939 -0.085477 -0.466154 -0.098269 -0.114757 0.098996 -0.594735 0.766680 -0.022108 0.147848 -0.173735\n",
      "70000: kouchner 0.450084 0.190265 0.473585 0.176422 0.016413 0.295090 -0.483182 0.988914 0.051797 -0.439687 -0.121778 0.155904 0.064542 -0.005883 -1.048804 1.514618 -0.624411 0.196875 -0.298258 -0.272241 -0.225100 1.401240 -0.507865 0.205506 0.012819 0.456970 0.026892 -1.274167 1.281391 -0.124293 -0.209896 0.088711 0.112363 0.109228 0.590768 -0.566777 -0.720450 -0.092626 0.395801 -1.001129 -0.341636 0.492298 0.091671 0.139924 0.275874 0.817077 -0.784032 0.178078 -0.452103 -0.760550 -0.335371 -0.575848 -0.149152 -0.353028 -0.140112 0.433133 0.038930 0.453046 -0.039339 0.345712 -0.928418 0.077105 -0.339010 0.597215 -0.510649 -0.031711 0.020999 0.189331 -0.033195 0.122937 0.327366 -0.613815 0.509318 0.924750 0.069000 -0.133057 -0.078171 0.060292 -0.197175 -0.250528 -0.101047 -0.953871 0.444727 0.138319 0.372611 0.796755 0.760921 0.126513 -0.145956 -0.332160 0.128141 -0.499276 0.315581 0.390823 -0.645213 0.665687 -0.457667\n",
      "80000: hoya -0.437406 0.403915 -0.017647 0.215593 0.178278 0.415177 0.070868 -0.066680 -0.005172 -0.593603 -0.224948 0.007076 -0.012903 -0.266268 -0.185033 -0.070015 -0.201223 0.615342 -0.187996 -1.021451 0.139104 -0.287154 -0.367899 0.449546 -0.246909 0.635063 -0.408980 0.397203 -1.257734 -0.469626 0.209464 0.299724 -0.503467 -0.047104 -0.857212 0.746851 0.215483 0.542215 0.902342 -0.107458 0.715878 0.838733 -0.737804 -0.038920 -0.514426 -0.205526 0.331927 -0.022138 -0.310167 0.543656 0.334197 0.018940 -0.378325 -0.075877 -0.315423 -0.161075 0.515983 -0.138557 -0.532183 -0.675578 0.551953 0.569284 -0.840993 -1.281426 0.043902 0.163560 0.198886 0.043311 -0.761292 0.093766 0.555288 0.477919 -0.120812 -0.464856 0.078673 0.301383 0.087364 0.824877 -0.375170 -0.523925 -0.614741 0.301365 0.485723 -0.087192 -0.092732 0.017123 0.104938 0.030650 -0.009029 0.004533 -0.327642 0.003370 0.615886 0.698217 0.900117 0.120966 -0.992545\n",
      "90000: j&f -0.335818 0.413810 0.244010 0.385562 0.088379 -0.490895 0.457133 -0.435002 -0.749592 -0.738411 1.164515 0.099580 1.171711 0.780146 0.898285 -0.036246 1.037152 0.676118 0.105324 0.510416 0.729272 -0.479752 0.707789 0.920534 0.527244 0.709938 -0.880187 0.255554 -0.535080 0.533026 -0.404423 0.902342 -0.709179 -0.303849 -0.221119 0.897750 -0.476885 -0.759702 -0.506233 -0.448399 0.592326 -0.358701 0.113986 -0.579582 -0.505707 -0.039610 -0.016251 0.195236 -0.997504 0.648508 0.021739 1.342032 -0.225774 0.357290 -0.158681 -1.097170 0.081132 -0.818305 -0.513543 -0.504516 -1.075088 -0.018976 0.472958 -0.247858 -0.614466 -1.200976 0.040991 0.279286 0.662720 -0.406471 0.671567 0.177249 0.081559 -0.261471 -1.045431 -0.528645 -0.288895 0.514343 0.217980 0.105562 -1.350684 0.177324 0.639575 0.261014 0.182119 -0.369484 -0.115245 0.921147 -0.028238 -0.334219 0.781758 0.089597 1.085559 -0.625973 -0.419937 0.814406 -0.334255\n",
      "100000: castra -0.133000 -0.224870 0.100251 -0.002748 -0.553791 -0.307429 0.522032 -0.762629 0.396580 0.426797 0.117342 -0.689981 0.601826 0.648396 -0.426372 -0.137145 0.322391 -0.370231 0.805799 -0.129124 -0.400184 0.424601 0.985878 0.183652 0.106746 0.188244 -0.127541 0.468382 -0.439974 0.567756 -0.438118 -0.218538 0.384816 0.105610 0.248237 1.173025 0.449514 -0.067591 0.468847 0.323908 0.107317 -0.325015 0.402580 0.181564 0.078450 0.701798 -0.613930 0.077581 1.221213 0.426246 0.076753 0.022298 0.947439 -0.101764 0.632776 -0.032885 -0.045778 -0.536526 0.067620 -0.050646 -0.828175 -0.103133 0.163303 0.199005 -0.148210 -0.252692 0.320600 0.205503 0.102642 -0.334754 0.115078 0.276635 -0.405170 0.048278 -1.003310 -0.389345 -0.201878 -0.768275 -0.336588 -0.319101 -0.263728 0.716734 0.662276 0.133053 0.567237 0.794694 0.901984 0.049893 -0.527943 -0.028121 0.304500 0.016748 -0.098511 0.846581 -0.616306 -0.813400 0.516098\n",
      "110000: gynt 0.527341 -0.688102 -0.591529 0.629688 -0.256187 0.329830 0.536165 -0.031938 -0.184550 -0.141435 -0.645797 0.170991 -0.102386 0.627678 0.112375 0.032482 -0.691307 0.781093 0.013634 -0.006909 -1.157441 0.656324 0.189320 -0.702483 -0.270673 0.378391 -0.144389 0.374514 -0.301433 0.116476 -0.877699 -0.525491 0.380947 -0.354176 -0.322542 -0.047913 1.132633 0.085576 0.351861 0.079870 0.311028 -0.623180 -0.184362 0.092253 -0.597002 0.090670 -1.172049 0.380051 -0.196350 -0.571290 -0.209454 -0.181171 0.011427 0.103177 0.709681 0.318914 0.074607 -0.847379 -0.268305 -0.039015 0.377563 -0.753488 0.615416 -1.481632 0.669308 0.084713 -0.163231 0.260009 1.055153 -0.284928 -0.240628 -0.293300 0.022684 -0.129400 -0.071412 0.612054 0.445024 -0.873825 0.159674 0.073104 -0.081264 0.358855 -0.553648 0.316761 -0.216545 -0.385573 0.198451 -0.082822 -0.777912 0.084706 -0.363319 -1.068823 -0.378955 0.451451 -1.059985 -0.152650 -0.050391\n",
      "120000: caddie 0.104055 -0.372045 0.071635 0.602490 0.261976 -0.144903 0.328593 0.137213 0.668576 0.720004 -0.832587 -0.106292 0.031338 -0.480534 -0.593717 0.556129 -0.829852 0.557918 0.017798 0.048982 0.038484 -0.132943 -0.083234 0.211930 -0.407396 0.462462 0.614505 -0.012663 0.422170 0.200962 -0.024737 -0.366993 0.001268 0.150718 0.377584 0.462271 -0.058410 0.607248 0.626298 -0.433236 -0.437818 0.530448 0.192791 -0.139840 -0.761649 0.752876 -0.623335 0.025268 0.149860 -0.166260 0.293572 0.003726 0.701914 -0.487737 -0.369818 -0.458202 0.018331 -0.387282 0.480796 -0.071350 0.339884 0.488902 -0.247857 -0.835916 0.329327 0.140699 -0.342306 -0.783646 0.226635 -0.297807 -0.176271 -0.275119 -0.168493 0.211384 -0.276963 0.002916 0.179793 -0.217313 0.473902 -0.113896 0.074066 0.020462 0.284790 0.662002 0.256580 -0.172891 -0.064472 -0.232864 -0.347619 -0.216776 0.312874 -0.675557 0.544341 0.243703 0.564142 -0.399545 0.660131\n",
      "130000: afluíam 0.163719 0.150149 0.549917 1.176841 -0.057785 -0.398048 0.056188 0.700695 -0.139697 0.735348 -0.230879 0.568855 -0.129317 -0.104182 0.283009 0.111535 -0.198858 0.674899 -0.056052 0.063528 0.247971 -0.751277 -0.056582 -0.316497 -0.145259 -0.311682 -0.552872 -0.342050 -0.567058 -0.217783 -0.341066 0.391882 -0.298197 0.123630 -0.046668 0.636307 -0.281691 -0.821178 0.349452 0.414742 0.086782 0.255989 1.102592 0.657430 -0.369667 0.230709 -0.006595 -0.704217 -0.420978 -0.316226 -0.853561 -0.159228 0.299316 0.155067 0.202514 0.240337 -0.148483 -0.250680 -0.279609 -0.195888 -0.466478 0.256502 -0.121601 0.829971 0.054325 -0.270682 -0.089326 0.521900 0.145956 -0.354298 0.548337 -0.158361 -0.120373 0.319880 -0.531197 -0.339846 -0.206244 0.299956 -0.015057 0.351381 -0.308695 -0.108954 -0.471175 -0.533098 0.899530 -0.275621 0.018947 0.434214 -0.274784 0.390922 -0.180317 0.875834 -0.129375 -0.275223 -0.254454 -0.166385 0.924053\n",
      "140000: nashua 0.644640 -0.238797 0.442243 0.016420 0.258370 -0.247079 -0.202147 0.340853 -0.580093 0.363491 -0.319324 0.105331 0.878052 0.702757 -0.413950 0.431303 0.421862 0.266702 -0.167607 -0.759654 -0.224547 0.379108 -0.137439 0.157708 0.228187 -0.042046 -0.290009 -0.009521 -0.038403 -0.310856 0.362924 -0.143854 -0.112658 -0.661177 0.365003 1.020357 0.593139 0.486532 0.313673 -0.340609 0.103933 -0.052481 -0.000434 0.255599 0.200394 0.182199 0.505179 -1.504307 0.192082 0.273703 0.259634 0.151897 0.425433 0.212553 0.109509 0.881026 -0.979001 -0.320578 -1.041044 -0.084929 -0.370480 0.689244 -0.003604 -0.662210 0.315970 -0.488109 0.496749 -1.068910 -0.100816 -0.157924 0.309113 -0.208195 0.396058 -0.267992 -0.059461 -0.758731 0.397201 -0.031042 0.324324 -0.434916 -0.373519 -0.083317 0.652968 0.523479 0.351505 -0.690281 -0.005035 0.671711 0.821997 -0.085667 0.170084 0.142226 -0.405871 0.217014 -1.217482 0.457880 0.114295\n",
      "150000: amok 0.010980 0.346474 -0.060475 0.524727 0.503857 0.060236 -0.074248 -0.065873 0.522731 -0.176079 -0.197379 0.382251 0.491150 -0.053059 -0.357695 0.496669 -0.186776 -0.062067 -0.731092 0.015359 -0.106005 -0.287282 -0.159428 -0.166002 -0.178115 0.141400 0.408693 0.501597 -0.291417 -0.005599 -0.258049 0.170662 0.586270 -0.682287 -0.017828 0.480279 0.161678 -0.115759 0.334337 -0.236829 0.119615 0.343692 0.136033 0.135613 -0.935575 0.026702 0.150527 -0.420499 -0.210478 0.704402 -0.172511 0.415490 -0.359154 0.124522 -0.117460 -0.274845 0.398219 -1.099334 -0.400384 0.362433 0.169606 -0.566534 0.016001 -0.501782 -0.115119 -0.341929 -0.428695 -0.602511 -0.016765 -0.554830 -0.103523 -0.194884 0.104814 0.037715 -0.475263 0.367794 0.061429 -0.242168 0.310892 -0.475209 -0.011351 0.162347 -0.441135 -0.153032 -0.663341 -0.623473 -0.002311 -0.138963 0.115451 -1.020930 0.212568 0.188383 -0.015323 -0.195201 -0.363183 -0.125024 -0.201394\n",
      "160000: pormenorizou -0.205142 0.418963 -0.211454 0.573997 -0.237402 0.230626 0.021562 -0.031983 0.117865 0.212800 0.025438 0.247747 0.426785 -0.035377 -0.544281 0.074832 0.244472 0.061943 -0.040924 -0.460759 -0.406890 -0.209225 0.061445 0.405379 0.408235 0.431578 -0.386790 0.119522 0.256958 -0.818099 -0.062777 -0.082119 -0.136527 -0.477186 0.008502 0.155565 -0.258734 -0.068314 0.050703 -0.210005 -0.318645 0.059384 0.034763 0.152022 0.046964 -0.189120 -0.449420 0.377326 -0.737651 -0.724830 0.293396 -0.286232 -0.111247 0.535401 -0.016760 0.073297 -0.243457 -0.372111 0.156384 0.376379 0.167443 0.425730 -0.305256 0.190740 0.086825 -0.003761 0.426453 0.376202 -0.238764 0.138043 -0.019956 -0.284292 0.013146 0.078663 -0.171176 -0.100968 -0.137724 -0.013766 -0.210353 -0.451873 -0.329921 0.070868 0.032371 0.120683 -0.219531 0.301600 -0.099705 0.023914 0.067863 0.073650 0.073575 -0.591147 -0.082958 -0.079304 -0.058835 0.260927 0.022226\n",
      "170000: otway 0.424749 0.038349 1.019149 0.254708 0.031800 -0.020381 0.037108 0.523601 -0.799228 -0.157493 0.074319 -0.103822 -0.359119 -0.391779 -0.288641 0.220949 0.495933 -0.272035 0.335165 -0.558043 -0.940852 -0.053060 -0.153787 -0.669994 -0.253059 0.295533 -0.150365 0.538276 -0.130957 -0.789226 0.013391 -0.339289 0.068418 -0.610901 0.159709 -0.283371 0.414819 0.376045 0.861286 -0.294715 -0.039904 -0.194255 0.008939 -0.262044 -0.558599 0.026319 -0.114670 0.170522 -0.094069 0.390751 -0.609902 -0.158740 -0.213269 -0.380544 1.041732 0.251227 -0.057555 0.287436 0.142647 0.548700 -1.037230 0.350768 -0.267687 -0.480524 -0.810058 -0.054137 0.003952 -0.185809 -1.551040 -0.241593 0.135678 -0.245989 0.239805 0.441544 -0.228014 -0.083267 -0.447906 -0.066650 0.126104 0.239556 -0.003499 0.032880 -0.059942 -0.447780 -0.163092 0.089962 -0.150503 0.382896 0.050724 0.200411 -0.426137 -0.382150 -0.806107 0.757618 -0.609958 0.296694 -0.436447\n",
      "180000: bandeirismo 0.218643 0.129686 0.200189 0.860476 0.506312 0.536858 0.296595 0.328490 0.690016 -0.651303 -0.002688 1.234573 -0.401021 0.008093 -0.370403 -0.350893 -0.530078 0.136359 -0.270642 -0.214787 -0.196840 -0.106354 -0.655390 0.322404 0.030584 0.261652 0.060137 -0.576947 -0.211026 -0.638789 0.246841 0.051157 -0.102668 -0.560378 0.598856 -0.195938 -0.535568 0.005989 0.854315 0.154378 0.098433 0.467680 -0.723487 0.740134 -0.180832 0.095802 0.266992 -0.480044 -0.073066 0.111052 -0.091485 -0.039079 -0.112321 0.439798 0.006098 0.391563 0.297338 -0.596398 0.266208 0.253523 0.437524 -0.115931 -0.026365 -0.140573 0.378250 0.038355 -0.378835 0.259003 -0.078091 -1.276206 -0.280848 -0.235860 0.159670 -0.298848 -0.013913 -0.183183 -0.535201 -0.299507 -0.827351 0.009690 -0.571541 0.804650 0.402558 -0.475769 -0.009939 -0.069349 0.432469 -0.029908 -0.452939 -0.077276 0.277797 0.110865 -0.208419 -0.224083 -0.440080 -0.323133 0.359590\n",
      "190000: críptico 0.089329 0.550050 0.460975 0.019165 -0.170332 0.498910 0.841452 -0.166312 -0.145012 -0.002484 -0.132234 -0.313027 -0.111119 -0.506532 -0.246426 0.063024 -0.100068 -0.454063 -0.122248 -0.230025 0.103971 0.128095 0.292656 0.406275 0.423589 0.191347 -0.110511 0.000504 -0.026837 0.382485 0.259609 -0.117537 0.093771 0.289132 -0.224980 0.274349 -0.757946 -0.183528 0.225914 0.468729 -0.275238 -0.004643 0.192195 0.149541 0.022309 0.218277 -0.214469 -0.002430 -0.559620 0.231478 0.219774 0.005302 -0.155427 0.692871 0.428231 0.081706 0.737968 -0.403235 0.146600 0.129356 0.101519 0.069138 0.172418 0.007719 0.060878 0.041069 -0.408674 -0.198216 -0.057917 -0.339707 0.067514 0.198487 -0.628227 -0.005508 0.147993 -0.076416 0.039022 -0.639436 0.067317 -0.259084 -0.151697 0.845986 0.615992 0.329775 0.206774 0.141044 -0.110340 -0.244149 0.223228 0.299257 0.235531 -0.106955 -0.209712 0.189370 0.090557 0.132024 0.223833\n",
      "200000: kinyarwanda 0.068832 0.333191 0.617240 0.242764 -0.547246 -0.006959 -0.037404 0.759305 0.628935 -0.484478 -0.062862 -0.623375 0.416523 -0.576547 -0.616533 0.309218 -0.248975 0.230643 0.217582 -0.399509 -0.030529 0.294737 0.155174 -0.367455 0.461614 0.314758 0.078722 0.022150 -0.296092 -0.087693 -0.447442 0.223862 0.107352 0.312299 0.529512 -0.549057 -0.292132 0.523338 0.366430 -0.632815 -0.635270 -0.113939 0.384321 0.391653 0.107971 -0.282802 0.259287 -0.769525 0.394919 -0.312925 0.680957 0.156711 0.636350 0.249340 1.051373 0.507676 0.357446 -0.240096 0.220270 0.581209 -0.342025 0.004136 0.172014 0.274669 0.184508 0.142541 0.417325 -0.214674 -0.340535 -0.322786 -0.338047 0.198000 -0.010962 0.056634 -0.851851 -0.143087 0.068219 -0.228012 -0.050094 -0.239690 -0.196017 0.062051 -0.241452 0.214109 -0.262126 0.059850 0.046109 0.072231 0.239343 0.306283 0.171166 -0.131162 0.248779 0.511734 -0.539959 0.005209 -0.282495\n",
      "210000: yari -0.359554 -0.187672 0.073621 -0.007301 0.034121 -0.220385 -0.007698 0.316574 0.365038 0.312091 -0.560505 -0.500676 1.250212 0.399867 0.209190 0.086803 -0.095221 0.196819 -0.040336 -0.401742 -0.577652 -0.526309 0.332751 -0.391141 0.356352 0.138432 0.041003 0.200456 -0.454797 -0.205701 -0.038189 -0.289195 0.296042 -0.277014 -0.289927 0.388388 -0.237546 0.112993 0.216429 -0.097990 0.183937 0.470854 -0.193348 0.008272 -0.909493 -0.384704 0.162540 -0.253599 -0.475768 0.043341 -0.101374 0.051927 -0.044077 0.241425 0.017637 -0.302054 0.349959 -0.229946 -0.255243 -0.281591 0.279239 0.207528 0.276367 0.189739 0.108923 -0.323519 0.506815 -0.331399 -0.090428 0.393962 -0.079584 -0.175928 -0.371465 0.004873 -0.620175 -0.016598 0.077373 -0.003638 -0.121910 -0.143096 -0.898902 0.141240 0.068559 0.368428 -0.444239 0.381319 0.070059 0.225814 0.029083 0.126729 0.706257 0.127499 0.511160 -0.119428 0.164600 -0.076270 -0.198225\n",
      "220000: picotado 0.540003 0.360680 0.265200 0.306756 0.189744 0.001874 0.253917 0.799567 0.065426 -0.442325 0.246925 -0.492314 -0.256731 -0.134688 -0.666541 0.054897 -0.525198 0.090815 -0.098753 -0.294423 -0.427817 -0.025427 0.432921 0.255757 0.256983 0.283052 -0.240436 0.094750 0.135802 0.001146 0.221297 0.181797 -0.489860 0.151744 0.525429 -0.175048 -0.498882 0.641992 0.029313 0.187753 -0.313888 -0.081902 -0.045200 0.366196 -0.290876 0.216560 -0.018521 0.154964 -1.253966 0.065515 0.838650 0.110757 -0.236889 -0.060290 -0.005373 -0.648144 0.193047 -0.009499 -0.111193 0.608113 0.281446 -0.544824 -0.357987 0.282373 -0.289173 0.547020 -0.398873 0.005540 -0.133957 0.617338 0.465609 -0.094941 -0.451673 -0.113574 -0.129050 0.112280 0.462672 -0.353873 0.519152 0.013064 0.022762 0.342591 0.154934 -0.005966 0.265636 0.943738 0.184277 -0.537677 -0.572530 0.498273 0.308108 -0.425318 0.778914 -0.095772 -0.329531 0.569938 -0.036741\n",
      "230000: roberth 0.094334 0.383259 0.498371 -0.011045 0.360410 -0.507211 0.762358 0.528060 -0.433852 -0.580828 -0.144263 -0.147685 -0.003112 0.391080 -0.077511 0.000471 -0.314820 0.298782 0.509215 -0.397173 -0.042294 0.233360 -0.266499 0.727267 0.115097 0.474110 -0.783193 -0.090782 0.382455 -0.966452 0.697856 0.037014 -0.188566 0.422187 -0.466445 -0.228301 0.322776 -0.021106 0.280554 0.561236 -0.482812 0.078808 0.037591 0.361214 -0.344471 0.190359 -0.852402 0.552159 0.131823 -0.210899 -0.142717 -0.429680 -0.449253 0.362513 0.172200 0.183953 -0.412537 0.147382 -0.530084 0.092491 0.155069 -0.307478 -0.549136 -0.351968 -0.156118 -0.178931 -0.127119 0.644704 -0.898117 -0.053696 -0.114515 -0.077191 -0.372481 0.065201 -0.142841 -0.230213 0.185877 -0.248693 -0.253768 0.564855 -0.517895 0.626262 0.847229 0.574013 0.950486 0.270662 0.446972 0.335443 -0.009626 0.127807 -0.606963 -0.519389 0.399204 0.289361 -0.000134 -0.288731 -0.687771\n",
      "240000: illex 0.493787 0.048833 0.395023 0.470583 -0.647781 -0.427668 0.187392 0.574199 0.239852 -0.095775 -0.565850 -0.090807 0.712990 0.276442 -0.421095 0.012431 0.748498 -0.276832 -0.253706 0.105407 -0.052677 0.250139 -0.729349 -0.400980 -0.195918 0.165343 -0.032378 0.056762 0.394984 0.247160 -0.133677 -0.271630 0.511366 0.325282 -0.560990 -0.229859 -0.357045 0.128894 0.917700 0.232419 0.123145 -0.291877 0.162218 0.139235 0.757154 0.307094 -0.325872 -0.317057 -0.063380 -0.009463 -0.243723 0.119247 0.391749 0.062622 0.085125 0.118064 0.548164 -0.414077 0.335237 0.028626 0.405826 0.138350 0.006288 0.172336 -0.224825 -0.339218 -0.087510 0.458682 0.062349 0.106539 0.002709 0.558928 0.562463 0.062258 -1.065087 0.085314 0.676826 0.311392 -0.622843 0.032919 -0.886690 0.148706 -0.709037 1.038079 -0.068857 0.492209 0.155187 0.928219 0.269292 0.265037 0.266187 -0.418175 -0.168431 0.886324 0.129546 0.022624 0.121141\n",
      "250000: og00 -0.388360 0.267061 0.251007 0.182327 0.348247 0.839822 -0.017083 0.411860 0.407853 -0.003355 -0.490890 -0.199242 0.251849 -0.192862 -0.471946 0.331654 0.610247 0.621688 -0.617253 -0.470686 -0.140593 -0.026310 0.009063 0.295633 0.897795 0.432965 0.301107 0.674089 0.025250 -0.174167 -0.768451 -0.086166 -0.210326 0.028260 0.319843 0.446222 0.422934 0.433812 0.814798 0.354835 -0.158882 0.325007 0.074222 0.199781 -0.305942 -0.435733 -0.065629 -0.021948 0.335476 0.190516 -0.811838 -0.056791 0.299247 -0.073283 0.423963 -0.107442 0.466519 -0.736325 -0.107425 0.055008 0.109975 0.184563 0.010104 -0.324391 0.013851 -0.322103 0.051813 -0.439072 -0.340389 0.370888 -0.362104 -0.024906 -0.027923 0.010563 -0.120128 0.406922 0.160891 -1.010076 0.187541 0.519079 -0.087716 0.002350 0.361138 0.242249 -0.420337 -0.082946 0.442559 0.381635 -0.144065 0.236178 0.629065 0.046789 0.110492 1.101507 0.191865 -0.163015 0.336301\n",
      "260000: kalin 0.632525 0.029286 0.357371 -0.180067 0.226793 -0.237621 0.000558 0.300204 -0.430424 -1.133342 -0.243447 -0.104198 -0.265263 -0.212893 -0.204695 0.025994 0.087092 0.514646 0.362223 -0.102265 -0.359953 0.246394 0.179647 0.720077 0.439397 -0.159398 -0.092923 0.211014 -0.382943 -0.083349 0.051238 -0.245345 0.296613 -0.027662 0.601306 -0.022095 -0.594370 0.204796 0.722737 -0.293157 -0.283920 0.218583 0.525357 0.338697 0.004676 -0.381991 0.505427 0.368584 -0.114605 0.731566 0.570005 0.492838 0.077802 0.458387 -0.099789 -0.151120 0.391881 -0.497102 -0.012960 -0.219797 -0.328820 -0.224446 0.390734 -0.315518 0.199687 -0.197170 0.301121 0.531123 -0.027436 0.095019 0.212838 -0.407274 -0.744448 0.024460 0.038378 0.084615 -0.208264 -0.043143 -0.265828 -0.288640 -0.129271 -0.099022 0.507752 -0.141744 -0.462939 0.061489 0.316621 0.095432 0.434385 0.269422 0.370631 -1.003160 0.004175 0.176573 -0.215616 -0.004098 -0.131226\n",
      "270000: autoridadeslocais 0.480283 0.037858 1.131939 0.795989 0.601920 -0.409215 -0.180497 -0.132005 -0.583133 0.030982 0.260823 -0.132874 0.081089 0.230928 -0.231784 -0.359784 0.000602 -0.582051 -0.075651 -0.032319 0.024904 0.200902 0.037860 0.761425 -0.447251 0.336473 0.288367 0.664547 0.515721 -0.087537 -0.170659 -0.296279 0.311263 -0.482024 -0.010662 0.190988 -0.031167 -0.369119 0.421306 0.160622 -0.004914 0.138131 0.365871 0.458784 -0.146775 -0.538305 0.150437 0.258470 -0.863674 -0.135722 0.209756 0.394604 -0.023186 0.108031 0.187062 -0.421985 -0.048065 -0.367313 0.806095 -0.366153 0.213187 -0.312173 -0.029050 -0.051674 -0.478263 -0.500089 0.081699 0.475969 -0.474977 0.250892 0.173464 0.293469 -0.845502 -0.199636 -0.166299 0.264164 0.178534 -0.086232 -0.080581 -0.010847 -0.003289 0.546636 -0.186047 0.174121 -0.105126 -0.014894 0.070372 -0.058018 0.073904 0.642730 -0.639714 -0.569179 -0.358649 -0.018398 0.257821 -0.021015 0.200440\n",
      "280000: goleava -0.050317 -0.325500 0.255600 0.479359 -0.009478 -0.590491 -0.634716 -0.165136 0.614644 -0.263703 -0.165813 -0.133291 0.121582 0.038273 0.269235 0.099263 -0.205161 -0.144277 0.213368 -0.240369 -0.658230 -0.010683 0.313976 0.293803 0.391294 0.473316 0.681441 -0.113449 -0.052637 -0.091457 0.154667 -0.322881 0.479241 -0.195709 0.178489 0.573241 -0.188438 0.347944 0.217866 -0.701762 -0.689085 -0.203718 0.591214 0.070018 -0.263789 -0.317756 -0.349124 0.195019 0.491631 -0.155083 0.083652 -0.356183 -0.188672 -0.342378 0.382971 0.280333 -0.073851 -1.016940 -0.161031 0.121239 -0.287432 0.155220 0.173149 0.425432 -0.032512 -0.449108 -0.559792 0.023662 0.264970 0.419723 0.376557 0.272977 0.097732 0.009806 -0.106812 0.136062 -0.348730 0.241595 0.445371 -0.063739 -0.649301 0.219382 0.465102 0.166128 0.527351 -1.167080 0.373418 -0.247862 -0.393526 -0.265682 -0.489372 -0.498373 -0.020245 -0.211617 -1.227418 0.632025 -0.366677\n",
      "290000: mambos -0.156114 0.352902 -0.225762 0.348487 -0.110348 -0.404350 0.482119 0.952977 0.305731 -0.290086 -0.480502 -0.435587 0.743117 -0.188910 -0.032072 -0.519567 -1.143876 0.377056 0.634002 -0.246166 -0.425816 0.070314 0.207774 0.077237 0.893925 -0.428977 0.129016 0.236335 -0.109252 0.432457 -0.255813 -0.196488 0.084380 0.153238 -0.196785 0.375083 -0.006691 -0.177437 0.891450 -0.544103 -0.400806 -0.436551 -0.445134 0.938690 -0.372465 0.012048 0.165902 0.272496 -0.755453 -0.085455 -0.168420 0.147883 -0.473016 -0.171233 0.576044 0.308156 -0.453177 -0.367124 -0.295835 -0.062292 -0.077075 0.040217 -0.161504 0.392769 -0.801595 -0.329291 0.499475 -0.427003 -0.270390 -0.066482 0.022355 -0.316390 -0.122689 0.519306 -0.219095 0.135286 -0.102102 -0.009307 0.109322 0.135593 -0.032800 0.238715 -0.177233 -0.294523 -0.037039 0.378691 0.079243 0.180108 -0.242048 0.252357 -0.274870 -0.155283 0.364101 -0.094376 0.052471 -0.174385 -0.106047\n",
      "300000: interesado -0.126067 -0.137778 0.533903 0.921008 0.045398 -0.290865 -0.012537 0.353143 1.013967 -0.006021 -0.736080 0.237921 -0.340204 -0.692049 0.245840 -0.332181 -0.224693 -0.052033 0.462205 0.172588 -0.410475 -0.892621 0.335358 0.073422 0.405864 0.187496 -0.485356 -0.094146 0.135959 0.037805 0.182760 -0.214328 -0.219043 -0.566014 -0.133544 0.107056 0.036888 0.410319 0.352776 -0.262162 -0.505477 -0.337762 0.529541 0.476078 0.170085 -0.460515 0.270727 0.073343 0.105136 0.008989 0.471047 -0.185786 0.373857 0.080643 0.294421 0.529464 -0.288061 -0.183747 0.003380 -0.296514 -0.109187 0.023015 -0.174177 0.454242 0.217587 0.925404 -0.082210 0.273172 -0.279299 -0.573777 0.273039 -0.069816 -0.628208 0.249309 -0.473661 -0.119435 -0.482794 -0.525222 -0.100818 -0.070952 -0.024792 0.030368 0.266208 0.418553 0.206502 -0.013177 0.488067 0.069244 0.069530 0.382887 0.401400 -0.247128 -0.179232 0.093082 -0.605223 0.271682 -0.376329\n",
      "310000: cpdlc -0.192489 0.189858 -0.050727 0.318573 0.142511 0.152516 0.139354 0.247034 -0.163526 0.184778 -0.009270 -0.457911 0.040127 0.308113 0.034710 0.131447 0.133207 -0.234526 -0.047610 0.017266 0.042178 -0.128811 0.082949 0.197739 0.063923 -0.113487 0.186805 0.469619 0.014579 -0.323840 -0.153738 0.105538 0.058865 -0.021693 -0.288722 -0.164365 0.051858 -0.046824 0.146953 0.071666 -0.099582 -0.252507 -0.100951 -0.073106 0.045891 0.099110 -0.241115 0.064072 -0.449284 0.092244 -0.035806 0.062779 -0.294781 0.027054 0.140518 -0.318323 -0.093036 0.167215 0.258929 -0.147911 -0.153130 -0.122471 -0.009415 0.022189 0.312456 0.108327 0.018309 0.003974 -0.221725 0.282308 -0.064440 -0.157025 0.135918 -0.167791 -0.070169 0.037028 -0.363447 -0.168151 -0.212649 -0.057067 -0.208640 0.502624 0.060841 0.255322 -0.180431 0.130291 0.139031 -0.106708 0.209342 0.296011 -0.458185 -0.113145 -0.029752 -0.043350 -0.154370 -0.063717 0.046548\n",
      "320000: samenwerkende 0.050740 -0.184333 0.105924 0.227521 0.231181 0.110082 0.129770 0.046354 0.404584 0.049387 -0.146347 -0.098522 0.356667 0.248814 -0.073560 0.211361 0.006854 -0.004988 -0.276260 -0.261307 -0.063642 -0.058839 0.137014 0.154995 0.185463 -0.030301 -0.110399 0.127628 -0.059489 -0.094925 -0.048978 0.382684 -0.075203 -0.259824 0.023478 -0.090217 -0.009940 0.154302 0.088661 -0.278178 0.166438 -0.239189 -0.089483 -0.062565 -0.206903 -0.200381 -0.213192 -0.152727 0.477361 -0.409690 -0.227327 -0.010961 -0.117521 -0.357195 0.123455 0.218487 -0.253526 -0.280998 -0.250309 -0.190659 0.098978 0.108130 -0.046561 0.184432 0.155605 -0.601716 -0.036461 -0.005794 -0.094866 0.332225 0.161836 -0.375821 0.018129 0.054946 -0.017127 0.076088 0.005170 -0.045551 0.135927 -0.327092 -0.647935 0.012221 0.147802 0.086229 -0.132592 0.076245 0.132921 0.199784 0.226297 0.026178 0.122863 -0.581131 -0.118543 0.193602 -0.082819 0.239322 0.031528\n",
      "330000: dimensсo -0.015724 0.033737 0.179546 0.236911 0.014885 -0.201864 0.163302 0.054769 0.288546 -0.074272 -0.321838 0.196531 0.171556 0.061816 0.183771 0.218446 -0.079360 -0.158403 -0.007404 -0.043535 -0.185534 0.200623 0.013286 0.044027 -0.154158 0.023675 0.125439 0.022123 0.005131 -0.411473 0.062296 -0.064627 -0.019194 0.020969 -0.045304 -0.126139 0.013038 0.202400 -0.059942 -0.128248 0.177320 -0.005741 0.398952 -0.307087 0.216299 -0.319398 0.346654 0.142108 -0.140252 -0.102962 0.304020 0.047748 0.149260 0.176935 -0.012812 0.173700 -0.292433 -0.005106 0.062702 -0.145941 -0.153617 -0.048518 0.099919 -0.179036 -0.094493 0.002546 0.084111 0.104296 0.082142 -0.064835 -0.212501 0.008443 0.004233 0.023219 -0.191770 -0.013248 0.270733 -0.275672 -0.147390 0.044807 0.101983 0.109662 -0.072601 -0.026064 0.213319 0.201119 -0.083458 -0.048541 -0.214402 0.043849 -0.038256 -0.060715 0.008549 0.018987 -0.136638 0.020879 -0.038205\n",
      "340000: monteggia 0.038064 -0.208270 0.034283 0.053334 -0.077361 -0.251257 -0.167614 -0.252508 0.091106 -0.003458 -0.229339 -0.050402 0.243025 0.112699 0.194519 -0.347450 0.271894 -0.076631 -0.206438 -0.097618 -0.186536 -0.404451 0.246873 -0.185361 -0.007689 -0.074502 -0.064035 -0.082208 0.115872 -0.186945 -0.076039 -0.324435 0.154255 -0.102624 -0.099159 -0.069416 0.153276 0.230136 -0.200142 -0.309818 0.178677 0.096537 0.185020 -0.357640 0.010155 -0.044706 -0.200764 -0.129790 0.242056 0.209021 0.285434 0.366529 0.031390 0.144876 0.134432 0.155538 -0.085116 0.083816 0.207905 0.261219 0.048962 -0.303912 -0.197762 -0.175740 -0.404823 0.163439 -0.249282 -0.007740 -0.132413 -0.380298 0.028451 0.095330 -0.081577 0.106632 0.115462 -0.197374 0.113782 -0.010744 -0.303095 0.279618 0.053735 0.075888 0.224499 0.252686 0.031798 0.344532 0.257276 0.094483 -0.142979 0.211757 -0.254218 -0.448222 0.342603 0.285556 0.051769 0.242224 -0.391891\n",
      "350000: sangrur -0.014071 0.141495 0.357989 0.223837 -0.002475 0.150017 0.351968 -0.034374 0.139856 0.195995 -0.018679 0.056510 -0.056681 0.069195 -0.100545 -0.110181 -0.021866 -0.106737 0.023071 -0.217182 -0.244688 -0.108403 0.218684 0.051171 0.046467 0.184422 -0.103567 0.179896 0.318104 0.016082 0.482829 -0.247919 0.061017 -0.014982 0.142043 0.311778 0.095511 0.198552 -0.191192 -0.097019 0.092557 0.148154 -0.146154 -0.241333 0.030052 -0.076955 0.007692 -0.151856 0.127576 0.069142 0.222270 0.075126 -0.110796 0.080915 -0.046341 -0.229274 -0.060197 0.331853 0.188161 -0.078374 -0.327220 -0.100772 0.160242 0.020478 0.112662 0.093398 -0.036663 -0.000233 -0.135264 -0.129950 0.001057 -0.005831 -0.042113 0.212283 -0.159444 -0.259450 0.046594 -0.115683 -0.162695 0.216946 -0.008706 0.037605 0.025395 0.015661 -0.048821 -0.102133 -0.041909 0.195184 -0.039263 0.062442 0.215348 -0.165516 -0.000194 0.426840 -0.138853 -0.007079 -0.155910\n",
      "360000: wuncler 0.123842 0.130764 -0.323290 0.041822 0.029945 0.168307 -0.085987 0.033431 0.053429 -0.037039 -0.052351 0.018927 0.014213 0.323874 0.082145 0.079728 0.375176 0.073115 -0.015942 -0.086197 0.004711 -0.008094 0.214023 -0.150857 -0.043132 0.010201 -0.137304 -0.113946 0.230389 -0.085880 -0.113369 0.071405 -0.127439 0.012893 -0.126287 0.169487 0.075205 0.192777 0.265615 -0.062940 0.075696 0.032951 -0.175807 -0.222444 -0.019752 -0.071572 -0.222275 0.076110 0.152164 0.040871 -0.100374 0.177296 0.195982 0.120756 -0.105064 0.387027 0.240290 0.114846 -0.178664 -0.010998 -0.050339 0.076201 -0.029725 0.159439 0.167219 0.159995 -0.013774 -0.224628 0.001555 0.210784 0.187278 0.052163 0.238890 0.278043 -0.457664 -0.034737 -0.093411 0.061908 0.180010 0.287402 0.093048 0.159559 0.201587 -0.073342 0.107943 0.092301 0.491642 0.054330 0.065669 0.112193 -0.255011 -0.259139 0.169698 -0.002090 -0.041527 0.249148 0.054122\n",
      "370000: villaputzu 0.077970 -0.054430 -0.074211 0.268997 -0.137320 0.000329 -0.078299 -0.000485 0.089568 -0.174517 -0.009101 -0.103066 0.078255 0.361800 0.052091 -0.074550 0.056269 0.179379 -0.163814 -0.208727 0.114288 -0.117786 -0.003799 0.040676 0.194344 -0.003363 0.028151 0.086116 0.135624 -0.053147 -0.102147 -0.021784 0.065464 0.089348 0.112311 -0.014236 0.116998 -0.023860 0.112616 -0.127523 0.009732 -0.047027 -0.137853 -0.162373 -0.079783 0.102084 0.069557 -0.074569 0.128928 -0.081869 -0.070744 0.227719 0.118082 0.024528 0.005752 -0.017414 0.129303 0.081881 -0.057322 0.075165 0.008582 -0.040464 -0.034198 0.053595 0.005908 -0.060634 -0.000722 -0.252563 0.092273 -0.033796 0.093596 0.142112 0.062248 0.296936 -0.062378 0.143328 -0.134538 0.005375 0.060888 0.062593 0.024436 0.027587 -0.037398 0.031711 0.049692 0.076412 0.030526 0.125470 0.235173 -0.174232 0.083651 -0.161447 -0.105554 -0.013290 0.031484 0.169950 -0.065372\n",
      "380000: zika.a -0.275437 0.161677 0.105490 -0.266125 0.160099 0.219079 -0.220418 0.066771 0.109668 -0.054115 -0.002705 0.158974 -0.113357 0.063069 -0.029648 0.075104 -0.125926 0.034051 -0.013232 -0.157415 0.012757 -0.054793 -0.346766 -0.131208 0.103124 0.131435 0.165441 -0.107239 0.284922 0.033302 0.207612 -0.119412 -0.142000 -0.226932 0.112170 0.126964 -0.010100 0.150055 -0.025996 -0.011775 -0.099918 -0.111894 0.196258 -0.111255 -0.354118 -0.189497 -0.168451 0.323069 -0.110478 -0.226194 -0.461459 -0.015893 -0.157735 0.116866 0.261225 -0.043397 0.081027 0.151680 0.116646 -0.240504 -0.272956 0.023383 0.182609 0.413446 0.102050 -0.226643 -0.099394 0.148979 0.065777 0.172011 -0.083949 -0.579589 0.310742 -0.132813 -0.268418 0.323249 0.099868 -0.432001 0.199940 0.005951 -0.287986 0.197762 -0.002005 0.400141 0.054709 0.045763 0.148367 -0.125212 -0.211122 0.232820 -0.210880 0.252925 0.244095 0.482076 0.226048 -0.425408 -0.182823\n",
      "390000: salvares 0.046623 -0.188223 0.022046 -0.073200 0.268599 0.177337 0.006073 -0.005776 0.210968 -0.065202 -0.053712 0.000730 -0.072969 0.178655 -0.030250 -0.155295 0.125656 0.136138 -0.178794 0.014866 0.140484 -0.087383 0.166728 0.028790 -0.115580 0.054787 -0.016719 -0.084605 -0.036313 -0.048614 0.074644 0.243750 0.111659 -0.046527 0.076808 -0.112940 -0.091867 0.493967 0.003826 -0.015785 -0.176791 0.182393 0.533896 0.091944 -0.210218 0.122230 -0.116166 0.045177 -0.266479 -0.118868 -0.023963 0.139722 0.057384 0.045408 0.153981 0.166002 -0.222753 -0.184131 -0.171217 0.447055 -0.159837 0.193872 0.053777 0.180521 0.057905 -0.044849 -0.151341 0.182566 0.026826 0.401026 0.102672 0.043064 0.116528 -0.061780 -0.125562 0.119663 0.280820 -0.101638 0.124879 0.064097 0.163354 0.280329 0.041593 0.051538 -0.010570 -0.049655 -0.223748 -0.204587 0.081806 0.033938 -0.134765 -0.037249 0.374230 -0.028318 0.221940 -0.068185 0.062160\n",
      "400000: panik 0.012698 -0.131221 0.251772 0.007560 0.154450 0.299549 0.010652 0.066770 0.255506 -0.076232 -0.128468 -0.011188 -0.095061 0.121190 0.113420 0.116551 -0.074862 0.010026 0.009067 -0.017066 -0.086466 0.170832 0.033828 0.393689 0.003811 0.121217 0.183254 0.150476 -0.032367 0.120770 0.028403 0.234481 0.229340 0.128415 0.035439 0.142937 0.121850 0.045144 0.007773 -0.014065 -0.264841 0.035147 0.081232 -0.103068 0.047117 0.170447 -0.179164 -0.210495 0.098189 -0.011181 -0.166897 0.057485 0.179829 -0.016708 -0.139932 0.282689 -0.003339 0.134999 -0.041717 0.084787 -0.029933 0.194076 0.100138 -0.066978 0.077409 0.157378 0.222341 -0.134647 -0.124275 0.043634 0.134231 -0.173051 -0.038514 0.080975 -0.266605 -0.037692 -0.067429 -0.179702 0.095011 0.117530 -0.066253 0.342024 0.183863 0.167347 0.282614 0.251552 0.275587 0.380843 0.065802 -0.112069 -0.107467 0.113597 0.069754 0.129712 -0.045741 0.006380 -0.008512\n",
      "410000: hh000 0.006403 0.172270 -0.229853 0.210855 0.031059 0.178267 -0.085586 0.263274 0.082152 -0.168114 -0.146658 -0.088790 -0.082540 -0.129008 0.202511 0.255979 -0.041591 0.153652 -0.184562 -0.223857 -0.142350 -0.154413 0.071025 -0.012351 0.385851 0.072802 0.044332 0.120242 0.199071 0.103332 -0.041007 -0.012154 -0.072932 -0.037107 0.206036 0.371781 0.300088 0.001376 0.097871 0.031238 -0.237790 0.127490 -0.124528 -0.101935 0.074487 -0.066416 0.064397 -0.086144 0.197826 0.038139 -0.502051 -0.101259 0.264741 -0.057004 -0.010937 -0.168141 -0.065020 0.017659 -0.001790 0.034535 0.024443 -0.102659 -0.201399 -0.228405 0.060017 0.363838 0.095862 -0.138425 0.162034 0.031788 -0.108060 0.107762 -0.284363 -0.139177 -0.174294 0.088558 0.050895 0.027581 -0.105503 0.037758 0.018602 0.093514 0.134876 -0.002220 -0.049616 0.032265 0.123635 0.233450 0.105166 -0.052620 0.195675 0.057095 0.186685 0.338918 -0.080367 -0.227603 -0.076230\n",
      "420000: boggies 0.183651 -0.067413 -0.134772 0.018371 -0.164312 -0.157672 -0.066159 -0.069804 0.172649 -0.188330 -0.165531 -0.040466 -0.046357 0.015653 0.075998 0.181721 -0.002412 -0.065431 0.088887 -0.108326 -0.004969 0.179075 0.088962 0.159021 -0.002554 0.018226 0.185670 0.246358 -0.153659 -0.072491 -0.046753 0.036544 -0.060378 -0.125661 -0.236931 0.086021 -0.083079 0.006755 0.479350 0.040123 0.019814 0.021363 0.237905 0.141388 0.099598 -0.044357 -0.321142 -0.190610 -0.181018 0.110514 0.003354 -0.106226 -0.135882 -0.122864 0.085985 -0.012309 -0.056069 -0.138775 -0.248051 0.044731 0.108786 0.058392 -0.111991 -0.140517 0.170173 -0.080021 0.206159 0.058004 0.021497 0.184516 -0.019724 -0.312252 0.110855 0.020631 -0.217916 0.087529 -0.224615 0.200528 0.002292 -0.073838 -0.048129 0.031226 -0.243074 0.032243 0.062804 -0.060527 0.031956 -0.280269 0.433876 -0.156437 -0.039378 -0.239523 0.025817 0.140880 0.053564 -0.087964 0.071973\n",
      "430000: super-licença 0.129807 -0.154552 -0.157084 0.133643 0.216188 -0.087745 0.104344 -0.001243 -0.049932 -0.141363 -0.149349 0.112071 -0.100941 0.297236 0.102103 0.074237 -0.052830 -0.034882 -0.122997 0.121864 0.087492 0.110161 0.098655 0.157444 -0.132337 0.390999 -0.163501 -0.048450 0.106802 -0.185649 -0.145620 -0.069334 -0.193603 -0.106320 -0.032735 -0.128813 0.075830 0.091218 0.002676 0.068790 -0.039549 -0.066235 0.069234 -0.237052 -0.170283 -0.299815 -0.162430 -0.110180 -0.340674 0.122905 0.044750 0.095585 0.068350 0.166414 0.117497 0.201143 -0.387312 0.153542 0.096275 0.128070 -0.212628 -0.088993 0.450004 0.040236 0.259812 0.211718 -0.035928 0.187942 -0.136729 -0.133163 0.026896 -0.252029 0.194632 0.190664 -0.151326 0.127054 0.295451 -0.217882 -0.041317 0.003313 -0.154437 0.134844 0.202690 -0.173720 0.127025 0.101357 -0.250822 -0.067627 0.085397 -0.006977 0.176772 0.039829 0.170075 0.175981 0.308417 0.073267 -0.009547\n",
      "440000: imeadiato 0.317194 -0.229641 -0.008629 0.103331 0.105961 0.136124 -0.022590 0.012803 -0.018400 -0.142842 0.285862 0.008461 0.348251 0.276892 -0.121385 -0.167612 0.052809 -0.110550 -0.159814 0.103360 0.157630 0.018733 0.105702 -0.220793 0.120964 0.081449 0.000460 -0.232613 0.323238 -0.107930 0.076632 -0.186626 0.171348 0.059975 -0.186172 -0.020709 -0.269772 -0.108763 0.189717 -0.282706 -0.247234 -0.093620 -0.234597 -0.184921 -0.081910 -0.264210 0.220717 0.183917 -0.418753 0.240736 -0.093925 0.144082 -0.115020 -0.046689 -0.016207 0.091211 0.218859 0.082836 0.187275 -0.204650 -0.019502 -0.074518 0.564712 -0.224419 -0.052274 0.054972 -0.115412 0.019422 -0.198551 -0.168892 0.119502 -0.052906 0.056240 -0.209354 -0.072868 -0.043251 0.463929 -0.443194 -0.604885 0.211728 -0.102272 0.661909 -0.023036 -0.063816 -0.050074 0.038623 0.018807 -0.025146 0.116184 0.185419 0.189209 -0.159496 -0.000338 -0.139387 0.293831 -0.513487 -0.031383\n",
      "450000: ad-libs 0.339333 -0.032567 -0.128122 0.289047 0.003069 -0.041691 -0.298054 -0.019660 0.096344 -0.028544 -0.188380 0.106275 -0.247779 0.029195 -0.120413 -0.188844 0.042348 -0.034008 -0.054552 -0.046032 0.127540 0.038325 0.275356 0.160861 0.055973 -0.139478 -0.022357 0.194026 0.074338 -0.068613 0.102901 -0.143149 0.172685 0.069957 0.100292 0.023547 -0.072125 0.067416 0.108455 -0.193332 0.078916 0.085820 -0.145487 -0.069379 0.097164 0.026527 -0.040165 -0.246985 -0.030744 0.137755 0.154715 0.151989 -0.131730 -0.054796 0.116457 0.077035 0.013369 -0.092260 -0.024892 -0.177376 0.003298 -0.001719 0.039199 -0.157203 0.182485 0.087230 -0.013478 -0.125670 -0.000712 0.290868 0.026745 -0.046743 0.013982 0.014366 -0.038964 -0.144443 0.042460 -0.184420 -0.047707 -0.255806 0.004511 0.331349 -0.132925 -0.021154 -0.039145 0.278975 -0.201777 0.000964 0.213023 0.018236 -0.083866 -0.004464 0.340074 -0.016450 -0.131929 -0.283723 0.097535\n",
      "460000: niinimaki 0.101234 -0.029993 0.188403 0.044599 -0.065070 0.101041 -0.435728 0.309256 0.095694 -0.592555 0.127863 0.031865 -0.054557 -0.136717 -0.163675 0.154105 -0.073484 -0.028698 0.095913 -0.060193 -0.127697 0.220100 0.294509 0.085314 0.214808 0.065775 -0.144400 0.104865 0.073327 -0.004576 0.248261 0.120101 -0.024743 0.039852 0.335391 0.112460 -0.046882 -0.047081 0.115565 -0.328373 -0.199766 -0.038038 0.113356 0.206757 0.104144 -0.216043 0.186485 -0.159894 -0.006014 0.303352 -0.177669 -0.004012 -0.083369 -0.000600 -0.029703 -0.146768 0.304649 0.026529 -0.003200 0.203314 -0.383214 -0.234546 -0.250002 0.159310 0.274414 -0.004573 -0.024303 0.060251 -0.015690 -0.204096 0.287374 -0.237542 0.148375 -0.125200 0.003954 -0.256397 0.113484 0.320879 -0.088359 -0.045577 0.066615 0.148913 0.253650 -0.214479 -0.033173 0.082467 -0.205455 -0.167189 0.131117 0.230194 0.353104 -0.329234 0.080819 0.136186 0.038884 -0.094836 0.062637\n",
      "470000: chhu 0.131076 -0.052601 0.004000 0.227890 -0.085503 -0.010751 -0.065877 0.058722 -0.056973 -0.356083 0.028028 -0.015122 0.115990 -0.040819 0.129832 0.133211 0.050000 0.204121 -0.147240 -0.427746 0.093335 -0.026887 -0.042675 0.292275 0.001152 -0.045966 -0.025417 0.033721 0.112897 0.028685 0.062139 0.184480 -0.100068 0.299857 0.058192 0.017996 0.044437 -0.075308 0.117572 0.031972 0.096760 -0.142903 -0.142608 0.052145 -0.203227 0.034884 -0.180220 -0.120832 -0.112002 -0.114560 0.207724 0.261198 -0.212287 -0.156613 -0.072993 0.081798 -0.102961 0.131381 0.088243 0.153059 0.142138 0.113186 -0.003415 0.157908 0.241847 -0.031295 0.207286 -0.139060 0.070888 0.013257 0.081655 -0.043410 -0.117741 0.164701 -0.245938 0.031160 -0.117233 -0.106133 0.002719 -0.123541 0.043349 0.204287 0.077394 -0.079398 0.010930 0.195153 0.045602 -0.026168 0.142509 -0.076352 0.262909 -0.454563 0.021073 -0.010541 0.157588 -0.106050 -0.118253\n",
      "480000: neuropáticas -0.009727 -0.072689 -0.007795 0.333521 0.282316 -0.007513 -0.219050 -0.233028 0.164820 -0.004404 0.010174 0.145487 0.214255 0.396714 -0.183137 -0.009924 -0.007360 -0.122292 -0.191360 0.337182 0.103829 -0.163581 0.182306 -0.006254 -0.029434 -0.157490 -0.015754 0.343682 0.477648 -0.102914 -0.009314 0.063471 0.150641 0.054338 -0.086698 -0.010674 -0.024147 -0.225356 0.005879 -0.289110 -0.062744 0.433694 -0.113815 0.194088 0.120595 0.007466 0.062600 0.134187 0.184107 0.078321 0.070621 0.033395 0.158346 0.164351 -0.060216 0.190163 0.096079 -0.028011 0.094258 0.469922 0.109295 -0.168669 -0.035920 -0.087914 -0.260306 -0.067922 0.101075 0.053349 0.147080 0.178156 0.093519 -0.110538 0.392330 -0.133889 -0.181462 0.182011 0.063128 0.026712 0.081817 -0.116658 0.146742 0.142008 -0.066413 0.247773 -0.180763 0.221781 0.288489 0.133185 0.180281 0.298528 -0.198708 -0.233619 0.117419 -0.383371 0.148733 0.127381 -0.020964\n",
      "490000: atufando-se 0.032419 -0.091326 0.072138 0.238049 0.062639 -0.133388 -0.088293 -0.111986 0.051924 -0.134417 0.061705 0.447436 0.299037 0.019672 0.049358 -0.455655 0.185185 0.088058 0.129384 -0.045269 -0.086724 -0.023611 0.032976 0.158281 0.139304 -0.034377 0.068099 0.020804 0.293314 -0.121337 0.245225 -0.131492 0.124016 0.075516 -0.018440 -0.178123 -0.078724 0.200029 -0.228100 -0.213741 -0.018724 -0.029582 -0.201285 0.242828 -0.080113 -0.021121 0.164631 -0.212754 -0.160701 -0.040632 -0.032767 -0.039267 -0.016186 -0.311416 0.095163 -0.070217 -0.066986 0.009808 0.079000 -0.096682 -0.031220 0.006679 -0.138718 0.139761 -0.109693 0.140653 0.068333 0.156059 0.010375 0.017211 -0.172362 0.080031 -0.217238 -0.111991 -0.204758 -0.031428 -0.100195 -0.119790 0.150666 0.291988 -0.066938 0.306440 0.067578 -0.220697 -0.118915 -0.239116 -0.050718 0.141330 0.249700 -0.004526 -0.094851 -0.219480 0.071616 0.154788 -0.176152 0.081947 0.051067\n",
      "500000: megaigrejas 0.007002 0.024366 -0.048059 0.340552 0.195004 -0.073308 0.085270 -0.048673 0.115492 0.009602 -0.191586 0.101344 -0.014003 0.078859 -0.213688 -0.250968 -0.095625 -0.264436 0.139040 -0.113709 0.175127 0.033983 0.162159 0.223489 0.187610 -0.256161 -0.166292 -0.092117 0.189649 -0.032305 0.047227 -0.223164 -0.044359 -0.116433 -0.167204 -0.032254 -0.248418 0.133052 0.100132 0.125114 -0.103002 0.035511 0.034712 -0.062532 0.025151 -0.133754 0.169655 -0.075828 -0.041226 0.218846 -0.193483 -0.227929 0.047524 -0.166554 0.156026 0.080502 -0.010159 -0.067703 -0.135522 -0.154583 0.025162 -0.017330 0.050655 0.034890 0.150096 0.054367 0.036702 -0.015615 0.115386 0.043478 -0.172517 -0.071988 0.085735 -0.004228 -0.036201 0.012575 -0.087869 0.061042 -0.047047 -0.046699 -0.176126 0.286520 -0.031759 0.145616 0.018427 -0.034868 0.068327 -0.199344 0.383125 0.227115 -0.034750 -0.117357 0.079798 -0.135539 0.104284 0.026995 0.098201\n",
      "510000: analisávamos 0.147057 -0.010991 0.000151 0.370986 -0.076869 -0.087680 -0.206091 -0.086800 -0.165874 -0.042166 -0.021110 0.003278 0.190731 -0.121265 0.229193 0.024991 -0.074200 0.128076 -0.096631 -0.003791 0.198623 0.105512 0.095861 0.171195 -0.032485 -0.223527 -0.170361 -0.274489 -0.100210 -0.111171 0.184556 0.260444 0.238682 -0.138251 -0.191210 0.106402 -0.110171 0.031045 0.267092 0.091235 -0.046287 0.090250 0.091189 0.386335 -0.077453 0.102296 -0.022319 0.100559 0.066165 -0.127813 0.147951 0.130271 0.069526 -0.207666 0.167169 0.099493 0.079395 -0.054028 0.214866 0.204712 0.007473 -0.207383 0.126040 0.024296 0.078727 -0.016883 0.185248 -0.111299 -0.110134 0.043086 -0.236245 0.105457 0.036598 0.035013 -0.190204 0.203217 -0.064978 -0.160955 -0.048833 0.041206 0.041573 0.180714 -0.186969 -0.150145 0.183423 -0.009431 0.062552 0.184509 0.117702 -0.168500 -0.083464 -0.155409 -0.008895 -0.170121 -0.034621 0.208969 0.065869\n",
      "520000: gitaigo 0.089956 0.052258 -0.039849 0.175159 -0.081032 0.009561 0.139037 0.076419 -0.075995 -0.204961 0.117128 0.121450 -0.080629 0.062526 0.355734 0.196939 0.072218 0.298313 -0.002120 -0.141413 -0.131382 -0.131757 0.008545 0.170318 -0.072399 -0.069461 -0.043393 -0.074506 -0.125703 0.021225 0.108531 0.109115 0.147248 -0.030870 0.189910 0.127257 -0.151821 0.095361 0.025679 -0.008778 -0.133589 0.025687 0.088516 -0.005118 -0.000996 0.108397 0.002967 0.026992 0.120474 -0.107644 -0.175479 0.066636 -0.010559 -0.025338 -0.102818 -0.021485 -0.045391 0.146412 0.021955 0.175281 0.091857 0.071362 0.126822 -0.121721 0.168464 0.001962 0.004736 -0.055495 -0.118906 -0.057926 0.067516 -0.108727 0.084861 0.097886 -0.250491 -0.143486 -0.003459 -0.109709 0.088249 0.057465 0.010820 0.232608 -0.025952 0.070780 -0.005625 -0.049727 -0.045108 0.114168 0.102307 -0.034903 0.220147 0.015741 -0.043386 0.053153 -0.021956 -0.166402 -0.042571\n",
      "530000: quichua 0.099499 0.070636 0.210806 0.160292 -0.157751 0.018518 0.241453 0.081006 0.116164 0.027519 -0.045104 -0.036715 0.207922 0.125706 0.068594 -0.110535 -0.230271 0.103446 0.086479 -0.150764 -0.244782 0.140164 0.006342 0.035373 0.138583 -0.030985 -0.025404 0.150493 0.146594 -0.195753 0.279099 -0.023295 0.149710 0.164309 0.079008 -0.073623 0.098246 0.267332 0.007624 -0.049608 0.012852 -0.156373 -0.014036 0.119283 0.228991 0.049621 0.084725 -0.260963 -0.020389 -0.082930 -0.044007 0.007037 -0.012278 0.021486 0.037691 0.255842 0.041119 -0.038362 0.099705 -0.041101 -0.220432 -0.003699 0.009626 0.189748 -0.011723 -0.149348 0.173278 0.118823 0.163182 -0.231153 -0.010616 -0.013834 -0.085248 0.170531 -0.243736 0.044881 0.020243 0.026927 -0.012723 0.259636 0.270777 0.130680 0.094665 0.093642 -0.051735 -0.015367 -0.097809 0.190489 0.433615 0.127775 -0.145612 -0.112799 -0.205321 -0.003225 -0.041274 -0.324903 -0.248379\n",
      "540000: baiocchi 0.088746 -0.163023 -0.042267 0.220802 -0.020361 -0.321087 -0.043999 0.002156 -0.149046 -0.078383 -0.135904 0.038851 0.099295 0.036105 0.008945 0.009793 0.000473 -0.035759 -0.185666 -0.222667 0.176809 -0.082944 0.193469 0.136944 0.173324 -0.036766 0.000262 0.006950 -0.016851 -0.207309 -0.004247 0.032606 0.123631 -0.275595 -0.133147 0.098603 0.050113 -0.018359 0.217604 -0.218725 -0.054212 -0.038658 0.250296 0.038045 0.020975 0.068430 0.006498 -0.161771 0.248308 0.232299 0.144613 0.035460 0.179405 -0.018392 0.096010 0.091221 -0.188288 -0.260921 -0.125885 -0.015282 0.136253 0.219589 0.027438 0.071636 -0.044808 0.125033 0.011667 0.173785 -0.113639 0.171564 0.003364 -0.200951 0.029745 0.034312 -0.114218 -0.078803 0.115507 -0.133945 0.189009 0.034071 -0.014809 0.235512 -0.010121 0.169373 0.046846 0.142030 -0.073895 0.208496 -0.007084 0.052872 -0.125644 -0.062215 0.092065 0.018459 -0.008665 0.136841 0.035500\n",
      "550000: jeder 0.143670 0.129596 0.040133 0.329226 -0.051128 -0.111039 -0.066223 0.054449 0.026871 -0.068423 -0.084907 -0.038140 0.026499 0.203268 0.268472 -0.105550 -0.114310 0.073518 -0.079248 -0.150017 -0.142899 0.196188 0.013720 0.008884 0.083014 0.075767 -0.095344 0.126119 -0.250418 -0.055986 0.097752 -0.045665 0.093407 -0.037353 0.119975 -0.059431 0.067325 0.085920 -0.111715 -0.181404 0.053114 0.128807 -0.170339 -0.027881 -0.122556 0.027150 0.012968 0.164710 0.093514 -0.055700 0.060705 0.042095 0.063072 -0.271931 0.155680 0.069566 0.108289 -0.204962 0.224892 -0.066249 0.193365 -0.044628 -0.011014 -0.033180 -0.028967 -0.137664 0.126798 -0.046197 0.081755 0.011339 0.117581 -0.178643 -0.000746 0.037444 -0.237496 0.059628 -0.137117 0.086928 0.194288 0.050842 -0.085942 0.180122 -0.150001 0.120363 -0.075347 0.068564 -0.011880 0.188046 0.281035 -0.033515 0.004057 0.018969 0.108400 0.183234 -0.024564 0.149911 -0.106399\n",
      "560000: tadros 0.101351 0.010606 -0.050654 0.044245 -0.201579 0.027686 0.097595 0.003576 0.060975 -0.300008 -0.052659 0.241864 0.117574 0.077990 0.180343 -0.105442 -0.037061 0.094943 -0.095051 0.016232 -0.253112 0.060694 0.157563 0.048794 0.185526 -0.000592 -0.109663 0.095808 0.181316 -0.471818 -0.130934 0.067032 0.039759 0.154980 -0.174969 0.134037 -0.073830 0.136265 -0.036719 -0.233485 0.191631 -0.069904 0.151765 0.151586 -0.088060 0.096248 -0.157666 0.160400 0.051979 -0.006598 -0.178247 0.162562 0.157405 0.014375 0.132218 0.112228 0.177777 0.124729 0.095053 0.220624 -0.112325 -0.004298 -0.111025 -0.013080 0.037345 -0.024399 -0.275771 0.091557 -0.094857 0.002703 0.247961 -0.096377 -0.092500 0.176220 -0.058445 -0.240598 -0.219639 -0.046145 0.025120 -0.132394 -0.285985 0.123508 0.029545 -0.111945 0.221665 -0.016098 -0.045480 0.073337 -0.004213 -0.119493 0.134610 0.051661 0.067658 -0.110618 0.007708 0.202663 0.012308\n",
      "570000: celebrou-a -0.088429 -0.133158 -0.060298 0.170486 0.113993 -0.114130 -0.047013 0.065241 -0.051083 -0.346968 -0.284916 -0.049057 0.077030 -0.331528 0.147815 -0.016192 -0.166123 0.102897 -0.153861 0.007547 0.014860 -0.138897 0.056218 -0.110405 0.098933 0.093772 -0.045881 0.039438 -0.132366 0.084134 -0.131938 0.061774 -0.102030 -0.100149 -0.056706 -0.018372 0.010719 0.131681 -0.123849 -0.095307 -0.186200 0.064583 0.065057 -0.063913 -0.117033 -0.282461 0.059826 0.008806 0.136992 -0.060184 0.066056 0.222493 0.001284 -0.179596 -0.071672 0.140803 -0.046187 0.045473 0.262678 -0.174491 0.068386 -0.121780 -0.042572 0.066964 -0.094844 0.072229 -0.068392 -0.149234 -0.047041 0.139623 0.206900 0.040808 -0.102297 -0.006124 -0.274002 0.097207 0.235803 0.035886 0.065184 0.007375 0.007020 0.180018 -0.101213 0.018563 0.183590 0.002424 -0.147233 0.120535 0.068294 0.112853 -0.037264 -0.276886 0.208489 0.082470 -0.188434 -0.123092 0.033137\n",
      "580000: hep-ph/0000000 0.306684 0.149756 -0.222161 0.081866 -0.341437 -0.115850 -0.326899 -0.019864 -0.163551 -0.280220 0.003726 0.174407 -0.366831 0.145018 0.124313 -0.222954 0.049805 0.090469 -0.320991 0.097774 0.116850 -0.039013 0.352578 -0.012055 0.201256 -0.169313 -0.198954 -0.085880 0.232913 0.133664 0.083228 0.036069 -0.014991 0.277558 -0.156174 -0.195198 0.013470 -0.081537 -0.221780 0.080644 -0.206077 -0.031876 0.093429 -0.071448 0.145621 0.200143 0.083748 0.016827 0.005755 -0.041035 0.091462 0.033091 -0.362655 0.220987 0.270206 0.120827 0.161666 0.246901 0.304897 -0.175190 -0.040032 0.185866 0.101673 0.095956 -0.169637 -0.026408 -0.349792 0.069927 0.201213 0.035680 0.092278 -0.306290 0.037441 0.034384 0.006777 0.061543 -0.203002 -0.051764 0.169610 0.185874 -0.051490 0.114324 0.135926 0.156642 -0.111852 -0.159935 -0.012200 0.137912 0.023559 -0.190136 -0.058786 0.109422 -0.215689 0.115264 0.120399 0.101852 -0.081169\n",
      "590000: palmview 0.066460 0.009855 -0.171019 0.137715 0.088024 0.014998 -0.025935 -0.010082 0.125547 -0.010903 -0.223111 0.157561 -0.225597 -0.027407 -0.119183 0.033127 0.144912 0.100480 -0.078419 -0.120749 0.055487 -0.015229 0.259299 -0.044818 -0.011705 0.008831 -0.111854 0.140082 -0.109968 -0.068664 0.007728 0.144806 -0.159996 -0.088182 -0.123190 0.102656 0.126653 0.059512 -0.035436 -0.024301 -0.275690 -0.181753 0.015282 -0.073769 -0.036525 -0.142986 -0.003445 -0.398723 0.114559 0.147476 0.017418 -0.037885 0.031164 -0.062437 0.167290 0.027073 -0.072584 0.178639 0.183784 -0.015510 -0.072037 -0.057311 0.042740 -0.123392 0.004744 0.196581 -0.093243 -0.016517 -0.004932 0.159297 0.130410 -0.098371 0.115131 -0.188700 -0.150630 -0.016587 -0.087866 -0.127143 0.073942 0.162006 0.061053 0.164089 -0.030595 0.194644 -0.145716 0.160331 -0.047372 0.075560 0.159232 0.160833 0.072991 -0.086750 0.115979 0.103116 -0.119760 0.035564 -0.013339\n",
      "600000: tuyakbay -0.047518 -0.089841 -0.158598 0.173207 -0.069695 0.104247 -0.224498 0.116737 -0.098061 0.020079 -0.118714 -0.145840 0.145236 -0.101207 -0.015460 -0.031379 -0.161890 -0.088284 -0.003942 -0.108539 0.105187 -0.056481 0.014494 0.158734 0.315462 0.155683 0.035435 -0.050030 0.108087 -0.134116 -0.099482 0.129151 0.005780 0.095587 -0.015585 0.100686 0.218078 0.030726 0.222498 -0.010193 -0.095015 0.040909 -0.019727 0.152106 -0.056886 -0.312321 0.025944 -0.131443 0.039069 0.001409 0.041858 -0.045838 0.215734 0.036866 0.168610 -0.047144 -0.118076 0.093587 0.067270 0.087476 -0.060781 0.158064 -0.121223 0.017665 -0.045096 0.159778 -0.045604 0.024945 0.135233 0.079980 -0.209421 -0.056894 -0.026818 -0.132986 -0.241399 0.068978 -0.214269 0.032016 -0.026840 0.097730 0.097573 0.300233 0.121709 -0.026943 -0.027806 -0.024037 -0.039592 0.209182 0.158004 0.028734 0.219765 -0.140259 0.150188 -0.168793 0.004869 0.018594 0.090054\n",
      "610000: comapny 0.175074 0.065162 -0.058060 0.103152 0.093239 -0.026078 0.053954 0.163486 0.007045 -0.040888 -0.069072 -0.101304 0.033996 0.066443 0.084941 0.151668 0.020842 0.131124 0.043985 -0.142455 0.010435 0.079099 0.103094 0.176427 0.058093 -0.074075 -0.119038 0.110105 -0.074108 -0.265216 0.016141 0.048179 -0.120290 -0.010337 0.021527 0.182050 0.114033 0.024990 0.125631 -0.043596 -0.099866 -0.033019 -0.003568 0.054256 0.062902 0.116523 0.100797 -0.100657 0.069034 -0.120231 -0.070166 -0.063885 -0.037145 0.087674 0.191470 0.069916 -0.065579 -0.085635 -0.053898 -0.017851 -0.044028 0.145963 0.085017 0.049573 0.066885 0.055579 -0.082441 -0.177884 0.067694 0.100915 0.018151 0.054404 0.012588 -0.022323 -0.416984 -0.132810 0.027313 0.018828 0.030974 0.047120 -0.058882 0.374275 0.135680 0.070617 -0.016953 0.100922 -0.000500 0.039507 0.071092 -0.197653 0.271380 -0.132278 -0.047486 -0.068797 -0.007921 0.004831 0.146243\n",
      "620000: júnior. 0.095124 0.140963 -0.103293 0.029032 -0.070555 0.160199 -0.113735 -0.099221 0.017763 -0.186724 -0.072090 0.062132 0.144376 0.199490 0.146704 0.064366 0.182801 0.069037 0.010598 -0.138815 -0.005728 0.020216 0.303447 0.197872 0.032037 -0.034852 -0.192752 0.014209 0.110130 -0.107433 0.000494 0.044224 0.066637 -0.019015 0.125754 0.012156 0.141595 0.045341 0.011597 0.027558 0.008847 0.057835 0.116765 -0.117940 -0.021289 -0.020915 0.047314 -0.151093 0.047439 0.031870 -0.199972 0.073976 -0.103423 -0.035800 -0.062121 -0.066857 0.040574 0.166263 0.040589 0.109421 -0.077287 -0.117239 -0.040699 -0.039137 -0.047923 -0.004784 0.117382 -0.160541 0.035290 0.076755 -0.042894 -0.052965 -0.053912 0.033443 -0.166733 -0.001205 0.045499 0.159143 0.050388 0.143104 -0.144946 0.125563 0.020512 0.028923 0.040597 0.072681 0.056762 0.144289 0.202421 0.055249 0.206335 -0.191067 0.172566 -0.067899 -0.014253 0.052284 -0.064332\n",
      "630000: reptiliomorfos 0.109843 0.201175 -0.099597 0.093905 -0.039445 -0.006968 -0.025982 0.066050 0.047765 -0.045482 -0.207684 0.094110 -0.012836 -0.016154 0.206933 -0.017259 0.077121 -0.027011 -0.030607 -0.008661 0.347503 0.001244 0.129894 -0.074740 0.180013 -0.062126 -0.071743 -0.142638 -0.131540 -0.010770 0.049926 0.033577 -0.055717 0.127413 -0.233791 0.208483 0.027038 0.155731 0.093510 0.056706 -0.057099 0.017491 0.048440 0.083053 -0.037097 -0.040605 -0.084001 -0.031291 0.073529 -0.126414 0.025287 -0.012809 -0.189172 -0.096334 0.056754 -0.154753 0.057293 -0.113900 0.183688 -0.028221 -0.021438 -0.051825 0.122909 0.240938 -0.137338 0.076279 0.138097 -0.130820 -0.103420 0.054473 0.016492 -0.162468 0.065774 -0.044307 -0.222121 0.042576 -0.153886 0.090097 0.038954 0.110035 0.062087 0.344376 -0.032715 0.014266 0.008613 -0.067410 0.164848 0.025722 0.046317 0.092161 -0.028345 -0.177038 0.070540 -0.015215 0.293692 0.126023 0.005625\n",
      "640000: aglonas 0.083063 -0.050166 -0.017672 0.259632 0.049153 -0.143903 -0.047287 -0.038292 0.007905 -0.128171 -0.165845 -0.049650 0.010513 -0.082329 -0.014677 -0.043587 -0.086320 -0.063024 -0.015383 -0.062086 0.030298 -0.046482 0.011752 0.103244 0.009831 -0.111217 0.006085 0.001787 -0.020069 -0.090965 0.115047 0.109834 0.012412 0.010222 -0.028186 0.045236 0.059332 0.043041 0.174215 -0.008560 -0.047324 -0.000676 0.098030 0.113163 0.023805 0.085749 0.044193 -0.010195 0.065015 -0.037142 0.090231 0.041639 0.049633 -0.015770 0.079755 -0.058545 0.002252 -0.062569 0.018728 0.051963 0.033930 0.038107 0.010925 -0.007280 0.045676 0.040120 0.075676 0.045487 -0.178568 0.080550 0.074402 -0.051525 -0.067884 -0.017050 -0.149716 0.094068 -0.118133 -0.066158 -0.051973 -0.073622 0.063109 0.177062 -0.095467 -0.028095 0.165270 0.029412 0.004014 0.062518 0.011234 0.053774 0.002351 -0.175545 0.154111 -0.057776 -0.012965 0.058235 -0.056045\n",
      "650000: coloniaes 0.146792 -0.032462 -0.159953 0.266006 -0.103238 -0.074234 -0.075254 -0.002063 -0.119008 -0.013765 0.010750 0.381996 0.293601 0.006431 0.075179 -0.043876 0.007183 -0.031722 -0.216343 -0.046887 0.003015 -0.146148 -0.112689 0.133358 0.024305 0.145625 0.243316 0.062213 0.059942 -0.100463 0.345537 -0.181341 0.085435 0.247732 -0.106023 0.015430 -0.030966 -0.087351 0.292549 -0.017943 0.001508 0.130673 0.251041 0.314539 0.022891 -0.057868 0.006307 -0.081769 0.185851 -0.143862 -0.085553 -0.142739 -0.043928 -0.199831 0.090107 -0.016142 -0.361309 0.106014 0.169806 0.066482 0.149421 -0.137447 -0.003506 0.056384 0.043069 -0.028186 0.034201 -0.180385 0.157394 -0.072359 -0.179641 -0.279560 -0.081792 -0.142043 0.023546 0.329805 -0.365874 0.083665 -0.101588 0.180664 -0.003379 0.244426 -0.313270 0.083386 0.220225 -0.081333 0.030386 -0.109988 0.282306 -0.160015 0.062364 -0.027478 -0.007978 0.313840 -0.114947 0.043828 0.130686\n",
      "660000: frontalot 0.124328 -0.047784 -0.118410 0.192705 0.013015 0.050915 -0.010216 0.105329 -0.066688 0.000198 0.011820 -0.106823 0.140919 -0.101487 -0.076426 -0.308565 -0.043667 0.135845 -0.044050 -0.050600 -0.106319 0.101632 0.129421 0.161616 0.118396 0.106267 -0.043584 0.131533 0.009766 -0.218381 0.063271 0.105378 0.069430 0.267889 0.079795 0.147954 -0.047648 -0.066195 0.295043 -0.103458 0.001298 0.076854 -0.095942 0.069172 -0.230030 0.059661 -0.042362 -0.021466 0.145774 0.051864 -0.305013 0.106094 0.098882 0.157873 -0.025693 0.052124 0.001359 0.044376 0.105638 0.111373 -0.075870 0.050053 0.025653 0.049725 -0.141461 0.033130 0.179361 -0.046336 0.178062 -0.010376 0.071648 -0.080516 -0.230153 0.088102 -0.376068 0.253605 -0.108028 0.045729 0.030277 0.196486 -0.041411 0.397529 0.263574 -0.006863 0.063206 -0.090208 0.038664 0.233308 0.146824 -0.217075 -0.040852 -0.060149 -0.013388 -0.106123 0.085056 0.084313 -0.111778\n",
      "670000: locomotivos 0.168896 -0.040459 -0.098671 0.264937 -0.081598 -0.010607 -0.277201 -0.009543 -0.024849 -0.118266 -0.031963 -0.041439 0.046393 0.087948 0.031418 -0.091091 0.054395 0.001743 -0.008790 -0.004441 0.083007 0.010315 -0.017028 0.112915 0.025868 -0.078609 0.022389 0.133317 -0.065398 -0.082117 0.248357 0.016764 0.032867 0.088092 -0.059507 0.051888 -0.021712 0.192965 0.264504 0.085684 -0.018198 0.020903 0.001812 0.056746 -0.044313 0.120887 0.159969 -0.094796 0.189665 0.057685 -0.090181 0.088477 -0.024694 -0.094055 0.014232 0.186703 0.034857 0.331358 0.025268 -0.058001 -0.077873 0.001130 0.060913 0.211593 0.084672 0.133085 0.163621 -0.010816 0.030728 -0.000493 0.075573 -0.193977 0.054325 -0.091160 -0.191050 0.076474 -0.047267 0.038626 -0.027829 -0.083261 0.024997 0.173157 -0.168517 -0.123183 0.037207 0.036980 -0.061047 -0.065538 0.190627 -0.104901 -0.028296 -0.085681 0.218436 0.001397 0.078046 -0.031507 0.083035\n",
      "680000: podlažice 0.027402 0.042893 0.056579 0.348949 0.056714 0.196248 0.193931 -0.031827 0.096896 -0.025846 -0.059778 0.126668 -0.035506 0.048752 0.027726 -0.077742 0.013602 0.056121 -0.100439 -0.025070 -0.149419 -0.070749 0.097065 0.095659 0.057732 -0.057662 -0.044614 -0.046601 0.029953 -0.098963 0.197637 -0.166351 -0.088159 0.072309 0.038624 0.178901 0.007033 0.088282 -0.033808 -0.074185 0.087860 0.194510 0.028569 -0.006111 -0.001173 -0.109459 -0.171315 -0.146161 0.002083 0.182071 0.097832 0.040841 -0.171383 -0.003923 0.061637 0.159401 0.025434 0.000740 0.108103 -0.115912 -0.133090 0.036812 -0.056847 0.084242 -0.023557 0.083766 -0.157588 -0.016420 -0.096253 -0.052281 0.041213 -0.090378 -0.119474 0.141292 -0.189770 -0.205753 0.090862 -0.140531 -0.055768 -0.065062 0.113542 0.250396 0.064304 -0.006792 0.150806 -0.086146 -0.076013 0.179013 0.035257 0.379818 0.078570 -0.147611 0.002775 0.154025 0.047199 -0.073468 -0.126091\n",
      "690000: tamta -0.071639 0.079024 0.012954 0.112881 0.129522 -0.194510 -0.203516 0.055340 -0.147020 -0.173615 -0.052984 0.092902 -0.032421 -0.132859 0.247821 -0.107535 0.046193 0.017391 -0.148666 0.046064 0.118538 0.069991 0.145557 0.113201 0.082314 -0.158994 -0.060666 -0.124180 -0.005476 -0.033860 -0.001118 -0.068562 0.077856 0.056028 -0.072326 0.025541 -0.002703 -0.141258 -0.217565 0.079601 -0.129092 0.033193 -0.098743 -0.112961 -0.259773 0.047330 0.054459 0.009419 0.181102 -0.037078 -0.084525 0.179799 0.149193 -0.088380 -0.004871 0.181117 0.074166 -0.009691 0.231600 -0.126266 -0.048328 0.017557 -0.187045 0.085978 -0.080800 -0.064068 0.093937 0.001553 0.029748 0.264932 -0.062823 -0.130868 0.097497 -0.023996 -0.130568 0.078076 0.159426 0.089889 0.232282 0.024684 0.053845 0.306665 -0.129012 -0.009002 0.213671 -0.104399 0.013375 0.031426 0.012114 -0.004103 -0.013961 -0.143095 0.051258 -0.092274 -0.140821 0.006086 -0.091921\n",
      "700000: alvadias 0.163526 0.050068 0.045964 0.218197 -0.103896 -0.198355 0.073774 0.122227 0.027370 -0.162262 0.040657 -0.078948 0.277804 0.032804 0.081841 0.035415 -0.179075 0.141811 -0.011994 -0.163046 0.033982 -0.008706 -0.045689 -0.135676 0.158723 -0.068517 -0.061726 0.041769 -0.077784 -0.254932 0.208690 -0.152805 0.030114 -0.106742 0.101404 0.202125 0.075222 0.012521 0.148528 -0.070796 0.194409 0.257224 -0.057804 0.171477 0.008302 -0.197004 -0.027801 -0.060195 -0.005974 0.036928 -0.232587 0.098279 0.056686 -0.056838 0.161499 0.046716 -0.246765 0.049326 -0.078047 -0.042990 -0.276299 -0.082540 0.088721 0.041840 -0.007866 -0.015488 0.057019 -0.020920 -0.057427 0.191702 0.027256 -0.206782 -0.111858 -0.056368 -0.138507 0.327853 -0.146260 -0.068395 -0.097793 0.035733 0.037994 0.260244 0.179178 0.251216 -0.170659 0.236009 0.000185 0.122464 0.134244 0.392889 -0.028710 -0.146923 0.113307 -0.030395 -0.153998 0.026652 -0.252755\n",
      "710000: decoded 0.177358 0.037223 -0.136862 0.360284 -0.088156 0.195162 -0.150882 0.006811 0.074941 -0.136116 -0.026629 0.019195 0.090186 0.147487 -0.035049 -0.161082 -0.049931 0.065102 0.058573 -0.331483 -0.008130 0.166008 0.204942 0.147319 0.182979 0.091551 -0.063208 0.055468 0.036553 0.019824 -0.073834 -0.008744 0.104821 -0.065474 0.155310 0.151630 0.098788 0.017398 0.237774 -0.047205 0.083070 -0.000029 -0.073080 0.003875 -0.096549 0.012471 0.024131 -0.205515 0.058489 -0.000014 -0.077857 -0.069449 -0.076544 -0.025955 0.158366 0.023942 0.011132 0.027303 0.122629 0.032291 0.087905 -0.075250 0.059129 0.006547 0.124302 0.072167 0.284763 -0.166617 -0.010312 -0.115087 0.049547 0.138238 0.034534 0.031463 -0.292689 -0.053821 -0.181884 0.002508 0.042566 0.044728 -0.054849 0.401540 -0.039548 0.134317 -0.031144 0.121067 0.092786 -0.065854 0.303096 0.025210 0.049755 -0.100785 0.052840 0.006324 0.200753 0.085939 0.078670\n",
      "720000: holder-bank -0.051398 0.091815 0.053888 0.055227 0.129802 -0.070654 -0.043377 0.033148 -0.047082 -0.078427 -0.055649 0.060626 0.149288 0.029459 0.127277 0.160823 -0.010782 0.029443 -0.151251 -0.029005 0.091151 -0.056564 0.136194 0.010227 -0.090858 0.114435 -0.028506 -0.028458 -0.038063 -0.247523 -0.082883 -0.042515 -0.034441 -0.089547 -0.087843 0.018649 0.086178 0.016294 0.038702 -0.156340 -0.137049 0.039292 0.075961 -0.261482 0.045123 -0.119661 -0.088772 -0.055532 -0.021502 -0.012073 -0.014247 0.043909 -0.018141 -0.104437 0.079724 0.268728 -0.009932 0.063241 0.092331 -0.049686 -0.116987 -0.021996 0.044928 0.016783 -0.095950 -0.057159 0.071308 0.062040 0.088265 -0.062824 -0.148730 -0.219883 -0.012459 -0.098204 -0.145071 -0.050591 0.106002 0.010893 -0.145179 0.173097 -0.186169 0.137351 -0.013436 -0.009699 -0.056830 0.136328 -0.001343 -0.062951 0.015457 0.071294 -0.021850 -0.007705 0.046844 0.128536 0.000910 0.023765 0.037545\n",
      "730000: notificar-lhe -0.031852 0.145093 -0.322002 0.278807 0.024711 -0.030854 0.049359 -0.084641 -0.089701 -0.082453 -0.320463 0.049820 0.191569 0.089764 -0.029478 -0.028089 -0.015000 -0.049946 -0.099820 -0.183410 -0.003474 -0.095247 0.062206 0.065432 0.062640 -0.173270 -0.065357 -0.018969 0.117488 -0.163996 0.042297 -0.063123 -0.078402 0.015910 -0.198906 -0.034806 0.055672 0.028070 0.285278 -0.215007 -0.325271 0.020313 -0.052625 0.121291 -0.017855 -0.033831 0.112787 0.032231 -0.189219 0.075336 0.048943 0.057285 0.112942 0.097669 0.214203 0.140885 -0.088079 0.123953 0.224503 -0.036140 0.082760 -0.078136 -0.038989 0.319216 0.025564 0.076514 0.072442 0.208096 -0.083918 0.043405 0.058298 -0.169683 0.146700 -0.149294 -0.231189 0.127274 -0.025959 0.186373 -0.087526 -0.040292 -0.165693 0.202496 -0.147237 0.034521 0.134739 0.046915 -0.052031 0.188648 0.079554 -0.017538 -0.073954 -0.015132 0.143366 -0.185089 -0.007183 0.099559 -0.067376\n",
      "740000: sipuncula 0.170354 0.118460 0.133815 0.046074 -0.121583 -0.111502 -0.113298 -0.061424 0.090765 -0.160360 0.070701 -0.009830 0.139040 0.140401 0.040983 -0.003021 -0.140310 0.079919 -0.183546 -0.171823 -0.096287 -0.000961 -0.006270 0.109996 0.138209 -0.076237 0.001218 0.049860 0.088886 -0.208003 0.095686 -0.075903 0.065236 0.109555 -0.060840 0.126257 0.154336 0.052317 0.208498 0.009814 -0.072766 0.054425 -0.112329 0.133862 -0.022464 0.006165 0.043839 0.000634 0.085385 -0.094325 0.004363 0.075854 -0.016583 -0.066140 0.031934 0.077863 -0.094396 -0.050980 0.195770 0.018326 -0.039477 0.029768 -0.058169 0.176780 0.068639 -0.065251 0.128328 -0.009178 0.106846 0.061893 -0.063346 -0.052207 -0.126971 0.062250 -0.210641 0.105681 -0.024588 0.112293 0.051054 0.071886 0.049035 0.213169 -0.078804 -0.050044 0.054751 0.067461 -0.058071 -0.134058 0.188085 0.008888 0.014778 -0.145465 -0.149926 -0.120092 0.001183 -0.067417 -0.135268\n",
      "750000: 0000pelos -0.064747 0.133802 0.034806 0.015251 -0.033420 0.106191 -0.152992 -0.091952 0.034121 -0.024332 -0.206001 0.002455 0.119688 -0.253083 -0.097376 -0.158590 -0.313234 -0.040306 0.111592 -0.029644 0.138601 0.008910 0.131925 0.004027 0.113003 0.119456 -0.070068 0.009299 0.044365 -0.301229 0.189050 -0.099076 -0.259706 -0.013257 -0.132219 0.216621 0.047555 -0.106407 0.073582 0.025639 -0.149002 0.158184 0.295069 0.104794 0.144547 -0.003045 0.180270 -0.117033 0.269449 0.073408 -0.167639 0.064958 0.057866 -0.199163 0.138357 0.221440 -0.311333 -0.135367 0.021530 -0.235151 -0.110397 -0.058594 -0.198627 0.212235 -0.214989 0.223059 -0.208230 -0.181040 0.120984 0.120641 0.045758 -0.090306 -0.237749 -0.097666 -0.329371 0.089189 -0.008546 0.082871 0.115361 -0.086963 -0.010445 0.321287 -0.089913 -0.210915 0.205689 -0.133287 -0.093422 -0.092515 0.082331 0.108645 0.012687 -0.351127 0.074084 0.064454 -0.051547 -0.055236 0.122375\n",
      "760000: batukada 0.196678 0.024601 0.189359 0.207300 0.083382 -0.062001 -0.097014 -0.164084 -0.078880 -0.177949 -0.147750 -0.087693 0.118684 0.051395 0.169255 -0.104776 -0.044123 0.021081 0.017890 -0.162837 0.030830 -0.135411 0.129202 -0.045631 0.004621 0.149297 -0.149896 -0.046693 0.155265 -0.384785 -0.017528 -0.163709 0.069188 0.075673 0.058517 0.051285 0.007511 -0.066381 -0.004179 0.078247 -0.158911 -0.062629 -0.127388 -0.090019 -0.086761 -0.013732 0.040438 0.110826 0.215388 -0.140338 -0.354703 0.132043 -0.086570 0.054227 0.266683 -0.201126 -0.045636 0.117972 0.127560 0.014635 0.036918 -0.099066 0.029467 0.142850 0.086041 -0.081694 0.135047 0.045181 -0.116836 0.004200 -0.409758 -0.191501 -0.065770 -0.047450 -0.033668 0.210754 -0.061501 0.292235 0.278824 0.039026 -0.145850 0.417199 0.096633 -0.091460 -0.065918 -0.082133 -0.044952 0.009721 -0.013158 -0.000024 0.127561 0.134076 0.163754 -0.224447 -0.196656 -0.239812 0.036930\n",
      "770000: conirostris -0.052702 0.044995 -0.093727 0.034519 0.090409 0.019063 -0.054477 0.047650 -0.047239 -0.013806 -0.209634 0.093120 -0.080268 -0.098220 0.104096 0.128342 -0.060170 0.018995 -0.129946 -0.147427 0.100591 -0.037416 0.240616 0.104891 0.047602 0.156567 0.091174 0.002140 -0.069470 -0.056811 0.079407 0.181333 -0.020978 0.059639 0.022243 -0.042373 0.103127 0.016233 0.023699 -0.105813 -0.281395 0.019906 0.089880 -0.127787 0.016386 -0.038088 0.015337 -0.047375 0.075680 0.051731 -0.018198 0.083424 0.002295 -0.033882 0.102329 -0.023015 -0.039884 0.237363 0.203076 -0.023180 0.009510 -0.112018 0.078253 -0.075160 0.005661 0.082614 0.070025 0.005483 0.140163 0.122263 -0.114597 -0.152544 -0.015358 -0.070693 -0.308674 0.016606 -0.088282 0.107437 0.013696 0.117241 -0.117685 0.200517 -0.049170 0.142856 -0.033886 0.045029 0.026209 0.174048 0.210438 -0.008772 0.010252 -0.058149 0.041466 -0.033748 -0.116429 -0.024508 -0.071137\n",
      "780000: ergoespirometria 0.148284 0.010170 -0.116788 0.134083 0.056302 -0.150500 -0.038062 0.015444 -0.016156 -0.195205 -0.034604 0.029660 -0.028782 0.005588 0.146072 0.026579 0.083346 -0.064181 -0.160347 -0.131341 0.158957 -0.083685 0.186164 0.054966 0.114884 -0.058584 -0.028314 -0.034349 0.101090 -0.206797 0.079301 0.063803 0.061408 -0.104990 0.008370 -0.032465 0.114746 0.177912 0.216127 -0.046344 0.022566 -0.129537 0.026239 -0.005170 -0.061797 -0.009052 -0.007000 -0.036925 0.058700 0.034870 -0.028336 0.034904 0.080616 -0.107599 -0.005538 -0.009350 -0.011025 0.020513 0.087535 0.041840 0.027921 -0.041812 0.028165 0.062624 0.029063 -0.027088 0.028107 0.084043 -0.117615 0.107480 -0.110687 -0.153963 -0.050521 0.083843 -0.105031 0.116917 0.031765 -0.054982 0.118715 0.013165 -0.012780 0.169054 0.064418 -0.091046 0.022251 0.053261 0.004159 0.009324 0.075586 0.015538 -0.000172 -0.172133 0.085630 0.008159 -0.026140 0.067425 -0.103803\n",
      "790000: harleyville 0.061426 0.054431 -0.124080 0.125947 0.071221 -0.016844 -0.071267 0.088091 0.082744 -0.041349 -0.171919 0.068926 -0.085812 -0.033417 0.076694 0.042963 -0.041371 0.036895 -0.098923 -0.062927 0.021455 -0.044891 0.124687 0.076723 0.055076 0.037154 -0.063922 0.013415 -0.096959 -0.031230 0.016471 0.079104 -0.051768 0.007414 -0.059858 0.061881 0.093743 0.018163 -0.011020 -0.069439 -0.143136 -0.001321 0.175753 -0.060291 0.022681 0.040594 -0.054052 -0.120959 0.103392 0.109292 -0.081801 -0.015189 -0.012032 -0.024983 0.054065 0.097515 0.047887 0.087150 0.087953 -0.056752 0.049520 -0.010851 -0.006151 -0.096516 0.007096 0.069660 0.030733 -0.038367 0.001227 0.110292 0.043476 -0.126903 -0.042574 -0.056563 -0.218737 0.002958 -0.130189 -0.044599 0.072212 0.087521 -0.083253 0.285455 -0.002661 0.045682 0.022345 0.077274 0.066170 0.121354 0.197899 0.104282 -0.048389 -0.088118 0.108559 0.028681 -0.058770 0.001365 -0.039445\n",
      "800000: lanlan 0.104268 -0.118406 0.046041 0.189511 -0.020407 -0.207482 -0.091503 0.172804 0.021578 -0.131221 -0.036392 -0.003381 0.023519 0.080396 0.137611 -0.034170 -0.157092 0.151823 -0.038180 -0.152195 -0.026956 0.126767 -0.087128 0.050651 0.159742 0.071979 0.095682 0.021886 0.139363 -0.056404 0.152786 -0.023562 0.030998 0.003121 -0.004020 0.077938 0.065587 0.110752 0.045207 -0.121382 -0.002038 -0.097056 0.006730 0.043180 0.016623 -0.031657 -0.048573 0.025449 -0.061547 -0.018688 0.019680 0.010261 0.079495 -0.117014 -0.138480 0.054303 0.150859 -0.052415 0.094360 -0.078356 -0.099416 0.004838 0.046994 -0.098390 0.026437 -0.077453 0.181840 -0.058633 -0.008825 -0.018630 -0.026143 -0.071106 -0.063784 -0.016079 -0.199326 0.044876 0.039617 -0.037865 0.038252 0.190753 -0.018709 0.353450 0.020400 -0.114980 0.029110 0.104757 -0.039374 0.111357 0.074754 0.042542 -0.040676 -0.137406 0.084033 -0.065311 -0.042012 0.098868 -0.016273\n",
      "810000: navigação 0.038477 0.065962 0.138496 0.104889 0.139056 -0.046931 0.149690 0.121072 -0.012129 -0.175928 0.023863 0.145658 -0.006954 0.024935 0.310768 -0.009311 -0.179097 -0.051567 -0.105314 -0.097504 -0.031486 0.024190 0.063068 0.006726 -0.198946 0.017813 -0.120168 -0.072246 -0.138450 -0.203382 0.182627 0.110953 -0.110195 0.033202 -0.168464 0.218112 0.036595 0.132424 -0.035825 -0.270213 -0.059517 0.063696 0.124815 -0.085714 0.116096 -0.201036 -0.326302 -0.170649 -0.142557 0.123907 0.113018 0.138326 -0.085103 0.161534 0.127825 0.245972 -0.091408 0.016805 -0.000988 -0.122324 -0.067757 0.085456 0.071173 0.095996 -0.149923 0.017325 0.118591 -0.010528 0.011678 -0.084791 0.060127 -0.239718 -0.035688 0.084845 -0.302660 -0.038278 -0.046266 -0.105667 -0.244599 0.088556 -0.003397 0.287542 0.087776 0.092058 -0.015542 0.317428 -0.033623 -0.063317 0.012186 0.273988 0.094296 0.086447 -0.199013 0.190388 0.174432 -0.007708 -0.125314\n",
      "820000: prolongara-se 0.107956 -0.040407 -0.033065 0.089197 -0.076007 -0.060127 -0.084690 0.118434 -0.146081 -0.102278 -0.214274 0.086500 -0.083759 -0.180730 0.154976 0.091825 -0.131917 0.116745 -0.354748 -0.034220 0.056814 -0.038179 0.088225 0.006500 0.054042 0.056114 -0.069412 -0.127008 -0.124998 -0.082734 0.013157 0.126837 -0.035505 -0.095864 -0.102269 -0.021123 0.015929 -0.082027 -0.062125 -0.194026 -0.209763 -0.082336 0.113659 -0.054749 0.035806 0.016318 0.093003 -0.089993 0.081362 0.070649 0.156734 0.183980 0.096356 -0.116829 0.085635 0.046879 -0.118696 0.299138 0.038562 -0.085739 0.063214 -0.182946 -0.063802 -0.130640 0.052402 0.021774 0.168050 0.003696 0.177784 -0.170260 -0.074882 -0.249161 -0.245869 0.026135 -0.230136 0.201923 -0.164556 0.016505 0.022277 0.135888 -0.044146 0.150225 -0.150315 0.161926 0.018925 0.019206 -0.064299 -0.080600 0.212939 -0.004189 -0.046984 -0.098478 0.153282 -0.125810 -0.099235 0.122554 0.086790\n",
      "830000: sitoli 0.093891 -0.027519 -0.014418 0.135873 0.071423 -0.033984 -0.049522 0.126889 0.063270 -0.107342 -0.174791 0.140134 0.056483 0.007302 0.022998 -0.068957 -0.147385 -0.043753 0.101914 -0.229646 -0.001536 0.030994 -0.011548 0.179280 0.002490 0.050373 0.003992 0.241445 0.021754 -0.070817 0.040931 0.058467 -0.065821 -0.008763 0.098295 0.055501 -0.126365 0.113403 0.027597 -0.225356 -0.340307 0.124345 -0.060460 0.070424 0.088132 -0.057726 0.085163 0.027103 0.048493 -0.015421 0.161131 0.143909 -0.075870 -0.104904 0.257223 0.212708 0.003224 0.081900 0.165268 -0.079071 -0.019341 0.084482 -0.092190 -0.072495 0.049940 0.077629 -0.063524 -0.043295 -0.123605 -0.017314 -0.004088 -0.132318 -0.064510 0.082024 -0.471703 -0.037024 -0.069364 0.046585 -0.091084 0.074197 -0.115876 0.389539 -0.098705 -0.026719 -0.046429 -0.040113 -0.017483 0.015933 0.070518 -0.011760 0.062547 -0.051114 0.038873 0.048408 -0.187418 0.036981 0.076578\n",
      "840000: vassiljeva 0.056589 0.028166 0.020244 0.177007 0.096741 -0.094070 -0.098737 0.034177 0.067775 -0.122118 -0.115774 -0.063154 0.003002 -0.001123 -0.053561 0.039375 -0.050924 0.072303 -0.078456 -0.166379 -0.003342 -0.033610 0.124316 0.016300 0.222853 -0.139841 -0.051739 -0.007575 -0.021568 -0.156216 0.114146 0.000538 0.077380 -0.054306 -0.006400 0.123348 0.200085 0.042249 0.236045 -0.010851 -0.208376 -0.047697 -0.042436 0.011280 -0.054097 0.056087 0.049534 -0.059942 0.142292 0.080889 -0.123768 -0.020451 0.044915 -0.087781 0.073213 -0.020164 -0.017461 -0.128768 -0.046267 -0.072781 -0.011345 0.021322 -0.066221 0.152037 0.082186 0.003473 -0.013333 0.001449 0.087051 0.120310 0.069369 -0.047090 0.017759 -0.087747 -0.112355 0.127997 -0.107378 -0.009392 0.145424 0.082192 -0.069693 0.165014 0.014032 0.035689 -0.013414 -0.097385 0.103320 -0.019860 0.055720 -0.003544 -0.108144 -0.263794 0.049367 0.027638 0.037790 0.049870 -0.125554\n",
      "850000: ajuda-lhe -0.030694 -0.019543 -0.062359 0.093234 0.075664 -0.138513 -0.196524 0.046837 0.052482 -0.134101 -0.042756 0.046041 -0.121949 0.029395 0.165664 0.076347 0.039871 0.003471 -0.199561 -0.091168 -0.028801 0.124549 0.116544 0.085189 -0.028739 0.065315 -0.114717 -0.129177 0.030941 -0.237072 -0.112475 0.015755 0.013477 0.126361 -0.130460 0.058924 0.124230 -0.013207 0.131269 0.019318 -0.294155 0.044901 0.191479 -0.205595 0.002641 -0.056787 -0.074106 -0.011744 0.119891 0.046503 -0.032134 0.004254 -0.043018 0.043207 0.169685 0.095082 -0.096856 0.065265 0.279657 -0.059901 0.014008 0.007733 -0.119245 0.000078 0.046881 -0.153787 -0.001700 -0.060446 0.043421 0.036245 -0.093795 -0.125724 -0.011878 -0.121042 -0.211796 0.064764 0.051184 -0.150786 -0.050664 0.127415 -0.002311 0.235927 -0.062077 0.052318 0.045286 -0.004048 0.027015 0.048095 0.186289 -0.175361 0.029280 -0.131235 0.239813 -0.102098 -0.072117 -0.007765 -0.119474\n",
      "860000: can-didaturas 0.043561 0.064868 -0.115444 0.184831 -0.005432 0.000847 0.015417 -0.026817 -0.119094 -0.010925 -0.286523 0.124900 0.035720 -0.043974 -0.012584 0.045120 -0.084567 -0.028839 -0.177431 -0.149091 0.204940 -0.214800 0.225568 0.164049 -0.182183 -0.248573 -0.100521 -0.029611 0.054916 -0.076413 0.136029 -0.015137 0.103356 -0.000058 -0.135484 0.109857 0.010001 0.062954 0.045316 -0.010550 -0.087108 0.044323 0.223914 0.218366 -0.026135 -0.053273 -0.100511 -0.028374 0.015977 0.194363 0.107676 -0.014612 0.208815 0.079192 0.261789 -0.037146 0.094405 -0.005712 0.086614 0.072811 0.106982 0.075131 0.027842 -0.038686 -0.112512 0.152546 0.187985 0.199559 -0.081297 0.305052 -0.063923 -0.092566 -0.012734 -0.320533 -0.215618 -0.007040 -0.108155 0.163883 0.050803 -0.074497 -0.160574 0.357602 -0.146093 0.020132 0.169570 -0.102795 -0.010372 0.332312 0.160695 0.139331 -0.066669 -0.158969 -0.089258 -0.000300 0.081148 -0.001746 -0.092586\n",
      "870000: dewar's 0.006781 0.039886 -0.052586 -0.030098 0.034958 0.088857 -0.033552 0.134379 -0.168526 -0.108273 -0.120224 -0.074097 -0.123533 -0.037160 0.176462 -0.097996 0.017695 0.079005 -0.172758 -0.293497 -0.296766 0.160711 0.042818 0.100657 0.110687 -0.033029 -0.137714 0.065725 0.027441 -0.146020 0.089076 0.078217 0.004404 0.076314 0.010393 0.046559 0.060113 0.136622 -0.066633 -0.047232 -0.269354 0.053763 0.019578 0.031420 -0.056991 0.024454 0.092225 0.069237 0.104156 0.072538 -0.028146 -0.002432 -0.085078 0.025445 0.221721 -0.102468 -0.074181 0.132903 0.217661 0.070845 -0.000718 -0.098136 -0.036149 0.167985 -0.070706 -0.110138 0.224697 -0.077111 0.054785 -0.071526 -0.215805 -0.072569 -0.121045 -0.144034 -0.125427 0.213276 -0.153494 -0.072920 0.071210 0.012074 -0.008041 0.414320 -0.077979 -0.018238 0.109810 -0.164084 -0.030446 0.019603 0.199347 0.013119 0.183719 -0.003282 0.070401 -0.016110 -0.147341 0.102759 0.118281\n",
      "880000: fritagelse -0.002684 0.090682 -0.121711 -0.018541 0.056666 0.039873 -0.219243 -0.054073 0.063715 -0.157296 -0.097916 0.176340 0.053316 0.123294 0.221777 0.085972 0.096040 0.011913 -0.113225 -0.165733 0.140770 -0.024018 0.053775 0.096598 0.059942 0.109763 -0.160110 -0.026509 -0.003504 -0.099531 0.075011 0.049598 -0.074958 -0.142355 -0.110986 -0.119918 0.043251 -0.158102 -0.213277 -0.122334 -0.239961 0.058317 -0.019871 -0.070031 -0.103598 -0.074383 0.049096 -0.181653 0.240639 -0.069840 -0.036059 -0.018330 0.033987 0.098551 0.095430 0.146949 -0.202945 0.032257 0.302786 -0.073909 0.144565 -0.046195 -0.075564 -0.039497 -0.035655 0.203695 -0.014705 -0.076754 -0.118985 0.212689 -0.131481 -0.108710 0.035160 0.091142 -0.314937 0.144023 -0.272439 -0.131905 0.156414 0.140691 0.077385 0.386201 0.073100 0.020884 -0.076517 0.028461 -0.129949 0.042181 0.198125 -0.109178 0.192432 -0.023128 -0.082080 -0.011228 -0.105569 -0.028004 0.020224\n",
      "890000: keppelmann 0.032285 0.075510 -0.053037 0.110519 0.025049 -0.015791 -0.119780 0.021990 0.085018 -0.137593 -0.094568 0.109922 -0.018117 0.018459 0.089940 0.038928 -0.058901 0.066650 -0.083551 -0.060381 -0.093428 0.007702 0.223175 0.059597 0.009016 0.010457 -0.061981 -0.102553 -0.043408 -0.016320 0.028724 0.057631 -0.130802 0.032415 -0.065478 0.038033 0.052718 0.151291 -0.066957 -0.102417 -0.181382 0.101131 0.094797 0.011795 0.035086 -0.061320 -0.029653 0.034042 0.086752 0.175845 -0.071283 -0.051251 0.012979 0.124020 0.071637 0.016290 0.031480 0.174980 0.235605 -0.095405 0.052241 -0.102488 -0.067448 0.048387 0.020456 0.157522 0.014077 -0.045905 0.020957 0.131865 -0.106955 -0.116328 -0.097245 0.038384 -0.298564 0.076365 -0.097482 0.002889 0.084635 0.032665 0.067848 0.328107 0.075356 0.107950 0.071914 0.023705 0.014934 0.056107 0.241081 0.041367 0.127796 -0.063763 -0.014216 0.009994 -0.229356 0.002127 -0.101986\n",
      "900000: nauti 0.130341 0.109843 -0.107137 0.204292 -0.056250 0.069070 -0.042611 -0.071886 0.230612 -0.153450 -0.228212 -0.019823 -0.135634 0.246841 0.085205 -0.019555 -0.058007 0.087057 -0.043196 0.020492 -0.010926 -0.095328 0.095970 0.115249 -0.004324 0.084654 -0.164369 -0.193957 0.161707 -0.014125 0.174519 -0.064316 0.002821 0.034252 -0.073565 0.054046 -0.109827 0.184448 -0.022112 0.027927 -0.129249 0.142449 -0.057412 -0.057030 -0.214632 0.137263 -0.103242 -0.081879 0.082921 0.209074 -0.061355 0.013537 -0.164200 0.106141 0.147548 -0.013478 -0.034376 0.173956 0.293314 0.096625 -0.078456 0.076998 -0.098001 -0.077656 -0.014222 -0.015717 0.151657 -0.005528 -0.006215 0.159736 0.048346 -0.131488 -0.079831 -0.069155 -0.183064 0.056026 -0.161991 -0.009033 0.107146 -0.029324 -0.216666 0.225326 -0.024837 -0.111537 0.057646 -0.041867 -0.097089 -0.089266 0.118169 0.208814 -0.031080 -0.113694 -0.003483 0.011791 -0.108970 -0.112530 0.017886\n",
      "910000: quarteirenses 0.047842 0.104761 -0.040103 0.312487 -0.042154 -0.056813 -0.168860 -0.008503 0.066618 -0.033153 -0.132772 0.049006 0.051567 0.093452 -0.091306 -0.115882 0.010312 0.013694 -0.019696 -0.026977 0.208356 0.037368 0.104634 0.163392 0.048882 -0.243632 -0.117482 -0.042692 -0.004611 -0.075796 0.067139 0.060284 0.077054 0.112650 -0.275769 0.034009 0.081262 -0.018042 0.084115 0.019545 -0.133669 -0.111804 0.207540 0.138607 -0.011670 0.082167 0.127082 0.096133 0.118632 -0.085014 -0.012901 0.037474 -0.076032 -0.096121 0.209547 0.121961 -0.053657 0.075101 0.020755 -0.115473 0.112013 -0.023404 -0.086415 0.071400 -0.063292 0.006317 0.166160 -0.035179 -0.074956 0.315278 -0.232785 -0.115369 0.066138 -0.042105 -0.423950 0.040185 -0.319858 0.110592 0.098619 0.018708 0.025458 0.174644 -0.151128 -0.023121 0.130224 0.033740 -0.092285 0.065299 0.026693 0.015761 0.065169 -0.169398 -0.095165 -0.159763 0.191066 0.062254 0.086074\n",
      "920000: successfactors 0.003424 0.031774 0.047298 0.080930 0.008125 -0.006162 -0.120028 -0.013747 0.031001 -0.091026 -0.192744 -0.003311 0.110857 0.204587 0.164359 0.024091 0.095779 0.007110 -0.080138 -0.065950 0.042979 0.079289 0.186950 0.159524 0.056927 0.075864 -0.156148 -0.035230 0.110776 -0.234017 0.024249 -0.026441 -0.042151 0.060385 -0.111682 0.159382 0.042858 0.170771 0.103201 -0.109998 -0.073960 0.092412 0.088214 -0.207961 -0.103148 -0.155739 0.103880 -0.147864 0.035608 -0.131284 -0.081662 -0.013712 0.047576 0.052606 0.116713 0.109313 -0.017182 0.276124 0.147151 0.031218 -0.020732 -0.121254 -0.023410 0.009470 -0.047725 -0.063027 0.055426 -0.124753 -0.035295 0.169001 0.025495 -0.198571 -0.078709 -0.075522 -0.184844 -0.000572 0.086498 0.045986 0.008613 0.138243 -0.075148 0.125266 -0.100610 -0.004273 0.051926 0.116644 0.003432 -0.008845 0.122116 0.067538 0.004944 0.007756 0.174678 0.091380 0.058325 0.130043 -0.132454\n",
      "Palavras ignoradas: 3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from embeddings.utils import get_embedding, plot_words_embeddings\n",
    "\n",
    "str_dataset = \"glove.en.100.txt\"\n",
    "dict_embedding_en = get_embedding(str_dataset)\n",
    "str_dataset = \"glove.pt.100.txt\"\n",
    "dict_embedding_pt = get_embedding(str_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `plot_words_embeddings` utiliza [Análise de Componentes Principais](https://pt.wikipedia.org/wiki/An%C3%A1lise_de_componentes_principais) (PCA, do inglês Principal Component Analisys) para reduzir cada embedding em 2 dimensões para, logo após, plotar em um grafico a posição dessas palavras de acordo com o embedding. Veja o gráfico apresentado abaixo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Não foi possivel encontrar a palavra principe\n",
      "Não foi possivel encontrar a palavra rei\n",
      "Não foi possivel encontrar a palavra rainha\n",
      "Não foi possivel encontrar a palavra conde\n",
      "Não foi possivel encontrar a palavra duquesa\n",
      "Não foi possivel encontrar a palavra duque\n",
      "Não foi possivel encontrar a palavra condessa\n",
      "Não foi possivel encontrar a palavra marquês\n",
      "Não foi possivel encontrar a palavra marquesa\n",
      "Não foi possivel encontrar a palavra homem\n",
      "Não foi possivel encontrar a palavra mulher\n",
      "Não foi possivel encontrar a palavra princesa\n",
      "Não foi possivel encontrar a palavra menina\n",
      "Não foi possivel encontrar a palavra menino\n",
      "Não foi possivel encontrar a palavra criança\n",
      "Não foi possivel encontrar a palavra garoto\n",
      "Não foi possivel encontrar a palavra garota\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pablo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\decomposition\\_pca.py:543: RuntimeWarning: invalid value encountered in divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_to_use = {\n",
    "                        \"en\":\n",
    "                            {\"embedding\":dict_embedding_en,\n",
    "                            \"words_to_use\":[\"prince\",\"princess\",\n",
    "                                            \"duchess\", \"duke\", \"countess\", \"marquis\", \n",
    "                                            \"marquise\",\"king\",\"queen\",\n",
    "                                            \"girl\",\"boy\",\"man\",\"woman\",\"child\"]},\n",
    "\n",
    "                        \"pt\":{\"embedding\":dict_embedding_pt,\n",
    "                          \"words_to_use\":[\"principe\",\"rei\",\"rainha\",\"conde\",\"duquesa\",\"duque\",\"condessa\",\n",
    "                           \"marquês\",\"marquesa\",\n",
    "                           \"homem\",\"mulher\",\"princesa\",\"menina\",\"menino\",\"criança\",\n",
    "                           \"garoto\",\"garota\"]}\n",
    "                }\n",
    "\n",
    "language = \"pt\" #mude de 'pt' para 'en' para ver em ingles tb!\n",
    "plot_words_embeddings(embeddings_to_use[language][\"embedding\"], \n",
    "                    embeddings_to_use[language][\"words_to_use\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exemplo acima, em português, veja que podemos pensar em dois conceitos claramente divididos: a realeza e o gênero. Pense: neste plano cartesiano, qual eixo corresponde ao conceito de realeza? E o de gênero? Perceba que \"criança\" deveria ter gênero neutro - de fato, está mais próximo do zero. Porém, pode haver algum ruído associando a palavra criança ao genero feminino. Isso, em português, pode haver uma explicação, pois utilizamos o artigo `a`, usado para palavras que remetem ao genero feminino, para se referir a criança. Assim, em português, os artigos podem aproximar uma palavra de gênero neutro a um determinado gênero.\n",
    "\n",
    "\n",
    "Em inglês, não foi possível verificar tão bem a divisão entre os conceitos de `genero` e `realeza`. Isso pode ocorrer devido a redução de dimensionalidade: os conceitos não necessariamente correspondem a um eixo no plano cartesiano e, mesmo se corresponderem, ao mapear itens com $n$ dimensões para um plano bidimensional, pode haver perda de informação. Mesmo assim, conseguimos ver a separação entre palavras da realeza e que não são da realeza. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sinta-se livre para \"brincar\", alterando/adicionando palavras. Por exemplo, adicione animais. Devido à ambiguidades, ao dataset e à própria redução de dimensionalidade, podem existir palavras que estão erroneamente próximas, se considerarmos o conceito das mesmas,  principalmente se adicionarmos palavras de conceitos muito distintos. Um detalhe: no dataset em português, há uso de palavras compostas e elas estão (geralmente) separadas por hífen. No dataset em inglês não há palavras compostas.\n",
    "\n",
    "Tanto nesta tarefa quanto na próxima você poderá perceber que os embeddings podem carregar preconceitos. Há uma forma de modificar os vetores para eliminar um determinado tipo de preconceito. Por exemplo, nesses embeddings existirão palavras erronemente similares a um determinado genero e, para corrigir, é possível deixar todas as palavras sem distinção pelo genero. Caso queira saber como minimizar esse problema, veja o artigo \"[Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://arxiv.org/abs/1607.06520)\". O título do artigo se remete a um preconceito descoberto ao usar analogias, que será o próximo tópico desta prática. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação de analogias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra caracteristica muito interessante ao usar embedding é a criação de analogias. Por exemplo, na frase `homem está para mulher assim como rei está para...`, fazendo operações com os _embeddings_, muitas vezes é possível chegar na analogia mais provável que, neste caso, seria a palavra `rainha`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 2 - cálculo da analogia: ** Nesta atividade, iremos implementar o método `calcula_embedding_analogia` da classe `Analogy`. Essa classe tem acesso ao dicionário de embeddings e a estrutura KDTree, que iremos explicá-la posteriormente. Considerando a frase <span style=\"color:blue\">\"**palavra_x** está para **palavra_y** assim como **assim_como** esta para **palavra_z**\"</span>, o método `calcula_embedding_analogia` recebe como parâmetro as palavras `palavra_x`, `esta_para` e `assim_ como` e retorna um embedding que, possivelmente, será muito próximo da `palavra_z`. \n",
    "\n",
    "Veja [na aula](https://docs.google.com/presentation/d/1-CggYUA2s7LW7_LcnGv7vlpUGFg9kEWG0j6lWGUnaLI/edit?usp=sharing) como é feito o cálculo e, logo após, faça o teste unitário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[1, 2, 3],[-1.2, 3.2, 1.2],[12.2, 31.2, 11.2]], dtype=np.float16)\n",
    "esta_para = np.array([[-3, 0, 1],[11, 56, 32.2],[0, 0.2, 0.4]], dtype=np.float16)\n",
    "assim_como = np.array([[2, 1, 1],[0.1,0.3,0],[1.23, 0.1, 1.2]], dtype=np.float16)\n",
    "\n",
    "for i,x_val in enumerate(x):\n",
    "    arr_embedding = assim_como[i]-x[i]+esta_para[i]\n",
    "    print(\"[\",end=\" \")\n",
    "    for val in arr_embedding:\n",
    "        print(float(val),end=\", \")\n",
    "    print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m embeddings.embedding_tests TestEmbeddings.test_calculo_analogia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 3 - busca da palavra mais similar:** O cálculo da atividade anterior resultou em um embedding e, agora, precisamos  procuramos a palavra mais próxima a este embedding obtido. Para isso, precisamos de: (1) uma forma eficiente para percorrer os embeddings para descobrir o mais similar; (2) uma métrica de similaridade/distância; \n",
    "\n",
    "**Como percorrer embeddings?** Para encontrarmos os embeddings similares, uma alternativa seria percorrer todos os vetores de embeddings e encontrar o mais similar. Porém, como estamos trabalhando com centenas de milhares de embeddings, essa operação seria muito custosa. Para isso, podemos usar uma estrutura de dados chamada **KDTree**. KDtree é uma arvore que organiza dados espaciais de tal forma que conseguimos alcançar elementos similares de forma mais eficiente. Caso esteja interessado em mais detalhes, [veja este video](https://www.youtube.com/watch?v=Glp7THUpGow).\n",
    "\n",
    "**Qual métrica de distancia/similaridade usaremos?**  Já foi demonstrado que esta métrica é eficiente para similaridade entre embeddings é a distância euclidiana [(Pennington et al., 2015)](https://nlp.stanford.edu/pubs/glove.pdf). A [distancia euclidiana](https://pt.wikipedia.org/wiki/Dist%C3%A2ncia_euclidiana) entre dois pontos $p$ e $q$ é calculada por meio do tamanho da linha entre esses pontos. Para um espaço bidimensional, considerando que os pontos $p$ e $q$ são representados pelas coordenadas $(p_1,p_2)$ e $(q_1,q_2)$, respectivamente, a equação é dada pela seguinte fórmula: $d(p,q) = \\sqrt{(p_1-q_1)^2+(p_2-q_2)^2}$ veja uma representação gráfica: \n",
    "\n",
    "<img width=\"400px\" src=\"img/distancia_euclidiana.svg\">\n",
    "\n",
    "Esta métrica pode ser generalizada para um espaço n-dimensional e o cálculo seria: $d(p,q) = \\sqrt{(p_1-q_1)^2+(p_2-q_2)^2+...+(p_n-q_n)^n}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, nesta atividade iremos utilizar [a implementação do kdtree do scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html). Nessa estrutura, é possível armazenar os embeddings e, logo após fazer consultas eficientes para, por exemplo, procurar os k elementos mais próximos. Veja o exemplo abaixo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "elementos = [[1,1],\n",
    "             [2,2],\n",
    "             [3,3],\n",
    "             [4,4],\n",
    "             [5,5],\n",
    "             [6,6],\n",
    "             ]\n",
    "#os elementos são passados como parametro na construção do KDTree junto com a métrica \n",
    "#de distancia que iremos usar\n",
    "kdtree = KDTree(elementos,  metric='euclidean')\n",
    "\n",
    "#retorna os 2 elementos mais próximos e sua distancia\n",
    "#como podemos fazer uma consulta por lista de pontos, temos que \n",
    "#passar uma lista de pontos como parametro\n",
    "ponto = [3,2]\n",
    "distancia,pos_mais_prox = kdtree.query([ponto], k=3, return_distance=True)\n",
    "for i,pos in enumerate(pos_mais_prox[0]):\n",
    "    elemento = elementos[pos]\n",
    "    distancia_ponto = distancia[0][i]\n",
    "    print(f\"O ponto {elemento} é o {i+1}º ponto mais próximo de {ponto} distância: {distancia_ponto}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dessa forma, cada embedding pode ser armazenado no KTree para, logo após, obtermos os embeddings mais próximos a um embedding em questão. Não é possível armazenar na estrutura do KDTree a palavra referente a cada embedding representado, por isso, armazenamos essa estrutura como um atributo da classe `KDTreeEmbedding` (arquivo `utils.py`) que armazena também os atributos `pos_to_word` mapeando, para cada posição a palavra correspondente e o atributo `word_to_pos` que faz o oposto: mapeia, para cada palavra, a posição correspondente. Veja no construtor de `KDTreeEmbedding` como é criado o KDTree. Nela, também será salvo um arquivo com a implementação do KDtree e os atributos `pot_to_word` e `word_to_pos` isso é necessário pois a criação da KDTree é muito custosa.\n",
    "\n",
    "\n",
    "Nesta atividade, você deverá implementar `get_most_similar_embedding` que obtém as $k$ palavras mais similares à palavra (ou embedding) representado pelo parâmetro `query` por meio do método `query` da KDTree. O parâmetro `query` pode ser a palavra (`string`) ou o proprio embedding (`np.array`). Logo após, implemente também o método `get_embeddings_by_similarity` que utiliza o método `query_radius` ([veja documentação](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree.query_radius)) que retorna todas as palavras que estão em um raio de `max_distance` da palavra alvo especificada pelo parametro `query`. Para ambas as implementações, utiliza-se o método `positions_to_word`, já implementado, para retornar as palavras de acordo com as posições indicadas. Caso haja alguma palavra a ser ignorada em `words_to_ignore` ela será excluída também no método `positions_to_word`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m embeddings.embedding_tests TestEmbeddings.test_get_most_similar_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m embeddings.embedding_tests TestEmbeddings.test_embeddings_by_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, você pode testar os métodos utilizando os datasets de embeddings. Lembre-se  que o KDTree pode demorar mais de 30 minutos para ser criado na primeira execução de cada idioma. Caso queira testar para o inglês, não esqueça de mudar de `\"kdtree.pt.p\"` para `\"kdtree.en.p\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.utils import get_embedding, KDTreeEmbedding\n",
    "str_dataset = \"glove.pt.100.txt\"\n",
    "kdtree_file = \"kdtree.pt.p\"\n",
    "dict_embedding = get_embedding(str_dataset)\n",
    "kdtree = KDTreeEmbedding(dict_embedding, kdtree_file)\n",
    "kdtree.get_most_similar_embedding(\"carro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 5 - 💞 apresentando as analogias 💞:** Agora você deverá implementar o método `analogia` da classe `Analogy` que deverá utilizar os métodos `calcula_embedding_analogia` e o `get_most_similar_embedding` para retornar as 4 palavras mais prováveis para completar uma determinada analogia, com os parâmetros indicados. Caso, dentre as 4 palavras, haja uma palavra dos pârametro de entrada, a mesma pode ser excluída, retorando menos palavras. Por exemplo, considerando \"**rei** está para **rainha** assim como **homem** está para...\", caso uma das palavras de saída para essa entrada  seja `rainha`, o método poderá retornar 3 palavras (eliminando a palavra rainha). Isso já é considerado no método `get_most_similar_embedding`. Lembre-se que o método `get_most_similar_embedding` é da classe KDTreeEmbedding e a `Analogy` possui o atributo `kdtree_embedding` que é uma instância da classe `KDTreeEmbedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m embeddings.embedding_tests TestEmbeddings.test_analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja as analogias (brinque à vontade com a representação em português e em inglês)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.utils import *\n",
    "dict_embedding = get_embedding( \"glove.pt.100.txt\",100)\n",
    "obj_analogy = Analogy(dict_embedding,\"kdtree.pt.p\")\n",
    "\n",
    "\n",
    "dict_analogias = {(\"brasil\",\"brasilia\"):[\"peru\",\"gana\",\"japão\",\"espanha\",\"india\"],\n",
    "                  (\"bahia\",\"salvador\"):[\"acre\",\"alagoas\",\"amapá\",\"amazonas\",\"ceará\",\"goiás\"],\n",
    "                  (\"brasil\",\"feijoada\"):[\"italia\",\"estados-unidos\",\"inglaterra\",\"argentina\",\"peru\"],\n",
    "                  (\"homem\",\"mulher\"):[\"garoto\",\"rei\",\"príncipe\",\"pai\",\"cavalo\",\"garçon\"],\n",
    "                  (\"grande\",\"pequeno\"):[\"cheio\",\"alto\",\"forte\",\"largo\"],\n",
    "                  (\"pelé\",\"futebol\"):[\"tyson\",\"bolt\",\"senna\"],\n",
    "                  (\"atena\",\"sabedoria\"):[\"afrodite\",\"poseidon\",\"zeus\",\"atena\"],\n",
    "                  (\"cruzeiro\",\"raposa\"):[\"atlético\",\"gremio\",\"palmeiras\",\"corinthians\"],\n",
    "                 }\n",
    "\n",
    "for (palavra,esta_para), arr_assim_como in dict_analogias.items():\n",
    "    print(f\"{palavra} está para {esta_para} assim como...\")\n",
    "    for assim_como in arr_assim_como:\n",
    "        palavras = obj_analogy.analogia(palavra,esta_para,assim_como)\n",
    "        print(f\"\\t{assim_como} está para {palavras[0]} (ou {palavras[1:]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algumas limitações desses embeddings é a dependência de idiomas e que palavras ambiguas não são tratadas. Por exemplo, Jaguar pode ser uma marca de carro ou um animal, dependendo do contexto.  Para diminuir o problema de ambuiguidades, o [BERT](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/) é um embedding que a representação da palavra é diferente de acordo com o seu contexto. O [MUSE](https://github.com/facebookresearch/MUSE) é um embedding multilingue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representação textual usando embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muitas vezes, precisamos de um único vetor para representar uma frase ou um texto ainda maior. Para isso, podemos usar a representação Bag of Words ou, ainda, representar por palavras chaves ou utilizarmos uma combinação de nossas representações por palavras. Neste tutorial, iremos mostrar como combinar embeddings de palavras e usar a representação por palavras chaves - podendo, inclusive, fazer uma expansão de palavras chaves por embeddings.\n",
    "\n",
    "Para isso, iremos usar o seguinte contexto: por meio de um dataset de revisões de produto da amazon, deseja-se prever automaticamente o sentimento do mesmo (positivo ou negativo). Utilizou-se uma amostra do [dataset do Kaggle para este exemplo](https://www.kaggle.com/bittlingmayer/amazonreviews). Veja abaixo o dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_amazon_reviews = pd.read_csv(\"datasets/amazon_reviews_mini.txt\",index_col=\"id\")\n",
    "df_amazon_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em um método de aprendizado de maquina, cada instância deve ser representada por um vetor numérico utilizando as representações ditas anteriormente. Iremos ilustrar cada exemplo utilizando uma pequena subamostra desta amostra com 5 exemplos positivos e 5 negativos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positive = df_amazon_reviews[df_amazon_reviews[\"class\"]==\"positive\"][:5]\n",
    "df_negative = df_amazon_reviews[df_amazon_reviews[\"class\"]==\"negative\"][:5]\n",
    "df_amazon_mini = pd.concat([df_positive,df_negative])\n",
    "df_amazon_mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of Words: ** um exemplo simples, sem usar embeddings, é a representação em bag of words, **já discutido aqui**. Assim, podemos  usar a classe `BagOfWords` que está no arquivo `textual_representation.py`. Para as representações bag of words, usaremos a função bag_of_words abaixo. Usando esta representação o nosso dataset ficaria representado da seguinte forma: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.textual_representation import BagOfWords\n",
    "#o vocabulario, quando vazio, será considerado todas as palavra (menos stopwords)\n",
    "def bag_of_words(data, vocabulary=None):\n",
    "    #obtem stopwords\n",
    "    stop_words = set()\n",
    "    with open(\"datasets/stopwords.txt\") as stop_file:\n",
    "        stop_words = set(stop_word[:-1] for stop_word in stop_file)\n",
    "\n",
    "    #instancia o bag of words, filtrando stopwords e considerando o vocabulario (se possivel)\n",
    "    bow = BagOfWords(\"bow\", stop_words=list(stop_words), words_to_consider=vocabulary)\n",
    "    \n",
    "    #o bag of words, é gerado separadamente a representação do treino e teste\n",
    "    #iremos usar apenas a representação considerando que \"data\" é o treino\n",
    "    data_preproc = bow.preprocess_train_dataset(data, \"class\")\n",
    "\n",
    "    #exibe apenas colunas não zedadas\n",
    "    m2 = (data_preproc != 0).any()\n",
    "    data_preproc = data_preproc[m2.index[m2].tolist()]\n",
    "    \n",
    "    return data_preproc\n",
    "bag_of_words(df_amazon_mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of words (filtrado por palavras chaves e embeddings similares)** Como bag of words é uma representação com milhares de atributos, poderiamos fazer uma restrição por palavras chaves. Por exemplo, caso usássemos como vocabulário do bag of words baseado nas palavras obtidas da roda de emoções proposta por [Scherer K., (2005)](https://journals.sagepub.com/doi/pdf/10.1177/0539018405058216): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_words = {\n",
    "                    \"pride\":{\"proud\"},\n",
    "                    \"elation\":{\"ecstatic\", \"euphoria\", \"exaltation\", \"exhilarating\"},\n",
    "                    \"happiness\":{\"joy\",\"cheer\", \"bliss\", \"delight\", \"enjoy\", \"happy\"},\n",
    "                    \"satisfaction\":{\"comfortable\",\"contentment\"},\n",
    "                    \"relief\":{},\n",
    "                    \"hope\":{\"buoyancy\", \"confident\", \"faith\", \"optimistic\"},\n",
    "                    \"interest\":{\"alert\", \"animation\", \"ardor\", \"curious\",\"enthusiasm\"},\n",
    "                    \"surprise\":{\"amazed\", \"astonishing\", \"dumbfounded\",\"thunderstruck\"},\n",
    "                    \"anxiety\":{\"anguish\",\"anxiety\",\"apprehensive\",\"jittery\",\"nervous\",\"worry\"},\n",
    "                    \"sadness\":{\"chagrin\", \"dejected\", \"gloom\", \"hopeless\", \"melancholy\", \"sad\", \"tear\"},\n",
    "                    \"boredom\":{\"ennui\",\"indifference\",\"tedious\"},\n",
    "                    \"shame\":{\"abashed\", \"ashamed\", \"embarrassing\", \"humiliating\"},\n",
    "                    \"guilt\":{\"blame\", \"contrition\", \"remorse\"},\n",
    "                    \"disgust\":{\"abhor\", \"aversion\", \"dislike\", \"disrelish\", \"nausea\",\"sick\"},\n",
    "                    \"contempt\":{\"denigration\",\"depreciate\",\"derision\",\"disdain\",\"scorn\"},\n",
    "                    \"hostile\":{},\n",
    "                    \"anger\":{\"anger\",\"angry\",\"furious\",\"fury\",\"incense\",\"infuriating\",\n",
    "                                \"mad\",\"rage\",\"resent\",\"temper\",\"wrath\"},\n",
    "                    \"recognition\":{\"respect\",\"acknowledgement\"}\n",
    "            }\n",
    "\n",
    "vocabulary = []\n",
    "for emotion_group, set_keywords in emotion_words.items():\n",
    "    vocabulary.append(emotion_group)\n",
    "    for word in set_keywords:\n",
    "        vocabulary.append(word)\n",
    "vocabulary = set(vocabulary)\n",
    "\", \".join(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O grande problema é que esse grupo de palavras é muito restrito. Veja como ficou a representação dos nossos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words(df_amazon_mini,vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembre-se que eliminamos as palavras que não apareceram em nenhuma instancia. Assim, como pode-se observar, apenas duas palavras foram usadas e alguns documentos não possuiam nenhuma palavra. Para ampliar o vocabulário, poderiamos expandir esta representação usando palavras similares a estas de acordo com o nosso embedding: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.utils import get_embedding, KDTreeEmbedding\n",
    "dict_embedding = get_embedding(\"glove.en.100.txt\")\n",
    "kdtree_embedding = KDTreeEmbedding(dict_embedding, \"kdt_en.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_expanded = []\n",
    "for word in vocabulary:\n",
    "    #obtem as 40 mais similares palavras de cada uma do vocab original\n",
    "    _,words = kdtree_embedding.get_most_similar_embedding(word,40)\n",
    "    vocabulary_expanded.extend(words)\n",
    "vocabulary_expanded = set(vocabulary_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja aqui as palavras usadas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\", \".join(vocabulary_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algumas palavras podem não estar relacionadas à emoção, porém, o método de aprendizado de máquina ainda é capaz de considerar palavras mais relevantes para uma determinada instancia, ignorando algum ruído. Veja como ficou a representação: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words(df_amazon_mini,vocabulary_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poderiamos agrupar as palavras chaves em conceitos, por exemplo, \"happiness\" ser sempre contabilizado quando houver um conjunto de palavras, por exemplo, '\"joy\",\"cheer\", \"bliss\", \"delight\", \"enjoy\", \"happy\"'. Porém, isso pode restringir muito o número de palavras e expandir com palavras usando embeddings, pode extrair palavras relacionadas com a emoção oposta (veja exemplo abaixo). Por isso, optamos por apresentar a representação usando bag of words. Mesmo assim, caso queira ver algum resultado dessa forma, a classe CountWords implementa expansão por grupos de palavras chaves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "distance, words = kdtree_embedding.get_most_similar_embedding(\"happy\",40)\n",
    "#veja que unhappy é relacionado com happy - além de outras palavras negativas e ruido\n",
    "\", \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.textual_representation import CountWords,InstanceWisePreprocess\n",
    "aggregate = CountWords(dict_embedding, emotion_words,max_distance=0.3)\n",
    "\n",
    "word_counter = InstanceWisePreprocess(\"word-counter\",aggregate)\n",
    "word_counter.preprocess_train_dataset(df_amazon_mini, \"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O max_distance é responsável por obter as palavras similares. Veja que diversos documentos negativos foram classificados com o grupo \"happiness\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Representação agregando embeddings das palavras:** Conforme proposto por [Shen et al.](https://arxiv.org/pdf/1805.09843.pdf), dado que uma frase é representado por um conjunto de embeddings $\\{e_1, e_2, ..., e_n\\}$  uma forma simples e que geralmente obtém resultados **comparáveis a métodos mais complexos** é fazer operações em cada dimensão do embedding, tais como: média e máximo por dimensão do embedding. Por exemplo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings de alguams palavras: \n",
    "dict_embedding = {'my':      [10, 11,14, 20, 15, 80],\n",
    "                  'house':   [11, 12,10, 24, 11, 30],\n",
    "                  'is':      [1,  3,  5, -1, 10, 20],\n",
    "                  'green':   [12,10, 20, 12, 10, 20]\n",
    "                   }\n",
    "#representação do texto \"my house is green\"\n",
    "arr_texto = \"my house is green\".split()\n",
    "arr_texto      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usando a média de cada dimensão dos embeddings:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def average_pooling(arr_texto, dim_embedding):\n",
    "    representacao = []\n",
    "    for i in range(dim_embedding):\n",
    "        #calcula a média da iésima posição do embedding\n",
    "        sum_pos = 0\n",
    "        for word in arr_texto:\n",
    "            sum_pos += dict_embedding[word][i]\n",
    "\n",
    "        representacao.append(sum_pos/len(arr_texto))\n",
    "    return representacao\n",
    "dim_embedding = 6\n",
    "representacao = average_pooling(arr_texto, dim_embedding)\n",
    "print(f\"Representação: {representacao}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usando o máximo de cada dimensão dos embeddings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_embedding = 6\n",
    "def max_pooling(arr_texto, dim_embedding):\n",
    "    representacao = []\n",
    "    for i in range(dim_embedding):\n",
    "        #calcula o valor máximo de cada iésima posição do embedding\n",
    "        first_word = arr_texto[0]\n",
    "        max_pos = dict_embedding[first_word][i]\n",
    "        for word in arr_texto[1:]:\n",
    "            if max_pos < dict_embedding[word][i]:\n",
    "                max_pos = dict_embedding[word][i]\n",
    "\n",
    "        representacao.append(max_pos)\n",
    "    return representacao\n",
    "representacao = max_pooling(arr_texto, dim_embedding)\n",
    "print(f\"Representação: {representacao}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como há palavras pouco relevantes (como stopwords) podemos remove-las e, também podemos utilizar apenas as palavras de um vocabulario controlado. Abaixo veja a representação. Como esta representação é vetorial, a mesma não é uma representação simples de ser entendida por humanos, porém, pode-se obter bons resultados. Você pode adicionar o vocabulario controlado ou as stopwords por meio dos parametros correpondentes. O parâmetro `aggregate_method` define se será feito um maximo ou média entre os embeddings colocando os valores `max` ou `avg`, respectivamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtem as stopwords\n",
    "stop_words = set()\n",
    "with open(\"datasets/stopwords.txt\") as stop_file:\n",
    "    stop_words = set(stop_word[:-1] for stop_word in stop_file)\n",
    "\n",
    "from embeddings.textual_representation import AggregateEmbeddings,InstanceWisePreprocess\n",
    "dict_embedding = get_embedding(\"glove.en.100.txt\")\n",
    "aggregate_keywords_exp = AggregateEmbeddings(dict_embedding, aggregate_method=\"avg\", \n",
    "                                            words_to_filter=stop_words, words_to_consider=vocabulary_expanded)\n",
    "emb_keywords_exp = InstanceWisePreprocess(\"emb_keywords_exp\",aggregate_keywords_exp)\n",
    "emb_keywords_exp.preprocess_train_dataset(df_amazon_mini, \"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação por meio de um método de aprendizado de máquina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os embeddings podem oferecer uma informação de proximidade de conceitos que o uso de Bag of Words não seria capaz. Mesmo assim, cada representação e preprocessamento tem sua vantagem e desvantagem e não existe um método que será sempre o melhor. Assim, para sabermos qual representação é melhor para uma tarefa, é importante avaliarmos em quais delas são maiores para a tarefa em questão. Como o foco desta prática não é a avaliação, iremos apenas apresentar o resultado, caso queira, você pode [assistir a video aula](https://www.youtube.com/watch?v=Ag06UuWTsr4&list=PLwIaU1DGYV6tUx10fCTw5aPnqypbbK_GJ&index=12) e [fazer a prática sobre avaliação](https://github.com/daniel-hasan/ap-de-maquina-cefetmg-avaliacao/archive/master.zip). Nesta parte, iremos apenas usar a avaliação para verificar qual método é melhor.  \n",
    "\n",
    "Para que esta seção seja auto contida, iremos fazer toda a preparação que fizemos nas seções anteriores\n",
    "\n",
    "**Criação da lista de stopwords e de vocabulário:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.utils import get_embedding, KDTreeEmbedding\n",
    "\n",
    "emotion_words = {\n",
    "                    \"pride\":{\"proud\"},\n",
    "                    \"elation\":{\"ecstatic\", \"euphoria\", \"exaltation\", \"exhilarating\"},#vs boredom\n",
    "                    \"happiness\":{\"joy\",\"cheer\", \"bliss\", \"delight\", \"enjoy\", \"happy\"},#vs sad\n",
    "                    \"satisfaction\":{\"comfortable\",\"contentment\"},#\n",
    "                    \"relief\":{},\n",
    "                    \"hope\":{\"buoyancy\", \"confident\", \"faith\", \"optimistic\"},\n",
    "                    \"interest\":{\"alert\", \"animation\", \"ardor\", \"curious\",\"enthusiasm\"},\n",
    "                    \"surprise\":{\"amazed\", \"astonishing\", \"dumbfounded\",\"thunderstruck\"},\n",
    "                    \"anxiety\":{\"anguish\",\"anxiety\",\"apprehensive\",\"jittery\",\"nervous\",\"worry\"},\n",
    "                    \"sadness\":{\"chagrin\", \"dejected\", \"gloom\", \"hopeless\", \"melancholy\", \"sad\", \"tear\"},\n",
    "                    \"boredom\":{\"ennui\",\"indifference\",\"tedious\"},\n",
    "                    \"shame\":{\"abashed\", \"ashamed\", \"embarrassing\", \"humiliating\"},\n",
    "                    \"guilt\":{\"blame\", \"contrition\", \"remorse\"},\n",
    "                    \"disgust\":{\"abhor\", \"aversion\", \"dislike\", \"disrelish\", \"nausea\",\"sick\"},\n",
    "                    \"contempt\":{\"denigration\",\"depreciate\",\"derision\",\"disdain\",\"scorn\"},\n",
    "                    \"hostile\":{},\n",
    "                    \"anger\":{\"anger\",\"angry\",\"furious\",\"fury\",\"incense\",\"infuriating\",\n",
    "                                \"mad\",\"rage\",\"resent\",\"temper\",\"wrath\"},\n",
    "                    \"recognition\":{\"respect\",\"acknowledgement\"}\n",
    "            }\n",
    "dict_embedding = get_embedding(\"glove.en.100.txt\") \n",
    "kdtree_embedding = KDTreeEmbedding(dict_embedding, \"kdt_en.p\")\n",
    "\n",
    "#obtem as stopwords\n",
    "stop_words = set()\n",
    "with open(\"datasets/stopwords.txt\") as stop_file:\n",
    "    stop_words = set(stop_word[:-1] for stop_word in stop_file)\n",
    "\n",
    "\n",
    "#palavras chaves a serem consideradas\n",
    "set_vocabulary = set()\n",
    "for key_word, arr_related_words in emotion_words.items():\n",
    "    set_vocabulary.add(key_word)\n",
    "    set_vocabulary = set_vocabulary | set(arr_related_words)\n",
    "\n",
    "#kdtree - para gerar o conjunto com palavras chaves e suas similares\n",
    "vocabulary_expanded = []\n",
    "for word in set_vocabulary:\n",
    "    _, words = kdtree_embedding.get_most_similar_embedding(word,60)\n",
    "    vocabulary_expanded.extend(words)\n",
    "vocabulary_expanded = set(vocabulary_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Representações usadas**: Iremos avaliar a filtragem de stopwords e usando um vocabulário restrito da representação bag of words e também da representação usando a média de embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.textual_representation import BagOfWords, AggregateEmbeddings,InstanceWisePreprocess\n",
    "\n",
    "#gera as representações\n",
    "aggregate = AggregateEmbeddings(dict_embedding, \"avg\")\n",
    "embedding = InstanceWisePreprocess(\"embbeding\",aggregate)\n",
    "\n",
    "aggregate_stop = AggregateEmbeddings(dict_embedding, \"avg\",words_to_filter=stop_words)\n",
    "emb_nostop = InstanceWisePreprocess(\"emb_nostop\",aggregate_stop)\n",
    "\n",
    "\n",
    "aggregate_keywords_exp = AggregateEmbeddings(dict_embedding, \"avg\",words_to_consider=vocabulary_expanded)\n",
    "emb_keywords_exp = InstanceWisePreprocess(\"emb_keywords_exp\",aggregate_keywords_exp)\n",
    "\n",
    "bow_keywords = BagOfWords(\"bow_keywords_exp\", words_to_consider=vocabulary_expanded)\n",
    "bow = BagOfWords(\"bow\", stop_words=stop_words)\n",
    "\n",
    "arr_representations = [embedding,emb_nostop, emb_keywords_exp, bow,bow_keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"datasets/amazon_reviews_mini.txt\",index_col=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, é executado um método de aprendizado  para cada representação. Esse processo pode demorar um pouco pois é feito a procura do melhor parametro do algoritmo. Algumas otimizações que talvez, você precise fazer é no arquivo `embedding/avaliacao_embedding.py` alterar o parametro `n_jobs` no método `obtem_metodo` da classe `OtimizacaoObjetivoRandomForest`. Esse parametro é responsável por utiizar mais threads ao executar o Random Forests.  O valor pode ser levemente inferior a quantidades de núcleos que seu computador tem, caso ele tenha mais de 2, caso contrário, o ideal é colocarmos `n_jobs=1`. Caso queira visualizar resultados mais rapidamente, diminua o valor da variável `num_trials` e `num_folds` abaixo. Atenção que `num_folds` deve ser um valor maior que um."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "from embeddings.avaliacao_embedding import calcula_experimento_representacao, OtimizacaoObjetivoRandomForest\n",
    "\n",
    "# Método de aprendizado de máquina a ser usado\n",
    "dict_metodo = {\"random_forest\":{\"classe_otimizacao\":OtimizacaoObjetivoRandomForest,\n",
    "                                \"sampler\":optuna.samplers.TPESampler(seed=1, n_startup_trials=10)},\n",
    "              }\n",
    "df_amazon_reviews = pd.read_csv(\"datasets/amazon_reviews_mini.txt\",index_col=\"id\")\n",
    "\n",
    "#executa experimento com a representacao determinada e o método\n",
    "for metodo, param_metodo in dict_metodo.items():\n",
    "    for representation in arr_representations:\n",
    "        print(f\"===== Representação: {representation.nome}\")\n",
    "        col_classe = \"class\"\n",
    "        num_folds = 5\n",
    "        num_folds_validacao = 3\n",
    "        num_trials = 100\n",
    "\n",
    "\n",
    "        nom_experimento = f\"{metodo}_\"+representation.nome\n",
    "        experimento = calcula_experimento_representacao(nom_experimento,representation,df_amazon_reviews,\n",
    "                                            col_classe,num_folds,num_folds_validacao,num_trials,\n",
    "                                            ClasseObjetivoOtimizacao=param_metodo['classe_otimizacao'],\n",
    "                                                sampler=param_metodo['sampler'])\n",
    "        print(f\"Representação: {representation.nome} concluida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como a experimentação é uma tarefa custosa, todos os resultados são salvos na pasta \"resultados\" - inclusive os valores dos parametros na classe optuna (a prática de avaliação apresenta mais detalhes da biblioteca Optuna). A macro f1 é uma métrica relacionada a taxa de acerto (se necessário, [veja a explicação neste video - tópico 2 e 3)](https://www.youtube.com/watch?v=u7o7CSeXaNs&list=PLwIaU1DGYV6tUx10fCTw5aPnqypbbK_GJ&index=13). Analise os resultados abaixo: qual representação foi melhor? A restrição de vocabulário ou eliminação de stopwords auxiliou? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from base_am.avaliacao import Experimento\n",
    "\n",
    "arr_resultado = []\n",
    "results_folder = \"resultados\"\n",
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)\n",
    "for resultado_csv in os.listdir(\"resultados\"):\n",
    "    if resultado_csv.endswith(\"csv\"):\n",
    "        nom_experimento = resultado_csv.split(\".\")[0]\n",
    "        \n",
    "        #carrega resultados previamente realizados\n",
    "        experimento = Experimento(nom_experimento,[])\n",
    "        experimento.carrega_resultados_existentes()\n",
    "        \n",
    "        #adiciona experimento\n",
    "        num_folds = len(experimento.resultados)\n",
    "        dict_resultados = {\"nom_experimento\":nom_experimento, \n",
    "                            \"macro-f1\":sum([r.macro_f1 for r in experimento.resultados])/num_folds}\n",
    "        #resultados por classe\n",
    "        for classe in experimento.resultados[0].mat_confusao.keys():\n",
    "\n",
    "            dict_resultados[f\"f1-{classe}\"] = sum([r.f1_por_classe[classe] for r in experimento.resultados])/num_folds\n",
    "            dict_resultados[f\"precision-{classe}\"] = sum([r.precisao[classe] for r in experimento.resultados])/num_folds\n",
    "            dict_resultados[f\"recall-{classe}\"] = sum([r.revocacao[classe] for r in experimento.resultados])/num_folds\n",
    "\n",
    "        arr_resultado.append(dict_resultados)\n",
    "\n",
    "pd.DataFrame.from_dict(arr_resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bolukbasi, T., Chang, K. W., Zou, J., Saligrama, V., & Kalai, A. (2016). **[Man is to computer programmer as woman is to homemaker? Debiasing word embeddings](https://arxiv.org/abs/1607.06520)**. \n",
    "\n",
    "Hartmann, N., Fonseca, E., Shulby, C., Treviso, M., Rodrigues, J., & Aluisio, S. (2017). [**Portuguese word embeddings: Evaluating on word analogies and natural language tasks.**](https://arxiv.org/abs/1708.06025)\n",
    "\n",
    "\n",
    "Pennington, J., Socher, R., & Manning, C. D. (2014, October).**[GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)**. In EMNLP 2015 \n",
    "\n",
    "\n",
    "Scherer, Klaus R. **[What are emotions? And how can they be measured?](https://journals.sagepub.com/doi/pdf/10.1177/0539018405058216)**. Social science information, v. 44, n. 4, p. 695-729, 2005.\n",
    "\n",
    "Shen, D., Wang, G., Wang, W., Min, M. R., Su, Q., Zhang, Y., Carin, L. (2018). [Baseline needs more love: On simple word-embedding-based models and associated pooling mechanisms](https://arxiv.org/pdf/1805.09843.pdf).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Licença Creative Commons\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br />Este obra está licenciado com uma Licença <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Atribuição-CompartilhaIgual 4.0 Internacional</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
