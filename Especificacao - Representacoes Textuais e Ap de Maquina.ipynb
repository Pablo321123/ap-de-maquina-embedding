{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta prática iremos apresentar o uso de embeddings. Para isso, você deve primeiro instalar as dependencias usando `pip install -r requirements.txt` (ou `pip3`, dependendo da forma que seu python está instalado)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente, você deverá baixar os repositórios em português e inglês e salvá-los na pasta `embedding_data` seguindo as seguintes instruções: \n",
    "\n",
    "- [No respositório da USP](http://www.nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc) baixe [este arquivo (Glove 100 dimensões)](http://143.107.183.175:22980/download.php?file=embeddings/glove/glove_s100.zip). Ele possui  um pouco mais de 600 mil palavras retiradas de textos de páginas Web tais como a Wikipedia e canais de notícias [(Hartmann et al., 2017)](https://arxiv.org/abs/1708.06025). Descomprima e renomeie o arquivo txt para `glove.pt.100.txt`.\n",
    "\n",
    "- No [repositório de Stanford](https://nlp.stanford.edu/projects/glove/), baixe [este arquivo](http://nlp.stanford.edu/data/glove.6B.zip) use o arquivo . Este arquivo compreende ~400 mil palavras de textos extraidos da Wikipédia e [GigaWord](https://catalog.ldc.upenn.edu/LDC2011T07) [(Pennington et al., 2015)](https://nlp.stanford.edu/pubs/glove.pdf). Descomprima e salve o arquivo com embeddings de 100 dimensões (nome `glove.6B.100d.txt`) na pasta `embedding_data` renomeando esse arquivo para `glove.en.100.txt`.\n",
    "\n",
    "Como você pode perceber, esta prática demandará um espaço livre em disco de aproximadamente 3GB. Os arquivos estão no seguinte formato: em cada linha, uma palavra e N valores representando o valor em cada uma das N dimensões do embedding desta palavra. Por exemplo, caso as palavras `casa`, `redondel` e `rei` sejam representadas por um embedding de 4 dimensões, uma possível representação seria:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "casa 0.12 0.1 0.5 -0.4\n",
    "redondel 0.2 0.1 -0.4 0.5\n",
    "rei 0.1 0.5 -0.1 0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função `get_embedding`, do arquivo `embeddings/utils.py` é responsável por ler esse arquivo e gerar um dicionário em que a chave é a palavra e o valor é sua representação por meio de embeddings. Para a  representação acima, a saída desta função seria seria: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dict_embedding_ex = {\n",
    "                        \"casa\":np.array([0.12,0.1,0.5,-0.4]),\n",
    "                        \"redondel\":np.array([0.2,0.1,-0.4,0.5]),\n",
    "                        \"rei\":np.array([0.1,0.5,-0.1,0.1]),\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa função, também é salvo o objeto criado usando [pickle](https://docs.python.org/3/library/pickle.html), assim, a próxima vez que seja lido o embedding, a leitura será mais rápida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 1 - obtenção do embedding**: Complete a função `get_embedding` obtendo a palavra e o vetor de embeddings com a dimensão `embeddings_size` substituindo os `None` apropriadamente. O dataset possui algumas incosistencias que você deve considerar ao modificar essas linhas: no dataset em português, a maioria das palavras compostas são separadas por hífen, porém, foi verificado que umas palavras foi separado por espaço. Por caso disso, você deve considerar que as `embeddings_size` últimas posições são os valores de cada dimensão, separados por espaço e, as demais, são a palavra. Sugiro \"brincar\" abaixo com o uso de [índice negativo](https://www.geeksforgeeks.org/python-negative-index-of-element-in-list/) entenda também o [método join](https://www.geeksforgeeks.org/join-function-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'pé de moleque 0.1 -0.5 0.5 0.1 -0.5': [ 0.1 -0.5  0.5  0.1 -0.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "linha = \"pé de moleque 0.1 -0.5 0.5 0.1 -0.5\"\n",
    "embedding_size = 5\n",
    "arr_line = linha.strip().split()\n",
    "\n",
    "word = \" \".join(arr_line[:])\n",
    "\n",
    "# colocamos float16 para economizar memória\n",
    "embedding = np.array(arr_line[3:], dtype=np.float16)\n",
    "print(f\"'{word}': {embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute o teste unitário abaixo para verificar o funcionamento do `get_embeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: rei\n",
      "Palavras ignoradas: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m embeddings.embedding_tests TestEmbeddings.test_get_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute os embeddings em português e ingles. Não se preocupe com as palavras ignoradas: foram algumas inconsistências no dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: the\n",
      "10000: persecution\n",
      "20000: baths\n",
      "30000: mortally\n",
      "40000: 1667\n",
      "50000: bec\n",
      "60000: baek\n",
      "70000: b/w\n",
      "80000: klinghoffer\n",
      "90000: azarov\n",
      "100000: capron\n",
      "110000: perpetua\n",
      "120000: biratnagar\n",
      "130000: 12.74\n",
      "140000: yaffa\n",
      "150000: cryogenics\n",
      "160000: ef1\n",
      "170000: franchetti\n",
      "180000: blintzes\n",
      "190000: birthstones\n",
      "200000: naadam\n",
      "210000: concertation\n",
      "220000: lesticus\n",
      "230000: containerboard\n",
      "240000: boydston\n",
      "250000: afterellen.com\n",
      "260000: acuff-rose\n",
      "270000: close-fitting\n",
      "280000: packbot\n",
      "290000: comptel\n",
      "300000: tanke\n",
      "310000: saraju\n",
      "320000: rouiba\n",
      "330000: discomfit\n",
      "340000: numurkah\n",
      "350000: hla-a\n",
      "360000: 90125\n",
      "370000: zipkin\n",
      "380000: lombarde\n",
      "390000: 1.137\n",
      "Palavras ignoradas: 0\n",
      "10000: distribuída\n",
      "20000: selena\n",
      "30000: sailor\n",
      "40000: aguaceiros\n",
      "50000: retrô\n",
      "60000: indesmentível\n",
      "70000: kouchner\n",
      "80000: hoya\n",
      "90000: j&f\n",
      "100000: castra\n",
      "110000: gynt\n",
      "120000: caddie\n",
      "130000: afluíam\n",
      "140000: nashua\n",
      "150000: amok\n",
      "160000: pormenorizou\n",
      "170000: otway\n",
      "180000: bandeirismo\n",
      "190000: críptico\n",
      "200000: kinyarwanda\n",
      "210000: yari\n",
      "220000: picotado\n",
      "230000: roberth\n",
      "240000: illex\n",
      "250000: og00\n",
      "260000: kalin\n",
      "270000: autoridadeslocais\n",
      "280000: goleava\n",
      "290000: mambos\n",
      "300000: interesado\n",
      "310000: cpdlc\n",
      "320000: samenwerkende\n",
      "330000: dimensсo\n",
      "340000: monteggia\n",
      "350000: sangrur\n",
      "360000: wuncler\n",
      "370000: villaputzu\n",
      "380000: zika.a\n",
      "390000: salvares\n",
      "400000: panik\n",
      "410000: hh000\n",
      "420000: boggies\n",
      "430000: super-licença\n",
      "440000: imeadiato\n",
      "450000: ad-libs\n",
      "460000: niinimaki\n",
      "470000: chhu\n",
      "480000: neuropáticas\n",
      "490000: atufando-se\n",
      "500000: megaigrejas\n",
      "510000: analisávamos\n",
      "520000: gitaigo\n",
      "530000: quichua\n",
      "540000: baiocchi\n",
      "550000: jeder\n",
      "560000: tadros\n",
      "570000: celebrou-a\n",
      "580000: hep-ph/0000000\n",
      "590000: palmview\n",
      "600000: tuyakbay\n",
      "610000: comapny\n",
      "620000: júnior.\n",
      "630000: reptiliomorfos\n",
      "640000: aglonas\n",
      "650000: coloniaes\n",
      "660000: frontalot\n",
      "670000: locomotivos\n",
      "680000: podlažice\n",
      "690000: tamta\n",
      "700000: alvadias\n",
      "710000: decoded\n",
      "720000: holder-bank\n",
      "730000: notificar-lhe\n",
      "740000: sipuncula\n",
      "750000: 0000pelos\n",
      "760000: batukada\n",
      "770000: conirostris\n",
      "780000: ergoespirometria\n",
      "790000: harleyville\n",
      "800000: lanlan\n",
      "810000: navigação\n",
      "820000: prolongara-se\n",
      "830000: sitoli\n",
      "840000: vassiljeva\n",
      "850000: ajuda-lhe\n",
      "860000: can-didaturas\n",
      "870000: dewar's\n",
      "880000: fritagelse\n",
      "890000: keppelmann\n",
      "900000: nauti\n",
      "910000: quarteirenses\n",
      "920000: successfactors\n",
      "Palavras ignoradas: 3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from embeddings.utils import get_embedding, plot_words_embeddings\n",
    "\n",
    "str_dataset = \"glove.en.100.txt\"\n",
    "dict_embedding_en = get_embedding(str_dataset)\n",
    "str_dataset = \"glove.pt.100.txt\"\n",
    "dict_embedding_pt = get_embedding(str_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `plot_words_embeddings` utiliza [Análise de Componentes Principais](https://pt.wikipedia.org/wiki/An%C3%A1lise_de_componentes_principais) (PCA, do inglês Principal Component Analisys) para reduzir cada embedding em 2 dimensões para, logo após, plotar em um grafico a posição dessas palavras de acordo com o embedding. Veja o gráfico apresentado abaixo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_to_use = {\n",
    "                        \"en\":\n",
    "                            {\"embedding\":dict_embedding_en,\n",
    "                            \"words_to_use\":[\"prince\",\"princess\",\n",
    "                                            \"duchess\", \"duke\", \"countess\", \"marquis\", \n",
    "                                            \"marquise\",\"king\",\"queen\",\n",
    "                                            \"girl\",\"boy\",\"man\",\"woman\",\"child\"]},\n",
    "\n",
    "                        \"pt\":{\"embedding\":dict_embedding_pt,\n",
    "                          \"words_to_use\":[\"principe\",\"rei\",\"rainha\",\"conde\",\"duquesa\",\"duque\",\"condessa\",\n",
    "                           \"marquês\",\"marquesa\",\n",
    "                           \"homem\",\"mulher\",\"princesa\",\"menina\",\"menino\",\"criança\",\n",
    "                           \"garoto\",\"garota\"]}\n",
    "                }\n",
    "\n",
    "language = \"pt\" #mude de 'pt' para 'en' para ver em ingles tb!\n",
    "plot_words_embeddings(embeddings_to_use[language][\"embedding\"], \n",
    "                    embeddings_to_use[language][\"words_to_use\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exemplo acima, em português, veja que podemos pensar em dois conceitos claramente divididos: a realeza e o gênero. Pense: neste plano cartesiano, qual eixo corresponde ao conceito de realeza? E o de gênero? Perceba que \"criança\" deveria ter gênero neutro - de fato, está mais próximo do zero. Porém, pode haver algum ruído associando a palavra criança ao genero feminino. Isso, em português, pode haver uma explicação, pois utilizamos o artigo `a`, usado para palavras que remetem ao genero feminino, para se referir a criança. Assim, em português, os artigos podem aproximar uma palavra de gênero neutro a um determinado gênero.\n",
    "\n",
    "\n",
    "Em inglês, não foi possível verificar tão bem a divisão entre os conceitos de `genero` e `realeza`. Isso pode ocorrer devido a redução de dimensionalidade: os conceitos não necessariamente correspondem a um eixo no plano cartesiano e, mesmo se corresponderem, ao mapear itens com $n$ dimensões para um plano bidimensional, pode haver perda de informação. Mesmo assim, conseguimos ver a separação entre palavras da realeza e que não são da realeza. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sinta-se livre para \"brincar\", alterando/adicionando palavras. Por exemplo, adicione animais. Devido à ambiguidades, ao dataset e à própria redução de dimensionalidade, podem existir palavras que estão erroneamente próximas, se considerarmos o conceito das mesmas,  principalmente se adicionarmos palavras de conceitos muito distintos. Um detalhe: no dataset em português, há uso de palavras compostas e elas estão (geralmente) separadas por hífen. No dataset em inglês não há palavras compostas.\n",
    "\n",
    "Tanto nesta tarefa quanto na próxima você poderá perceber que os embeddings podem carregar preconceitos. Há uma forma de modificar os vetores para eliminar um determinado tipo de preconceito. Por exemplo, nesses embeddings existirão palavras erronemente similares a um determinado genero e, para corrigir, é possível deixar todas as palavras sem distinção pelo genero. Caso queira saber como minimizar esse problema, veja o artigo \"[Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://arxiv.org/abs/1607.06520)\". O título do artigo se remete a um preconceito descoberto ao usar analogias, que será o próximo tópico desta prática. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação de analogias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra caracteristica muito interessante ao usar embedding é a criação de analogias. Por exemplo, na frase `homem está para mulher assim como rei está para...`, fazendo operações com os _embeddings_, muitas vezes é possível chegar na analogia mais provável que, neste caso, seria a palavra `rainha`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 2 - cálculo da analogia:** Nesta atividade, iremos implementar o método `calcula_embedding_analogia` da classe `Analogy`. Essa classe tem acesso ao dicionário de embeddings e a estrutura KDTree, que iremos explicá-la posteriormente. Considerando a frase <span style=\"color:red\">\"**palavra_x** está para **palavra_y** assim como **assim_como** esta para **palavra_z**\"</span>, o método `calcula_embedding_analogia` recebe como parâmetro as palavras `palavra_x`, `esta_para` e `assim_ como` e retorna um embedding que, possivelmente, será muito próximo da `palavra_z`. \n",
    "\n",
    "Veja [na aula](https://docs.google.com/presentation/d/1-CggYUA2s7LW7_LcnGv7vlpUGFg9kEWG0j6lWGUnaLI/edit?usp=sharing) como é feito o cálculo e, logo após, faça o teste unitário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.0, -1.0, -1.0, ]\n",
      "[ 12.296875, 53.09375, 30.984375, ]\n",
      "[ -10.96875, -30.90625, -9.6015625, ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[1, 2, 3],[-1.2, 3.2, 1.2],[12.2, 31.2, 11.2]], dtype=np.float16)\n",
    "esta_para = np.array([[-3, 0, 1],[11, 56, 32.2],[0, 0.2, 0.4]], dtype=np.float16)\n",
    "assim_como = np.array([[2, 1, 1],[0.1,0.3,0],[1.23, 0.1, 1.2]], dtype=np.float16)\n",
    "\n",
    "for i,x_val in enumerate(x):\n",
    "    arr_embedding = assim_como[i]-x[i]+esta_para[i]\n",
    "    print(\"[\",end=\" \")\n",
    "    for val in arr_embedding:\n",
    "        print(float(val),end=\", \")\n",
    "    print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m embeddings.embedding_tests TestEmbeddings.test_calculo_analogia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 3 - busca da palavra mais similar:** O cálculo da atividade anterior resultou em um embedding e, agora, precisamos  procuramos a palavra mais próxima a este embedding obtido. Para isso, precisamos de: (1) uma forma eficiente para percorrer os embeddings para descobrir o mais similar; (2) uma métrica de similaridade/distância; \n",
    "\n",
    "**Como percorrer embeddings?** Para encontrarmos os embeddings similares, uma alternativa seria percorrer todos os vetores de embeddings e encontrar o mais similar. Porém, como estamos trabalhando com centenas de milhares de embeddings, essa operação seria muito custosa. Para isso, podemos usar uma estrutura de dados chamada **KDTree**. KDtree é uma arvore que organiza dados espaciais de tal forma que conseguimos alcançar elementos similares de forma mais eficiente. Caso esteja interessado em mais detalhes, [veja este video](https://www.youtube.com/watch?v=Glp7THUpGow).\n",
    "\n",
    "**Qual métrica de distancia/similaridade usaremos?**  Já foi demonstrado que esta métrica é eficiente para similaridade entre embeddings é a distância euclidiana [(Pennington et al., 2015)](https://nlp.stanford.edu/pubs/glove.pdf). A [distancia euclidiana](https://pt.wikipedia.org/wiki/Dist%C3%A2ncia_euclidiana) entre dois pontos $p$ e $q$ é calculada por meio do tamanho da linha entre esses pontos. Para um espaço bidimensional, considerando que os pontos $p$ e $q$ são representados pelas coordenadas $(p_1,p_2)$ e $(q_1,q_2)$, respectivamente, a equação é dada pela seguinte fórmula: $d(p,q) = \\sqrt{(p_1-q_1)^2+(p_2-q_2)^2}$ veja uma representação gráfica: \n",
    "\n",
    "<img width=\"400px\" src=\"img/distancia_euclidiana.svg\">\n",
    "\n",
    "Esta métrica pode ser generalizada para um espaço n-dimensional e o cálculo seria: $d(p,q) = \\sqrt{(p_1-q_1)^2+(p_2-q_2)^2+...+(p_n-q_n)^n}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, nesta atividade iremos utilizar [a implementação do kdtree do scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html). Nessa estrutura, é possível armazenar os embeddings e, logo após fazer consultas eficientes para, por exemplo, procurar os k elementos mais próximos. Veja o exemplo abaixo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O ponto [3, 3] é o 1º ponto mais próximo de [3, 2] distância: 1.0\n",
      "O ponto [2, 2] é o 2º ponto mais próximo de [3, 2] distância: 1.0\n",
      "O ponto [1, 1] é o 3º ponto mais próximo de [3, 2] distância: 2.23606797749979\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "elementos = [[1,1],\n",
    "             [2,2],\n",
    "             [3,3],\n",
    "             [4,4],\n",
    "             [5,5],\n",
    "             [6,6],\n",
    "             ]\n",
    "#os elementos são passados como parametro na construção do KDTree junto com a métrica \n",
    "#de distancia que iremos usar\n",
    "kdtree = KDTree(elementos,  metric='euclidean')\n",
    "\n",
    "#retorna os 2 elementos mais próximos e sua distancia\n",
    "#como podemos fazer uma consulta por lista de pontos, temos que \n",
    "#passar uma lista de pontos como parametro\n",
    "ponto = [3,2]\n",
    "distancia,pos_mais_prox = kdtree.query([ponto], k=3, return_distance=True)\n",
    "for i,pos in enumerate(pos_mais_prox[0]):\n",
    "    elemento = elementos[pos]\n",
    "    distancia_ponto = distancia[0][i]\n",
    "    print(f\"O ponto {elemento} é o {i+1}º ponto mais próximo de {ponto} distância: {distancia_ponto}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dessa forma, cada embedding pode ser armazenado no KTree para, logo após, obtermos os embeddings mais próximos a um embedding em questão. Não é possível armazenar na estrutura do KDTree a palavra referente a cada embedding representado, por isso, armazenamos essa estrutura como um atributo da classe `KDTreeEmbedding` (arquivo `utils.py`) que armazena também os atributos `pos_to_word` mapeando, para cada posição a palavra correspondente e o atributo `word_to_pos` que faz o oposto: mapeia, para cada palavra, a posição correspondente. Veja no construtor de `KDTreeEmbedding` como é criado o KDTree. Nela, também será salvo um arquivo com a implementação do KDtree e os atributos `pot_to_word` e `word_to_pos` isso é necessário pois a criação da KDTree é muito custosa.\n",
    "\n",
    "\n",
    "Nesta atividade, você deverá implementar `get_most_similar_embedding` que obtém as $k$ palavras mais similares à palavra (ou embedding) representado pelo parâmetro `query` por meio do método `query` da KDTree. O parâmetro `query` pode ser a palavra (`string`) ou o proprio embedding (`np.array`). Logo após, implemente também o método `get_embeddings_by_similarity` que utiliza o método `query_radius` ([veja documentação](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree.query_radius)) que retorna todas as palavras que estão em um raio de `max_distance` da palavra alvo especificada pelo parametro `query`. Para ambas as implementações, utiliza-se o método `positions_to_word`, já implementado, para retornar as palavras de acordo com as posições indicadas. Caso haja alguma palavra a ser ignorada em `words_to_ignore` ela será excluída também no método `positions_to_word`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.006s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m embeddings.embedding_tests TestEmbeddings.test_get_most_similar_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m embeddings.embedding_tests TestEmbeddings.test_embeddings_by_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, você pode testar os métodos utilizando os datasets de embeddings. Lembre-se  que o KDTree pode demorar mais de 30 minutos para ser criado na primeira execução de cada idioma. Caso queira testar para o inglês, não esqueça de mudar de `\"kdtree.pt.p\"` para `\"kdtree.en.p\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000: distribuída\n",
      "20000: selena\n",
      "30000: sailor\n",
      "40000: aguaceiros\n",
      "50000: retrô\n",
      "60000: indesmentível\n",
      "70000: kouchner\n",
      "80000: hoya\n",
      "90000: j&f\n",
      "100000: castra\n",
      "110000: gynt\n",
      "120000: caddie\n",
      "130000: afluíam\n",
      "140000: nashua\n",
      "150000: amok\n",
      "160000: pormenorizou\n",
      "170000: otway\n",
      "180000: bandeirismo\n",
      "190000: críptico\n",
      "200000: kinyarwanda\n",
      "210000: yari\n",
      "220000: picotado\n",
      "230000: roberth\n",
      "240000: illex\n",
      "250000: og00\n",
      "260000: kalin\n",
      "270000: autoridadeslocais\n",
      "280000: goleava\n",
      "290000: mambos\n",
      "300000: interesado\n",
      "310000: cpdlc\n",
      "320000: samenwerkende\n",
      "330000: dimensсo\n",
      "340000: monteggia\n",
      "350000: sangrur\n",
      "360000: wuncler\n",
      "370000: villaputzu\n",
      "380000: zika.a\n",
      "390000: salvares\n",
      "400000: panik\n",
      "410000: hh000\n",
      "420000: boggies\n",
      "430000: super-licença\n",
      "440000: imeadiato\n",
      "450000: ad-libs\n",
      "460000: niinimaki\n",
      "470000: chhu\n",
      "480000: neuropáticas\n",
      "490000: atufando-se\n",
      "500000: megaigrejas\n",
      "510000: analisávamos\n",
      "520000: gitaigo\n",
      "530000: quichua\n",
      "540000: baiocchi\n",
      "550000: jeder\n",
      "560000: tadros\n",
      "570000: celebrou-a\n",
      "580000: hep-ph/0000000\n",
      "590000: palmview\n",
      "600000: tuyakbay\n",
      "610000: comapny\n",
      "620000: júnior.\n",
      "630000: reptiliomorfos\n",
      "640000: aglonas\n",
      "650000: coloniaes\n",
      "660000: frontalot\n",
      "670000: locomotivos\n",
      "680000: podlažice\n",
      "690000: tamta\n",
      "700000: alvadias\n",
      "710000: decoded\n",
      "720000: holder-bank\n",
      "730000: notificar-lhe\n",
      "740000: sipuncula\n",
      "750000: 0000pelos\n",
      "760000: batukada\n",
      "770000: conirostris\n",
      "780000: ergoespirometria\n",
      "790000: harleyville\n",
      "800000: lanlan\n",
      "810000: navigação\n",
      "820000: prolongara-se\n",
      "830000: sitoli\n",
      "840000: vassiljeva\n",
      "850000: ajuda-lhe\n",
      "860000: can-didaturas\n",
      "870000: dewar's\n",
      "880000: fritagelse\n",
      "890000: keppelmann\n",
      "900000: nauti\n",
      "910000: quarteirenses\n",
      "920000: successfactors\n",
      "Palavras ignoradas: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.0,\n",
       "  3.8529124618382,\n",
       "  3.879822890301186,\n",
       "  4.110205347652384,\n",
       "  4.330550049048636],\n",
       " ['carro', 'veículo', 'caminhão', 'motorista', 'moto'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embeddings.utils import get_embedding, KDTreeEmbedding\n",
    "str_dataset = \"glove.pt.100.txt\"\n",
    "kdtree_file = \"kdtree.pt.p\"\n",
    "dict_embedding = get_embedding(str_dataset)\n",
    "kdtree = KDTreeEmbedding(dict_embedding, kdtree_file)\n",
    "kdtree.get_most_similar_embedding(\"carro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 5 - 💞 apresentando as analogias 💞:** Agora você deverá implementar o método `analogia` da classe `Analogy` que deverá utilizar os métodos `calcula_embedding_analogia` e o `get_most_similar_embedding` para retornar as 4 palavras mais prováveis para completar uma determinada analogia, com os parâmetros indicados. Caso, dentre as 4 palavras, haja uma palavra dos pârametro de entrada, a mesma pode ser excluída, retorando menos palavras. Por exemplo, considerando \"**rei** está para **rainha** assim como **homem** está para...\", caso uma das palavras de saída para essa entrada  seja `rainha`, o método poderá retornar 3 palavras (eliminando a palavra rainha). Isso já é considerado no método `get_most_similar_embedding`. Lembre-se que o método `get_most_similar_embedding` é da classe KDTreeEmbedding e a `Analogy` possui o atributo `kdtree_embedding` que é uma instância da classe `KDTreeEmbedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rei rainha homem ['mulher', 'mg', 'amazonas', 'pascoa']\n",
      "pé de moleque junina ovo ['minas gerais', 'rei']\n",
      "minas gerais mg amazonas ['rainha', 'mulher', 'am']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m embeddings.embedding_tests TestEmbeddings.test_analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja as analogias (brinque à vontade com a representação em português e em inglês)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000: distribuída\n",
      "20000: selena\n",
      "30000: sailor\n",
      "40000: aguaceiros\n",
      "50000: retrô\n",
      "60000: indesmentível\n",
      "70000: kouchner\n",
      "80000: hoya\n",
      "90000: j&f\n",
      "100000: castra\n",
      "110000: gynt\n",
      "120000: caddie\n",
      "130000: afluíam\n",
      "140000: nashua\n",
      "150000: amok\n",
      "160000: pormenorizou\n",
      "170000: otway\n",
      "180000: bandeirismo\n",
      "190000: críptico\n",
      "200000: kinyarwanda\n",
      "210000: yari\n",
      "220000: picotado\n",
      "230000: roberth\n",
      "240000: illex\n",
      "250000: og00\n",
      "260000: kalin\n",
      "270000: autoridadeslocais\n",
      "280000: goleava\n",
      "290000: mambos\n",
      "300000: interesado\n",
      "310000: cpdlc\n",
      "320000: samenwerkende\n",
      "330000: dimensсo\n",
      "340000: monteggia\n",
      "350000: sangrur\n",
      "360000: wuncler\n",
      "370000: villaputzu\n",
      "380000: zika.a\n",
      "390000: salvares\n",
      "400000: panik\n",
      "410000: hh000\n",
      "420000: boggies\n",
      "430000: super-licença\n",
      "440000: imeadiato\n",
      "450000: ad-libs\n",
      "460000: niinimaki\n",
      "470000: chhu\n",
      "480000: neuropáticas\n",
      "490000: atufando-se\n",
      "500000: megaigrejas\n",
      "510000: analisávamos\n",
      "520000: gitaigo\n",
      "530000: quichua\n",
      "540000: baiocchi\n",
      "550000: jeder\n",
      "560000: tadros\n",
      "570000: celebrou-a\n",
      "580000: hep-ph/0000000\n",
      "590000: palmview\n",
      "600000: tuyakbay\n",
      "610000: comapny\n",
      "620000: júnior.\n",
      "630000: reptiliomorfos\n",
      "640000: aglonas\n",
      "650000: coloniaes\n",
      "660000: frontalot\n",
      "670000: locomotivos\n",
      "680000: podlažice\n",
      "690000: tamta\n",
      "700000: alvadias\n",
      "710000: decoded\n",
      "720000: holder-bank\n",
      "730000: notificar-lhe\n",
      "740000: sipuncula\n",
      "750000: 0000pelos\n",
      "760000: batukada\n",
      "770000: conirostris\n",
      "780000: ergoespirometria\n",
      "790000: harleyville\n",
      "800000: lanlan\n",
      "810000: navigação\n",
      "820000: prolongara-se\n",
      "830000: sitoli\n",
      "840000: vassiljeva\n",
      "850000: ajuda-lhe\n",
      "860000: can-didaturas\n",
      "870000: dewar's\n",
      "880000: fritagelse\n",
      "890000: keppelmann\n",
      "900000: nauti\n",
      "910000: quarteirenses\n",
      "920000: successfactors\n",
      "Palavras ignoradas: 3\n",
      "brasil está para brasilia assim como...\n",
      "brasil brasilia peru ['aneura', 'trong', 'lacrima', 'vorskla']\n",
      "\tperu está para aneura (ou ['trong', 'lacrima', 'vorskla'])\n",
      "brasil brasilia gana ['aneura', 'seraing', 'fene', 'dewsbury', 'gria']\n",
      "\tgana está para aneura (ou ['seraing', 'fene', 'dewsbury', 'gria'])\n",
      "brasil brasilia japão ['yeonpyeong', 'lieja', 'pottstown', 'belém-pa', 'flexi']\n",
      "\tjapão está para yeonpyeong (ou ['lieja', 'pottstown', 'belém-pa', 'flexi'])\n",
      "brasil brasilia espanha ['logroño', 'valladolid', 'kalapa', 'cádiz', 'baiona']\n",
      "\tespanha está para logroño (ou ['valladolid', 'kalapa', 'cádiz', 'baiona'])\n",
      "brasil brasilia india ['vjm00', 'mailman', 'excursion', 'nsi', 'clementia']\n",
      "\tindia está para vjm00 (ou ['mailman', 'excursion', 'nsi', 'clementia'])\n",
      "bahia está para salvador assim como...\n",
      "bahia salvador acre ['macapá', 'aracaju', 'cuiabá']\n",
      "\tacre está para macapá (ou ['aracaju', 'cuiabá'])\n",
      "bahia salvador alagoas ['aracaju', 'maceió', 'teresina']\n",
      "\talagoas está para aracaju (ou ['maceió', 'teresina'])\n",
      "bahia salvador amapá ['macapá', 'amazonas', 'xinguara']\n",
      "\tamapá está para macapá (ou ['amazonas', 'xinguara'])\n",
      "bahia salvador amazonas ['maceió', 'aracaju', 'macapá']\n",
      "\tamazonas está para maceió (ou ['aracaju', 'macapá'])\n",
      "bahia salvador ceará ['maceió', 'cuiabá', 'aracaju', 'recife']\n",
      "\tceará está para maceió (ou ['cuiabá', 'aracaju', 'recife'])\n",
      "bahia salvador goiás ['goiânia', 'cuiabá', 'aracaju', 'macapá']\n",
      "\tgoiás está para goiânia (ou ['cuiabá', 'aracaju', 'macapá'])\n",
      "brasil está para feijoada assim como...\n",
      "brasil feijoada italia ['esparguete', 'via-crúcis', 'negundo', 'pana', 'taxifolia']\n",
      "\titalia está para esparguete (ou ['via-crúcis', 'negundo', 'pana', 'taxifolia'])\n",
      "brasil feijoada estados-unidos ['cebolada', 'portugas', 'atribuindo-os', 'bróculos', 'surtida']\n",
      "\testados-unidos está para cebolada (ou ['portugas', 'atribuindo-os', 'bróculos', 'surtida'])\n",
      "brasil feijoada inglaterra ['worcestershire', 'falmouth', 'lincolnshire', 'lowestoft']\n",
      "\tinglaterra está para worcestershire (ou ['falmouth', 'lincolnshire', 'lowestoft'])\n",
      "brasil feijoada argentina ['retrete', 'esparguete', 'carnico', 'frita']\n",
      "\targentina está para retrete (ou ['esparguete', 'carnico', 'frita'])\n",
      "brasil feijoada peru ['frita', 'molho', 'guisado', 'assado']\n",
      "\tperu está para frita (ou ['molho', 'guisado', 'assado'])\n",
      "homem está para mulher assim como...\n",
      "homem mulher garoto ['menina', 'garota', 'namorada', 'mãe', 'irmã']\n",
      "\tgaroto está para menina (ou ['garota', 'namorada', 'mãe', 'irmã'])\n",
      "homem mulher rei ['rainha', 'princesa', 'esposa', 'príncipe']\n",
      "\trei está para rainha (ou ['princesa', 'esposa', 'príncipe'])\n",
      "homem mulher príncipe ['princesa', 'rainha', 'filha', 'esposa']\n",
      "\tpríncipe está para princesa (ou ['rainha', 'filha', 'esposa'])\n",
      "homem mulher pai ['filha', 'mãe', 'esposa', 'irmã', 'marido']\n",
      "\tpai está para filha (ou ['mãe', 'esposa', 'irmã', 'marido'])\n",
      "homem mulher cavalo ['dama', 'égua', 'carruagem', 'irmã']\n",
      "\tcavalo está para dama (ou ['égua', 'carruagem', 'irmã'])\n",
      "homem mulher garçon ['cabeleireira', 'edna', 'pescadora', 'elisabetha']\n",
      "\tgarçon está para cabeleireira (ou ['edna', 'pescadora', 'elisabetha'])\n",
      "grande está para pequeno assim como...\n",
      "grande pequeno cheio ['armário', 'saco', 'gato', 'minúsculo']\n",
      "\tcheio está para armário (ou ['saco', 'gato', 'minúsculo'])\n",
      "grande pequeno alto ['baixo', 'comprido', 'redondo']\n",
      "\talto está para baixo (ou ['comprido', 'redondo'])\n",
      "grande pequeno forte ['fraco', 'parecido', 'baixo']\n",
      "\tforte está para fraco (ou ['parecido', 'baixo'])\n",
      "grande pequeno largo ['beco', 'fronteiro', 'comprido']\n",
      "\tlargo está para beco (ou ['fronteiro', 'comprido'])\n",
      "pelé está para futebol assim como...\n",
      "pelé futebol tyson ['hóquei', 'basquetebol', 'campeão']\n",
      "\ttyson está para hóquei (ou ['basquetebol', 'campeão'])\n",
      "pelé futebol bolt ['hóquei', 'atletismo', 'ciclismo']\n",
      "\tbolt está para hóquei (ou ['atletismo', 'ciclismo'])\n",
      "pelé futebol senna ['ciclismo', 'liga', 'campeonato']\n",
      "\tsenna está para ciclismo (ou ['liga', 'campeonato'])\n",
      "atena está para sabedoria assim como...\n",
      "atena sabedoria afrodite ['bondade', 'harmonia', 'compaixão', 'benevolência']\n",
      "\tafrodite está para bondade (ou ['harmonia', 'compaixão', 'benevolência'])\n",
      "atena sabedoria poseidon ['inefável', 'sábio', 'bondade', 'intuição']\n",
      "\tposeidon está para inefável (ou ['sábio', 'bondade', 'intuição'])\n",
      "atena sabedoria zeus ['compaixão', 'bondade', 'alma', 'espiritual']\n",
      "\tzeus está para compaixão (ou ['bondade', 'alma', 'espiritual'])\n",
      "atena sabedoria atena ['bondade', 'serenidade', 'generosidade', 'compaixão']\n",
      "\tatena está para bondade (ou ['serenidade', 'generosidade', 'compaixão'])\n",
      "atlético está para galo assim como...\n",
      "atlético galo cruzeiro ['raposa', 'tricolor', 'tigre']\n",
      "\tcruzeiro está para raposa (ou ['tricolor', 'tigre'])\n",
      "atlético galo gremio ['bangalafumenga', 'krakauer', 'elande', 'belo-horizontino']\n",
      "\tgremio está para bangalafumenga (ou ['krakauer', 'elande', 'belo-horizontino'])\n",
      "atlético galo palmeiras ['tricolor', 'flu', 'alviverde']\n",
      "\tpalmeiras está para tricolor (ou ['flu', 'alviverde'])\n",
      "atlético galo corinthians ['timão', 'tricolor', 'palmeiras']\n",
      "\tcorinthians está para timão (ou ['tricolor', 'palmeiras'])\n"
     ]
    }
   ],
   "source": [
    "from embeddings.utils import *\n",
    "dict_embedding = get_embedding( \"glove.pt.100.txt\",100)\n",
    "obj_analogy = Analogy(dict_embedding,\"kdtree.pt.p\")\n",
    "\n",
    "\n",
    "dict_analogias = {(\"brasil\",\"brasilia\"):[\"peru\",\"gana\",\"japão\",\"espanha\",\"india\"],\n",
    "                  (\"bahia\",\"salvador\"):[\"acre\",\"alagoas\",\"amapá\",\"amazonas\",\"ceará\",\"goiás\"],\n",
    "                  (\"brasil\",\"feijoada\"):[\"italia\",\"estados-unidos\",\"inglaterra\",\"argentina\",\"peru\"],\n",
    "                  (\"homem\",\"mulher\"):[\"garoto\",\"rei\",\"príncipe\",\"pai\",\"cavalo\",\"garçon\"],\n",
    "                  (\"grande\",\"pequeno\"):[\"cheio\",\"alto\",\"forte\",\"largo\"],\n",
    "                  (\"pelé\",\"futebol\"):[\"tyson\",\"bolt\",\"senna\"],\n",
    "                  (\"atena\",\"sabedoria\"):[\"afrodite\",\"poseidon\",\"zeus\",\"atena\"],\n",
    "                  (\"atlético\",\"galo\"):[\"cruzeiro\",\"gremio\",\"palmeiras\",\"corinthians\"],\n",
    "                 }\n",
    "\n",
    "for (palavra,esta_para), arr_assim_como in dict_analogias.items():\n",
    "    print(f\"{palavra} está para {esta_para} assim como...\")\n",
    "    for assim_como in arr_assim_como:\n",
    "        palavras = obj_analogy.analogia(palavra,esta_para,assim_como)\n",
    "        print(f\"\\t{assim_como} está para {palavras[0]} (ou {palavras[1:]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algumas limitações desses embeddings é a dependência de idiomas e que palavras ambiguas não são tratadas. Por exemplo, Jaguar pode ser uma marca de carro ou um animal, dependendo do contexto.  Para diminuir o problema de ambuiguidades, o [BERT](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/) é um embedding que a representação da palavra é diferente de acordo com o seu contexto. O [MUSE](https://github.com/facebookresearch/MUSE) é um embedding multilingue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representação textual usando embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muitas vezes, precisamos de um único vetor para representar uma frase ou um texto ainda maior. Para isso, podemos usar a representação Bag of Words ou, ainda, representar por palavras chaves ou utilizarmos uma combinação de nossas representações por palavras. Neste tutorial, iremos mostrar como combinar embeddings de palavras e usar a representação por palavras chaves - podendo, inclusive, fazer uma expansão de palavras chaves por embeddings.\n",
    "\n",
    "Para isso, iremos usar o seguinte contexto: por meio de um dataset de revisões de produto da amazon, deseja-se prever automaticamente o sentimento do mesmo (positivo ou negativo). Utilizou-se uma amostra do [dataset do Kaggle para este exemplo](https://www.kaggle.com/bittlingmayer/amazonreviews). Veja abaixo o dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>Do NOT WASTE Your Time: This book, to put it n...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>Peels the paint off the walls: I first heard t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>History With Modern Appeal: This is a must rea...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>Worse Music cd ever: I tried putting this in a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>Deliberately Obtuse Nonsense: I don't know wha...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29215</th>\n",
       "      <td>Better than the movie?: YES! This book gets be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256457</th>\n",
       "      <td>The Best RE yet: This is the best in the RE se...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210215</th>\n",
       "      <td>What are they waiting for?: This has got to be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200693</th>\n",
       "      <td>Hollywood - promoting the Antichrist again?: C...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169551</th>\n",
       "      <td>Best American TV Series Ever: With its combina...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     class\n",
       "id                                                                 \n",
       "204215  Do NOT WASTE Your Time: This book, to put it n...  negative\n",
       "208138  Peels the paint off the walls: I first heard t...  positive\n",
       "157010  History With Modern Appeal: This is a must rea...  positive\n",
       "274316  Worse Music cd ever: I tried putting this in a...  negative\n",
       "57708   Deliberately Obtuse Nonsense: I don't know wha...  negative\n",
       "...                                                   ...       ...\n",
       "29215   Better than the movie?: YES! This book gets be...  positive\n",
       "256457  The Best RE yet: This is the best in the RE se...  positive\n",
       "210215  What are they waiting for?: This has got to be...  positive\n",
       "200693  Hollywood - promoting the Antichrist again?: C...  negative\n",
       "169551  Best American TV Series Ever: With its combina...  positive\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_amazon_reviews = pd.read_csv(\"datasets/amazon_reviews_mini.txt\",index_col=\"id\")\n",
    "df_amazon_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em um método de aprendizado de maquina, cada instância deve ser representada por um vetor numérico utilizando as representações ditas anteriormente. Iremos ilustrar cada exemplo utilizando uma pequena subamostra desta amostra com 5 exemplos positivos e 5 negativos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>Peels the paint off the walls: I first heard t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>History With Modern Appeal: This is a must rea...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>NIV Bible: The NIV Bible is good, but I wish I...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49225</th>\n",
       "      <td>pouch can be better: I recently bought it at A...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158265</th>\n",
       "      <td>Great book!: Dawn of a Thousand Nights is extr...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>Do NOT WASTE Your Time: This book, to put it n...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>Worse Music cd ever: I tried putting this in a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>Deliberately Obtuse Nonsense: I don't know wha...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200048</th>\n",
       "      <td>Disappointed: Very small wipes canister, not v...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60933</th>\n",
       "      <td>The most horrible Blu-Ray: First, Night scenes...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     class\n",
       "id                                                                 \n",
       "208138  Peels the paint off the walls: I first heard t...  positive\n",
       "157010  History With Modern Appeal: This is a must rea...  positive\n",
       "101657  NIV Bible: The NIV Bible is good, but I wish I...  positive\n",
       "49225   pouch can be better: I recently bought it at A...  positive\n",
       "158265  Great book!: Dawn of a Thousand Nights is extr...  positive\n",
       "204215  Do NOT WASTE Your Time: This book, to put it n...  negative\n",
       "274316  Worse Music cd ever: I tried putting this in a...  negative\n",
       "57708   Deliberately Obtuse Nonsense: I don't know wha...  negative\n",
       "200048  Disappointed: Very small wipes canister, not v...  negative\n",
       "60933   The most horrible Blu-Ray: First, Night scenes...  negative"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_positive = df_amazon_reviews[df_amazon_reviews[\"class\"]==\"positive\"][:5]\n",
    "df_negative = df_amazon_reviews[df_amazon_reviews[\"class\"]==\"negative\"][:5]\n",
    "df_amazon_mini = pd.concat([df_positive,df_negative])\n",
    "df_amazon_mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of Words:** um exemplo simples, sem usar embeddings, é a representação em bag of words, **já discutido aqui**. Assim, podemos  usar a classe `BagOfWords` que está no arquivo `textual_representation.py`. Para as representações bag of words, usaremos a função bag_of_words abaixo. Usando esta representação o nosso dataset ficaria representado da seguinte forma: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>13</th>\n",
       "      <th>16</th>\n",
       "      <th>18</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>70</th>\n",
       "      <th>absolute</th>\n",
       "      <th>actors</th>\n",
       "      <th>ahead</th>\n",
       "      <th>album</th>\n",
       "      <th>...</th>\n",
       "      <th>wooden</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "      <th>worse</th>\n",
       "      <th>worst</th>\n",
       "      <th>wow</th>\n",
       "      <th>wrenching</th>\n",
       "      <th>written</th>\n",
       "      <th>years</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.231984</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.239204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119602</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49225</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158265</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200048</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60933</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.273879</td>\n",
       "      <td>0.13694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              13        16        18        20        21        70  absolute  \\\n",
       "id                                                                             \n",
       "208138  0.115992  0.231984  0.115992  0.115992  0.115992  0.115992  0.115992   \n",
       "157010  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "101657  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "49225   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "158265  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "204215  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "274316  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "57708   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "200048  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "60933   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          actors     ahead     album  ...    wooden   working     world  \\\n",
       "id                                    ...                                 \n",
       "208138  0.000000  0.115992  0.115992  ...  0.000000  0.000000  0.000000   \n",
       "157010  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.239204   \n",
       "101657  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "49225   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "158265  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "204215  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "274316  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "57708   0.129047  0.000000  0.000000  ...  0.129047  0.000000  0.000000   \n",
       "200048  0.000000  0.000000  0.000000  ...  0.000000  0.223607  0.000000   \n",
       "60933   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "           worse     worst      wow  wrenching   written     years     class  \n",
       "id                                                                            \n",
       "208138  0.000000  0.000000  0.00000   0.115992  0.000000  0.000000  positive  \n",
       "157010  0.000000  0.000000  0.00000   0.000000  0.000000  0.119602  positive  \n",
       "101657  0.000000  0.000000  0.00000   0.000000  0.000000  0.000000  positive  \n",
       "49225   0.000000  0.000000  0.00000   0.000000  0.000000  0.000000  positive  \n",
       "158265  0.000000  0.000000  0.00000   0.000000  0.245462  0.000000  positive  \n",
       "204215  0.000000  0.000000  0.00000   0.000000  0.000000  0.000000  negative  \n",
       "274316  0.283463  0.000000  0.00000   0.000000  0.000000  0.000000  negative  \n",
       "57708   0.000000  0.000000  0.00000   0.000000  0.000000  0.000000  negative  \n",
       "200048  0.000000  0.000000  0.00000   0.000000  0.000000  0.000000  negative  \n",
       "60933   0.000000  0.273879  0.13694   0.000000  0.000000  0.000000  negative  \n",
       "\n",
       "[10 rows x 250 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embeddings.textual_representation import BagOfWords\n",
    "#o vocabulario, quando vazio, será considerado todas as palavra (menos stopwords)\n",
    "def bag_of_words(data, vocabulary=None):\n",
    "    #obtem stopwords\n",
    "    stop_words = set()\n",
    "    with open(\"datasets/stopwords.txt\") as stop_file:\n",
    "        stop_words = set(stop_word[:-1] for stop_word in stop_file)\n",
    "\n",
    "    #instancia o bag of words, filtrando stopwords e considerando o vocabulario (se possivel)\n",
    "    bow = BagOfWords(\"bow\", stop_words=list(stop_words), words_to_consider=vocabulary)\n",
    "    \n",
    "    #o bag of words, é gerado separadamente a representação do treino e teste\n",
    "    #iremos usar apenas a representação considerando que \"data\" é o treino\n",
    "    data_preproc = bow.preprocess_train_dataset(data, \"class\")\n",
    "\n",
    "    #exibe apenas colunas não zedadas\n",
    "    m2 = (data_preproc != 0).any()\n",
    "    data_preproc = data_preproc[m2.index[m2].tolist()]\n",
    "    \n",
    "    return data_preproc\n",
    "bag_of_words(df_amazon_mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of words (filtrado por palavras chaves e embeddings similares)** Como bag of words é uma representação com milhares de atributos, poderiamos fazer uma restrição por palavras chaves. Por exemplo, caso usássemos como vocabulário do bag of words baseado nas palavras obtidas da roda de emoções proposta por [Scherer K., (2005)](https://journals.sagepub.com/doi/pdf/10.1177/0539018405058216): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dislike, mad, aversion, surprise, buoyancy, shame, worry, chagrin, ennui, joy, thunderstruck, melancholy, satisfaction, proud, jittery, anxiety, indifference, nausea, alert, abhor, sick, guilt, disgust, angry, animation, incense, cheer, tear, ardor, rage, tedious, respect, contentment, acknowledgement, temper, nervous, hopeless, hope, faith, astonishing, exhilarating, contrition, depreciate, boredom, enthusiasm, dumbfounded, furious, infuriating, humiliating, optimistic, scorn, anger, ecstatic, gloom, derision, comfortable, abashed, bliss, amazed, pride, remorse, euphoria, sad, resent, curious, happiness, recognition, wrath, hostile, anguish, embarrassing, elation, relief, sadness, blame, disrelish, dejected, denigration, happy, delight, exaltation, confident, apprehensive, fury, interest, disdain, ashamed, contempt, enjoy'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_words = {\n",
    "                    \"pride\":{\"proud\"},\n",
    "                    \"elation\":{\"ecstatic\", \"euphoria\", \"exaltation\", \"exhilarating\"},\n",
    "                    \"happiness\":{\"joy\",\"cheer\", \"bliss\", \"delight\", \"enjoy\", \"happy\"},\n",
    "                    \"satisfaction\":{\"comfortable\",\"contentment\"},\n",
    "                    \"relief\":{},\n",
    "                    \"hope\":{\"buoyancy\", \"confident\", \"faith\", \"optimistic\"},\n",
    "                    \"interest\":{\"alert\", \"animation\", \"ardor\", \"curious\",\"enthusiasm\"},\n",
    "                    \"surprise\":{\"amazed\", \"astonishing\", \"dumbfounded\",\"thunderstruck\"},\n",
    "                    \"anxiety\":{\"anguish\",\"anxiety\",\"apprehensive\",\"jittery\",\"nervous\",\"worry\"},\n",
    "                    \"sadness\":{\"chagrin\", \"dejected\", \"gloom\", \"hopeless\", \"melancholy\", \"sad\", \"tear\"},\n",
    "                    \"boredom\":{\"ennui\",\"indifference\",\"tedious\"},\n",
    "                    \"shame\":{\"abashed\", \"ashamed\", \"embarrassing\", \"humiliating\"},\n",
    "                    \"guilt\":{\"blame\", \"contrition\", \"remorse\"},\n",
    "                    \"disgust\":{\"abhor\", \"aversion\", \"dislike\", \"disrelish\", \"nausea\",\"sick\"},\n",
    "                    \"contempt\":{\"denigration\",\"depreciate\",\"derision\",\"disdain\",\"scorn\"},\n",
    "                    \"hostile\":{},\n",
    "                    \"anger\":{\"anger\",\"angry\",\"furious\",\"fury\",\"incense\",\"infuriating\",\n",
    "                                \"mad\",\"rage\",\"resent\",\"temper\",\"wrath\"},\n",
    "                    \"recognition\":{\"respect\",\"acknowledgement\"}\n",
    "            }\n",
    "\n",
    "vocabulary = []\n",
    "for emotion_group, set_keywords in emotion_words.items():\n",
    "    vocabulary.append(emotion_group)\n",
    "    for word in set_keywords:\n",
    "        vocabulary.append(word)\n",
    "vocabulary = set(vocabulary)\n",
    "\", \".join(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O grande problema é que esse grupo de palavras é muito restrito. Veja como ficou a representação dos nossos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interest</th>\n",
       "      <th>sick</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49225</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158265</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200048</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60933</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        interest  sick     class\n",
       "id                              \n",
       "208138       0.0   0.0  positive\n",
       "157010       1.0   0.0  positive\n",
       "101657       0.0   0.0  positive\n",
       "49225        0.0   0.0  positive\n",
       "158265       0.0   0.0  positive\n",
       "204215       0.0   0.0  negative\n",
       "274316       0.0   0.0  negative\n",
       "57708        0.0   1.0  negative\n",
       "200048       0.0   0.0  negative\n",
       "60933        0.0   0.0  negative"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words(df_amazon_mini,vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembre-se que eliminamos as palavras que não apareceram em nenhuma instancia. Assim, como pode-se observar, apenas duas palavras foram usadas e alguns documentos não possuiam nenhuma palavra. Para ampliar o vocabulário, poderiamos expandir esta representação usando palavras similares a estas de acordo com o nosso embedding: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: the\n",
      "10000: persecution\n",
      "20000: baths\n",
      "30000: mortally\n",
      "40000: 1667\n",
      "50000: bec\n",
      "60000: baek\n",
      "70000: b/w\n",
      "80000: klinghoffer\n",
      "90000: azarov\n",
      "100000: capron\n",
      "110000: perpetua\n",
      "120000: biratnagar\n",
      "130000: 12.74\n",
      "140000: yaffa\n",
      "150000: cryogenics\n",
      "160000: ef1\n",
      "170000: franchetti\n",
      "180000: blintzes\n",
      "190000: birthstones\n",
      "200000: naadam\n",
      "210000: concertation\n",
      "220000: lesticus\n",
      "230000: containerboard\n",
      "240000: boydston\n",
      "250000: afterellen.com\n",
      "260000: acuff-rose\n",
      "270000: close-fitting\n",
      "280000: packbot\n",
      "290000: comptel\n",
      "300000: tanke\n",
      "310000: saraju\n",
      "320000: rouiba\n",
      "330000: discomfit\n",
      "340000: numurkah\n",
      "350000: hla-a\n",
      "360000: 90125\n",
      "370000: zipkin\n",
      "380000: lombarde\n",
      "390000: 1.137\n",
      "Palavras ignoradas: 0\n"
     ]
    }
   ],
   "source": [
    "from embeddings.utils import get_embedding, KDTreeEmbedding\n",
    "dict_embedding = get_embedding(\"glove.en.100.txt\")\n",
    "kdtree_embedding = KDTreeEmbedding(dict_embedding, \"kdt_en.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_expanded = []\n",
    "for word in vocabulary:\n",
    "    #obtem as 40 mais similares palavras de cada uma do vocab original\n",
    "    _,words = kdtree_embedding.get_most_similar_embedding(word,40)\n",
    "    vocabulary_expanded.extend(words)\n",
    "vocabulary_expanded = set(vocabulary_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja aqui as palavras usadas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"understandably, aversion, firestorm, lg03, awesome, spirited, diarrhea, rollicking, deform, manoeuvre, invective, animators, inflexibility, firecrackers, annoyance, insensitivity, respecting, realizing, soulful, lest, opportunity, nonplussed, raisonnable, spirit, exclusivism, defeats, dislikes, canisters, grossness, www.slarmy.org, .0206, respect, dumbstruck, despair, concur, sorrow, decent, defensiveness, humble, temperament, indignant, threat, triumphalism, cheerily, carnality, neither, ungenerous, blames, biotechtrst, doubles_biggio, exhilarating, obsession, boredom, qualms, disorientation, exploding, ingenuous, everybody, nasty, humiliating, wondrous, breathless, thrown, befuddling, permanence, disgusted, shrewdness, mtow, fear, slovenliness, gone, laziness, joyous, ignominious, honor, panicky, suzuya, rebukes, elegiac, jealousy, fact, nice, precaution, meant, http://www.mediabynumbers.com, soothing, thus, twitchy, regret, luck, complained, compensators, fulfillment, anti-clericalism, easy, overconfidence, clamoring, entrancing, jubilation, disheartening, insisted, responding, frenzied, shields, crankiness, academicism, ashamed, mendacity, heartbroken, adore, stirred, rousing, talky, cowardliness, self-pity, demonstrate, sparks, brought, putting, kalamity, shaken, always, ennui, denial, unleash, tendency, simply, doctrine, sadly, lament, detest, mortifying, inscrutable, slander, bloating, 28aou94, romanticize, wanting, esteem, fearing, depressing, dog, rectitude, cannons, mocks, contribution, honored, excited, calumny, relentless, popi, puzzlement, yearning, laughter, mind, repentance, reflecting, provided, worship, perplexity, exhilarated, bizarre, heartless, giving, saw, unworldly, terrifying, intense, eagerness, reflexively, .0342, horrified, suspicion, debasement, inattention, admirable, orderliness, feelings, bringing, allusion, fend, rates, odd, documentary, misfortune, liveliness, embarrassment, 'm, canonicus, misery, give, presumption, gloom, equate, intimation, need, distrust, check, religion, poignant, amazed, pride, frightened, reason, engaged, resent, admired, curious, higher, reprisal, resultant, sublime, skyrocket, intrigued, thrilled, bullish, signal, distraught, aches, frenzy, loveliness, compassion, rootlessness, cushioning, pregnant, cries, individualization, delirious, healthy, takeover, pity, agitated, monotony, anxieties, destitute, str94, libations, aback, filmmakers, aggressiveness, fired, weekend, nosebleeds, terrified, headache, bullishness, immortality, predicament, ethereal, accustomed, brutalization, protesters, 're, unsatisfying, .0207, .0163, disappointment, sassacus, avoid, legitimacy, joke, expectations, captivating, scorned, expressing, apologise, peeved, eloquence, recognized, foolish, ironically, bemused, mystifying, survivability, cowardice, pig, ill, expecting, repetitious, symbolized, see, skeptical, engage, awe, depletes, over-produced, regard, indestructibility, migraine, forlorn, plaudits, latest, assure, believer, cheer, generalise, engaging, outrage, trepidation, puzzling, sympathize, radiance, really, lechery, disrespected, frustrations, betrayed, brightly, temper, tiredness, betrayal, greenness, delightful, soothed, cheered, vulnerable, surreal, dramatic, evacuation, bemoan, exoneration, soon, tumult, efforts, resents, fatigue, selflessness, impression, clarity, furthermore, dissatisfaction, lust, frightening, acknowledge, protestors, surveillance, alerting, excruciatingly, rebuilding, stay, afraid, uninterested, disgruntled, blurbed, clumsiness, fearlessness, overanalyze, js04bb, spared, strangeness, unquestionable, guileless, intimidated, relaxed, topside, anguish, demonization, keeping, deprogrammed, ..., coloradans, cramping, 24aou94, grief, miffed, mindful, drohs, scornful, insolence, peppermints, entranced, reluctance, spraying, lending, understand, fahnt, ingratitude, despised, moral, hoped, euoplocephalus, enamored, vitriol, brominated, distrusted, complacent, thunderstruck, ironic, engendering, greatness, satisfaction, pointless, overconfident, pleasure, painstaking, dignity, covetousness, affirmation, culpability, apparent, detractors, remarkable, irreverence, ought, maybe, longevity, complain, crowds, confidence, delighted, referring, gratitude, pranked, regretful, sentimentality, genuine, fatigued, impressive, heartfelt, cared, inability, accepted, transcendence, kroyts, imagine, uselessness, votives, earthiness, retardants, debase, thoughtlessness, liking, mothers, coded, astounding, frantic, pleased, wondering, fool, vomiting, intemperance, unrelenting, palpable, haunting, wearying, swings, callous, unconvincing, hydrodynamic, encouragement, ambition, committment, spurred, scorn, glib, transference, greet, ready, torment, itching, wanderlust, remembered, explain, put, suggestion, worse, forget, bittersweet, hopefulness, theatricality, distracted, theatrical, current, shelter, gph04bb, unsurprisingly, alerted, dissapointed, belief, shortness, pleasantness, undeterred, gentleness, http://www.opel.com, particular, hurry, fearful, arrogance, inspiration, lobbing, sensuous, silliness, upsetting, loyalty, expectation, astonished, relief, loathe, providing, vulnerability, turning, incredible, confess, celebration, despondent, despondency, interestingly, staying, lonely, unpleasant, perturbed, goal_montreal, eager, thoughtfulness, passions, here, js94bb, weary, dehumanization, devaluating, convinced, unimpressed, success, tranquillity, amused, apologize, insouciance, formulaic, disinterest, open-mindedness, upbeat, perfidy, unremorseful, attentiveness, christ, shocking, undecipherable, counter, hilarity, dispatched, uninteresting, equal, heartwarming, bombast, crispness, misunderstand, sprayed, trouble, hypocrisy, bulletinyyy, guilt, _____________________________________________, awestruck, incense, despise, apathy, discombobulated, goofiness, ardor, pessimism, swoon, castigation, tedious, incredulous, assistance, fortunate, scared, seductive, callousness, warn, mortified, discontent, disaster, resenting, overlong, ferocity, pervading, apology, panicked, insanity, greeted, embarassing, vivacity, distaste, demoralising, unnerved, throwing, upset, ignorance, rebelliousness, angst, unflustered, astonishment, sure, lucky, anyway, vigilant, yearn, kindling, cow, emergency, acetylene, looks, frivolity, joyless, dizzying, euphoria, scented, annoyed, god, time-consuming, exuberant, organgn, films, invoking, unsettled, hungry, focus, solipsism, hopes, wrath, destitution, actions, admiration, disillusionment, looking, spray, burned, wretched, clumsy, visualise, lollipops, hopeful, fondness, spooked, garlands, delight, shocked, raised, impatience, interesting, belated, kudos, absurdity, fascinating, exhilaration, contrarians, solitude, going, integrity, sociability, explode, greg.wilcoxdailynews.com, disney, dreamy, felicitously, vituperation, compensator, appreciating, want, unhappiness, unease, imperieuse, engrossing, puppetry, aware, hypnotised, livid, breathtaking, exactitude, ill-founded, unironic, seeing, definitely, admitting, assurance, indifferent, exasperating, sick, untruth, propriety, thing, stress, clear, unbelievable, unambiguous, fit, shyness, warlike, depreciated, darkening, accuse, constricting, admiring, feeling, purifier, frazzled, distasteful, rage, acknowledgement, bad, joyful, myrrh, mania, exoticism, reverence, stabiliser, riot, humiliated, endearing, sullen, excitement, boldness, splendid, chore, deflate, believing, infuriating, anger, monotonous, votive, quietude, satisfied, wonder, perfectionism, announcement, sympathetic, depress, exasperated, crazy, playful, bb94, admonition, lightheadedness, sweating, determination, frankness, happiness, awful, recognition, wisdom, naborsind, value, cautiously, competence, impishly, feel, .0208, fretted, individuality, poignancy, lately, future, coldness, keep, plodding, shrilly, monster, furor, hopelessness, cheerfulness, achievements, detestation, mournful, dying, bewilderment, burning, hauteur, think, doom, gleeful, lyricism, cinematic, studio, bravado, jolting, rehabilitation, groggy, idealize, enthused, scare, stylish, discouraged, magnificent, oblivious, unfairness, prideful, omniscience, unsettling, insist, triumph, believe, cry, assist, 3-d, apologies, heartache, undeniable, humility, omission, uncertainty, distracting, behest, envy, confusion, creations, impress, termed, incomparable, merriment, claustrophobia, sending, flabbergasted, watching, tear, effort, grumble, shamed, stupendous, messy, sense, craving, malevolent, action, gyroscopic, surprises, optimism, outcry, grandiosity, tempered, didacticism, symbolize, cautioned, sorry, snobbery, reminded, david.lazarus@latimes.com, plenty, guffaws, hopeless, hope, acknowledging, stubbornness, raucous, treated, bitterness, helpless, k587-1, depreciate, hatred, uncomfortable, disgrace, appreciation, irritating, degrading, musculature, suggests, better, erratic, em96, retribution, irrepressible, fabulousness, illumined, physicality, cushioned, shouts, bullets, airiness, continuing, divine, thrill, cue, discomfiting, desire, rescue, cartoon, aid, longing, protectiveness, wafted, ornery, rallying, unfathomable, thanks, resentful, imprimatur, bewildered, demonstrators, antagonistic, addition, complaining, phenomenal, infuriates, lit, perturb, prospects, ambuscade, needs, attacking, raising, impatient, delights, infuriated, achievement, irritation, panache, displeased, minions, stupidity, insomnia, inclusion, troubling, loneliness, hypocritical, batons, nerves, restlessness, video, awed, pellets, edginess, patterson, privilege, cramps, doubt, coarseness, praise, attitude, newborn, chagrin, perfect, self-love, wary, hurled, anticipate, calmed, melancholic, solid, misunderstood, drudgery, alert, insinuation, expect, admire, malevolence, soulfulness, warrant, manner, good, sorrowfully, embittered, looked, forgiveness, homesickness, thank, affectation, firing, uneasiness, contentment, prosecuted, riveting, creative, hardly, panic, worried, spite, disgusting, bb96, immediate, demonstrates, oftentimes, occasion, implying, unfortunate, 'd, spirituality, tearing, supplant, ridiculousness, everyone, given, penury, optimistic, glum, townsfolk, pessimistic, outbursts, comfortable, certain, thankful, overact, likening, baffled, price, .0202, propensity, engagement, nonetheless, unprepared, love, speechless, friendliness, inevitable, skittish, disgraceful, sparklers, blaming, stunned, exasperation, needed, ghastly, insularity, profess, vilification, rather, underwhelmed, filmmaking, spend, cannon, courtesy, blame, liken, fascination, signifying, appalling, rubber, shellshocked, abasement, discomfort, devaluate, reassurances, cat, antagonism, rejecting, disillusion, drubbing, candle, exuberance, enchanting, care, distraction, ferocious, confused, achieved, uninitiated, yet, evocative, sluggishness, bedraggled, enthralling, wonderment, dullness, indulged, giddy, zeal, suited, insult, importantly, joy, feature, surprised, neuroses, melancholy, guiltily, curiosity, sanguine, know, anxiety, mistake, maddening, implication, circumspect, crestfallen, appreciate, mystified, proclivities, jumpy, artfulness, fantastic, ordeal, comfy, getting, reticent, decry, flamboyance, constipation, mishandle, petulance, youthfulness, stupefied, demeaning, hug, maneuverability, licentiousness, tedium, beeswax, http://www.nifc.gov/, acceptance, pitiful, restless, excellence, alarmed, faith, outraged, unexcited, treat, insipid, mockery, belligerent, enthusiasm, heaped, jaundice, lifeline, cheering, increasing, bb97, bruising, amazing, edgy, disappointed, recklessness, evildoers, mesmerizing, emotionalism, pictures, visual, revulsion, photography, abashed, angered, beast, frankly, explicitness, perceive, passivity, timidity, error-prone, crunch, fierce, hubbub, helplessness, failure, depreciating, sad, humbling, recognising, testifying, vruhl, agitate, infused, depreciates, constrain, overjoyed, enigmatically, paranoia, agony, interactive, enjoying, concern, unfocused, judgment, symbolizes, boisterous, unsure, likely, prefer, preoccupation, desultory, disheartened, censers, pleasurably, malice, denunciation, indignation, tiring, bereft, significance, empathy, srivalo, fergalicious, viciousness, desperation, redemption, weariness, unexpected, re-visited, wanted, recognize, compelled, horrific, wondered, brave, beguiling, cathartic, asserting, emotions, diminishes, experience, displeasing, kd97, shock, presume, truly, frustration, plausibility, solemnity, paradoxically, prosecution, comfortably, heightened, emasculation, oly-2004-fhockey, resisting, nausea, stranger, angry, ambivalence, threats, precocity, ambivalent, contrary, reproach, indication, dismaying, jingoism, baffling, apologizing, peevish, copal, disliked, viewed, meekness, circularity, censer, forsake, perceived, astonishing, assured, trivialize, distressing, participation, wishes, nowhere, overwrought, glad, notice, ascribe, magnanimity, revelations, sort, devalue, revenge, hell, x.xx.xx.xx.x, discomfited, grotesque, searing, grenades, alerts, convoluted, piety, consternation, overabundance, lambasting, unleashed, otherwise, envious, soothe, puzzled, abandon, kd96, dismayed, danger, fatalism, risible, ignored, 30-270, startled, live-action, unassertive, gobsmacked, afflicted, parents, reflected, evocation, warned, brashness, sinicization, recovery, uncouth, alarms, clamor, elation, sickness, pleasantly, shadows, incredulity, complacency, raise, heartbreaking, sorrowful, nowadays, exaltation, aggressive, action-packed, shouting, nostalgia, contend, apprehensive, exhausting, vwahr, awkwardness, thankless, tantrums, unserious, spellbinding, hesitant, protesting, dejection, buoyancy, concision, worry, gentility, fiery, advocation, wearisome, euphoric, dispersed, disturbing, strange, productions, recognised, idleness, compounded, sticks, symptoms, gripped, flood, playfulness, alarm, cuteness, effervescence, headaches, teargas, resentment, aghast, rattled, stressing, studios, downcast, sandalwood, cgi, ugly, menace, upon, feistiness, sleeplessness, dampened, adventurousness, ill-informed, bdb94, animator, despite, warnings, terrible, cautious, contrition, patient, questioning, enjoyable, disregard, masterful, followed, buoyant, affectations, remorseful, essence, resented, candor, inviting, bloodlust, ridicule, kid, vengeance, dispersing, irony, quietness, denouncement, disperse, selfishness, confronted, rw95, insulted, grace, unnerving, legacy, disconsolate, skillz, heartburn, recognizing, disbelief, ardour, provoked, curiously, leniency, irate, chance, aimlessness, amazement, evident, truth, dazzling, pains, cheerleaders, ponderous, wrong, laborious, benumbed, wonderful, horrendous, hesitation, ethos, ineptitude, dismay, borrowing, come, cheers, stop-motion, irritated, cruel, condescension, lanterns, celebrating, fervour, surprising, wo, support, vigilance, contempt, concerned, tiresome, unbothered, hipness, shout, storytelling, nightmare, earnestness, multimedia, mercy, depredations, mayhem, exhortation, principle, deliriously, disrespect, merits, 65stk, lopsided, gloomy, proud, surely, crowd, perfection, graciousness, much, biodegrade, inadequacy, abhor, tempted, acknowledgment, disgust, enlightening, flustered, mo95, usual, fatuous, k978-1, drowsiness, sight, humaneness, repudiate, superlatives, antsy, ignoring, reassured, rejoicing, unlikely, suspenseful, perspire, brokenhearted, purposefulness, ignore, palpitations, maelstrom, independency, stupid, geotagging, shamefaced, malaise, importance, thrilling, insufferable, christian, leave, ecstatic, expressed, bliss, combativeness, remorse, crazed, menacing, camphor, prompted, understanding, k977-1, sloppiness, southpaws, fervor, beenz, scandalous, although, rioters, babies, lucidity, fixated, predilection, goodness, stones, canister, coming, neediness, help, payback, enjoys, dejected, uncharacteristic, penchant, withering, epically, uproar, jubilant, prompts, savage, equated, treachery, exultant, affronted, egomania, stun, mad, devotion, oly-2004-tennis, burst, fretful, shame, turn, stunning, oddness, sensitivity, politeness, status, dreadful, inquisitive, incensed, jittery, seeming, bring, bangkokians, visuals, precautions, concerns, enjoyment, vega@globe.com, why, anguished, applauding, backlash, deplore, animation, moreover, brooding, believers, speculation, appreciative, ballast, depressed, inconsolable, motivation, humiliation, worrying, torches, establishment, overdressed, whatever, moralize, came, indeed, loath, forcefulness, surprisingly, doldrums, 'll, tired, retaliation, emotion, foreboding, constricted, realization, obsessed, antipathy, depth-charged, dumbfounded, despairing, contingent, cackles, childlike, astounded, signified, fluctuate, take, dishonour, elicited, saves_mrivera, dubbed, catharsis, creativity, provide, letting, loved, decisiveness, impressed, miserable, munificence, obstinate, antigravity, wistful, invigorating, broken, inconsequential, elderly, bullet, noting, say, burn, frustrated, admit, innocence, diarrhoea, unfortunately, besides, sensational, calling, morose, embarrassing, beliefs, tears, mesmerized, nastiness, unleashes, sadness, booing, uplifting, protection, homeless, folks, emphasized, insistent, calamitous, self-identity, omnipresence, powerless, heft, rest, grateful, supportive, anticipation, demoralizing, hysterical, falseness, re-organise, doomed, propitiation, wake, piyanart, amidst, moment, awkward, pixar, helping, predictably, inconstancy, hesitancy, nevertheless, cynicism, stoicism, kd94, sprinkled, clearly, accomplished, fevers, reconstruction, timelessness, idiosyncrasy, scripts, sanctimony, welcome, conviction, tradition, amorality, merciless, reassure, fluidity, pensive, hatter, madness, excercise, wish, horrible, glory, storyboard, .000105, treating, regards, proclivity, humanitarian, remember, quite, bhagya, peacefulness, cumbersome, mistaken, response, throngs, evoked, nervous, though, arguing, flummoxed, disconcerted, indigestion, commitment, strongly, conscience, dehydration, debauchery, languor, appalled, nonplused, acquiescence, vatten, suggesting, abrade, continue, burners, eschew, villainy, rabbit, deployed, disenchantment, pretty, http://www.oklahomacitynationalmemorial.org, angering, horrifying, else, explosiveness, clueless, regrets, newfound, sitting, pathetic, alienation, js03, affection, realize, offended, regardless, knowing, str95bb, patriotism, defiance, freedom, believes, frustrating, rp-1, bemusement, predicting, voluptuousness, fun, revelatory, prompting, surliness, pleading, rashes, animations, unforgettable, respects, nervousness, animated, hostility, obviously, jaded, applaud, homoeroticism, ire, giddiness, ornaments, outsiders, smugness, mellow, shameful, frankincense, distinction, warning, howls, fxff, marveled, reticence, loathing, injustice, flame, bafflement, enraged, diffident, anticipating, chafe, counteract, immediately, tellingly, liberty, engendered, recognizes, whimsy, weird, surprise, familiarity, illness, inexplicable, farcical, enjoyed, nonchalance, dizziness, quick, .0170, look, displeasure, dreamworks, opposing, pathos, demented, peculiar, outspokenness, adulation, …, noticed, nor, way, crystallise, summons, tragic, befuddled, eventful, sentimental, tankage, heartlessness, hand-drawn, elated, despises, offering, instream, impotent, reaffirmation, acasta, ominous, mims, blandness, editing, uniqueness, passion, serenity, perfunctory, stupefaction, firebombs, wacky, dispiriting, ruminative, starving, devaluing, furious, pain, cacophony, tantrum, candles, unsatisfied, internalize, film, potpourri, timeless, opprobrium, pronouncement, antics, de-emphasize, comforted, enraging, montanans, caring, promise, expression, unfriendly, staggering, allied, menorah, abhorred, sympathy, patronize, intimacy, rudeness, catcalls, rise, publicized, marvelous, scary, haughtiness, equanimity, opposed, alabamians, squeamish, dreams, leery, anticipated, happy, detests, marvellous, certainly, fury, disdain, messiness, dispirited, exclamations, idiocy, sudden, vengeful, visibly, enjoy, dislike, intellectualism, amusing, benefit, .000088, wariness, reconsideration, rheology, scapegoating, denunciations, dishonesty, unseemly, buying, anxious, elaboration, liberality, unprompted, sake, arduous, blunt, cleverness, indifference, skepticism, determined, onslaught, preoccupied, pretentiousness, directness, irksome, electrifying, intemperate, washingtonians, irked, disastrous, fascinated, discouragement, unamused, compliments, fading, loving, simple-minded, incredibly, cautiousness, unhappy, uncertain, unfazed, reassuring, ripoffs, approbation, threatening, find, postmodernists, similarly, sent, embrace, benignly, apotheosis, wishing, stressed, interpenetration, unconvinced, thermoregulation, idolize, encumber, reverie, blamed, wicked, investor, perplexed, affected, self-monitoring, irritability, derision, predominance, virtue, jitters, unconcerned, tihg, skylarking, nuttiness, startling, hysteria, downsize, dread, fears, suffering, boring, inscrutably, curses, flatten, deepens, torturous, disliking, introspective, script, hostile, useless, informality, steadiness, hoping, migraines, seething, chagrined, celebrate, lethargy, claiming, denigration, immaturity, embarrassed, mocked, interest, true, disorienting, confident\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join(vocabulary_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algumas palavras podem não estar relacionadas à emoção, porém, o método de aprendizado de máquina ainda é capaz de considerar palavras mais relevantes para uma determinada instancia, ignorando algum ruído. Veja como ficou a representação: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>canister</th>\n",
       "      <th>disappointed</th>\n",
       "      <th>experience</th>\n",
       "      <th>fact</th>\n",
       "      <th>fascinating</th>\n",
       "      <th>find</th>\n",
       "      <th>give</th>\n",
       "      <th>god</th>\n",
       "      <th>good</th>\n",
       "      <th>...</th>\n",
       "      <th>put</th>\n",
       "      <th>putting</th>\n",
       "      <th>sense</th>\n",
       "      <th>sick</th>\n",
       "      <th>sympathetic</th>\n",
       "      <th>true</th>\n",
       "      <th>video</th>\n",
       "      <th>wanting</th>\n",
       "      <th>worse</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.606043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.606043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.490297</td>\n",
       "      <td>0.490297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.324199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49225</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.551556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158265</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.606043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>0.606043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.606043</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200048</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60933</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.313903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.474727</td>\n",
       "      <td>0.474727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             bad  canister  disappointed  experience      fact  fascinating  \\\n",
       "id                                                                            \n",
       "208138  0.000000  0.000000      0.000000    0.606043  0.000000     0.000000   \n",
       "157010  0.000000  0.000000      0.000000    0.000000  0.490297     0.490297   \n",
       "101657  0.000000  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "49225   0.000000  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "158265  0.000000  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "204215  0.000000  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "274316  0.606043  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "57708   0.000000  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "200048  0.000000  0.707107      0.707107    0.000000  0.000000     0.000000   \n",
       "60933   0.000000  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "\n",
       "        find      give  god      good  ...       put   putting     sense  \\\n",
       "id                                     ...                                 \n",
       "208138   0.0  0.606043  0.0  0.000000  ...  0.000000  0.515192  0.000000   \n",
       "157010   0.0  0.000000  0.0  0.324199  ...  0.416798  0.000000  0.000000   \n",
       "101657   0.0  0.000000  0.0  1.000000  ...  0.000000  0.000000  0.000000   \n",
       "49225    0.0  0.000000  0.0  0.551556  ...  0.000000  0.000000  0.000000   \n",
       "158265   0.0  0.000000  0.5  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "204215   0.0  0.000000  0.0  0.000000  ...  0.515192  0.000000  0.606043   \n",
       "274316   0.0  0.000000  0.0  0.000000  ...  0.000000  0.515192  0.000000   \n",
       "57708    0.5  0.000000  0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "200048   0.0  0.000000  0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "60933    0.0  0.000000  0.0  0.313903  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "        sick  sympathetic      true     video  wanting     worse     class  \n",
       "id                                                                          \n",
       "208138   0.0          0.0  0.000000  0.000000      0.0  0.000000  positive  \n",
       "157010   0.0          0.0  0.000000  0.000000      0.0  0.000000  positive  \n",
       "101657   0.0          0.0  0.000000  0.000000      0.0  0.000000  positive  \n",
       "49225    0.0          0.0  0.000000  0.000000      0.0  0.000000  positive  \n",
       "158265   0.0          0.0  0.000000  0.000000      0.5  0.000000  positive  \n",
       "204215   0.0          0.0  0.000000  0.000000      0.0  0.000000  negative  \n",
       "274316   0.0          0.0  0.000000  0.000000      0.0  0.606043  negative  \n",
       "57708    0.5          0.5  0.000000  0.000000      0.0  0.000000  negative  \n",
       "200048   0.0          0.0  0.000000  0.000000      0.0  0.000000  negative  \n",
       "60933    0.0          0.0  0.474727  0.474727      0.0  0.000000  negative  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words(df_amazon_mini,vocabulary_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poderiamos agrupar as palavras chaves em conceitos, por exemplo, \"happiness\" ser sempre contabilizado quando houver um conjunto de palavras, por exemplo, '\"joy\",\"cheer\", \"bliss\", \"delight\", \"enjoy\", \"happy\"'. Porém, isso pode restringir muito o número de palavras e expandir com palavras usando embeddings, pode extrair palavras relacionadas com a emoção oposta (veja exemplo abaixo). Por isso, optamos por apresentar a representação usando bag of words. Mesmo assim, caso queira ver algum resultado dessa forma, a classe CountWords implementa expansão por grupos de palavras chaves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"happy, feel, glad, sure, everyone, 'm, definitely, 'd, 'll, remember, everybody, wish, proud, 're, really, always, maybe, excited, good, lucky, obviously, thrilled, pleased, pretty, wonderful, know, afraid, delighted, looking, want, thing, imagine, think, unhappy, satisfied, realize, knowing, going, tired, crazy\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "distance, words = kdtree_embedding.get_most_similar_embedding(\"happy\",40)\n",
    "#veja que unhappy é relacionado com happy - além de outras palavras negativas e ruido\n",
    "\", \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pride</th>\n",
       "      <th>elation</th>\n",
       "      <th>happiness</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>relief</th>\n",
       "      <th>hope</th>\n",
       "      <th>interest</th>\n",
       "      <th>surprise</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>sadness</th>\n",
       "      <th>boredom</th>\n",
       "      <th>shame</th>\n",
       "      <th>guilt</th>\n",
       "      <th>disgust</th>\n",
       "      <th>contempt</th>\n",
       "      <th>hostile</th>\n",
       "      <th>anger</th>\n",
       "      <th>recognition</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49225</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158265</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200048</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60933</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pride  elation  happiness  satisfaction  relief  hope  interest  \\\n",
       "208138      0        0         14             0       0     7         0   \n",
       "157010      0        0          5             0       0     5         2   \n",
       "101657      0        0          7             0       0     5         0   \n",
       "49225       0        0          4             0       0     3         0   \n",
       "158265      0        0          3             0       0     1         0   \n",
       "204215      0        0          0             0       0     1         0   \n",
       "274316      0        0          2             0       0     0         0   \n",
       "57708       0        0         10             0       0     7         0   \n",
       "200048      0        0          4             0       0     2         0   \n",
       "60933       0        0         12             0       0     2         0   \n",
       "\n",
       "        surprise  anxiety  sadness  boredom  shame  guilt  disgust  contempt  \\\n",
       "208138         0        4        0        0      0      0        0         0   \n",
       "157010         0        3        0        0      0      0        0         0   \n",
       "101657         0        0        0        0      0      0        0         0   \n",
       "49225          0        1        0        0      0      0        0         0   \n",
       "158265         0        1        0        0      0      0        0         0   \n",
       "204215         0        1        0        0      0      0        0         0   \n",
       "274316         0        0        0        0      0      0        0         0   \n",
       "57708          0        5        0        0      0      0        1         0   \n",
       "200048         0        0        0        0      0      0        0         0   \n",
       "60933          0       12        1        0      0      0        0         0   \n",
       "\n",
       "        hostile  anger  recognition     class  \n",
       "208138        0      0            0  positive  \n",
       "157010        0      0            0  positive  \n",
       "101657        0      0            1  positive  \n",
       "49225         0      0            0  positive  \n",
       "158265        0      0            0  positive  \n",
       "204215        0      0            0  negative  \n",
       "274316        0      0            0  negative  \n",
       "57708         0      0            0  negative  \n",
       "200048        0      0            0  negative  \n",
       "60933         0      0            0  negative  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embeddings.textual_representation import CountWords,InstanceWisePreprocess\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "aggregate = CountWords(dict_embedding, emotion_words,max_distance=0.3)\n",
    "\n",
    "word_counter = InstanceWisePreprocess(\"word-counter\",aggregate)\n",
    "word_counter.preprocess_train_dataset(df_amazon_mini, \"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O max_distance é responsável por obter as palavras similares. Veja que diversos documentos negativos foram classificados com o grupo \"happiness\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Representação agregando embeddings das palavras:** Conforme proposto por [Shen et al.](https://arxiv.org/pdf/1805.09843.pdf), dado que uma frase é representado por um conjunto de embeddings $\\{e_1, e_2, ..., e_n\\}$  uma forma simples e que geralmente obtém resultados **comparáveis a métodos mais complexos** é fazer operações em cada dimensão do embedding, tais como: média e máximo por dimensão do embedding. Por exemplo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'house', 'is', 'green']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embeddings de alguams palavras: \n",
    "dict_embedding = {'my':      [10, 11,14, 20, 15, 80],\n",
    "                  'house':   [11, 12,10, 24, 11, 30],\n",
    "                  'is':      [1,  3,  5, -1, 10, 20],\n",
    "                  'green':   [12,10, 20, 12, 10, 20]\n",
    "                   }\n",
    "#representação do texto \"my house is green\"\n",
    "arr_texto = \"my house is green\".split()\n",
    "arr_texto      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usando a média de cada dimensão dos embeddings:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representação: [8.5, 9.0, 12.25, 13.75, 11.5, 37.5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def average_pooling(arr_texto, dim_embedding):\n",
    "    representacao = []\n",
    "    for i in range(dim_embedding):\n",
    "        #calcula a média da iésima posição do embedding\n",
    "        sum_pos = 0\n",
    "        for word in arr_texto:\n",
    "            sum_pos += dict_embedding[word][i]\n",
    "\n",
    "        representacao.append(sum_pos/len(arr_texto))\n",
    "    return representacao\n",
    "dim_embedding = 6\n",
    "representacao = average_pooling(arr_texto, dim_embedding)\n",
    "print(f\"Representação: {representacao}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usando o máximo de cada dimensão dos embeddings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representação: [12, 12, 20, 24, 15, 80]\n"
     ]
    }
   ],
   "source": [
    "dim_embedding = 6\n",
    "def max_pooling(arr_texto, dim_embedding):\n",
    "    representacao = []\n",
    "    for i in range(dim_embedding):\n",
    "        #calcula o valor máximo de cada iésima posição do embedding\n",
    "        first_word = arr_texto[0]\n",
    "        max_pos = dict_embedding[first_word][i]\n",
    "        for word in arr_texto[1:]:\n",
    "            if max_pos < dict_embedding[word][i]:\n",
    "                max_pos = dict_embedding[word][i]\n",
    "\n",
    "        representacao.append(max_pos)\n",
    "    return representacao\n",
    "representacao = max_pooling(arr_texto, dim_embedding)\n",
    "print(f\"Representação: {representacao}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como há palavras pouco relevantes (como stopwords) podemos remove-las e, também podemos utilizar apenas as palavras de um vocabulario controlado. Abaixo veja a representação. Como esta representação é vetorial, a mesma não é uma representação simples de ser entendida por humanos, porém, pode-se obter bons resultados. Você pode adicionar o vocabulario controlado ou as stopwords por meio dos parametros correpondentes. O parâmetro `aggregate_method` define se será feito um maximo ou média entre os embeddings colocando os valores `max` ou `avg`, respectivamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: the\n",
      "10000: persecution\n",
      "20000: baths\n",
      "30000: mortally\n",
      "40000: 1667\n",
      "50000: bec\n",
      "60000: baek\n",
      "70000: b/w\n",
      "80000: klinghoffer\n",
      "90000: azarov\n",
      "100000: capron\n",
      "110000: perpetua\n",
      "120000: biratnagar\n",
      "130000: 12.74\n",
      "140000: yaffa\n",
      "150000: cryogenics\n",
      "160000: ef1\n",
      "170000: franchetti\n",
      "180000: blintzes\n",
      "190000: birthstones\n",
      "200000: naadam\n",
      "210000: concertation\n",
      "220000: lesticus\n",
      "230000: containerboard\n",
      "240000: boydston\n",
      "250000: afterellen.com\n",
      "260000: acuff-rose\n",
      "270000: close-fitting\n",
      "280000: packbot\n",
      "290000: comptel\n",
      "300000: tanke\n",
      "310000: saraju\n",
      "320000: rouiba\n",
      "330000: discomfit\n",
      "340000: numurkah\n",
      "350000: hla-a\n",
      "360000: 90125\n",
      "370000: zipkin\n",
      "380000: lombarde\n",
      "390000: 1.137\n",
      "Palavras ignoradas: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>-0.059204</td>\n",
       "      <td>0.191895</td>\n",
       "      <td>0.372070</td>\n",
       "      <td>0.126587</td>\n",
       "      <td>-0.102600</td>\n",
       "      <td>-0.024734</td>\n",
       "      <td>-0.340088</td>\n",
       "      <td>0.083252</td>\n",
       "      <td>-0.264160</td>\n",
       "      <td>-0.224731</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258545</td>\n",
       "      <td>0.219971</td>\n",
       "      <td>-0.457031</td>\n",
       "      <td>-0.332764</td>\n",
       "      <td>-0.326172</td>\n",
       "      <td>-0.164795</td>\n",
       "      <td>-0.160400</td>\n",
       "      <td>0.378906</td>\n",
       "      <td>0.456543</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>-0.064270</td>\n",
       "      <td>0.268555</td>\n",
       "      <td>0.395020</td>\n",
       "      <td>-0.094360</td>\n",
       "      <td>-0.176514</td>\n",
       "      <td>0.011276</td>\n",
       "      <td>-0.220093</td>\n",
       "      <td>-0.191528</td>\n",
       "      <td>-0.177979</td>\n",
       "      <td>-0.433350</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.135132</td>\n",
       "      <td>0.084656</td>\n",
       "      <td>-0.274170</td>\n",
       "      <td>-0.524414</td>\n",
       "      <td>-0.061218</td>\n",
       "      <td>-0.264160</td>\n",
       "      <td>-0.521973</td>\n",
       "      <td>0.185547</td>\n",
       "      <td>0.541992</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>-0.030762</td>\n",
       "      <td>0.119934</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>-0.437012</td>\n",
       "      <td>-0.739258</td>\n",
       "      <td>-0.153442</td>\n",
       "      <td>0.081116</td>\n",
       "      <td>-0.385498</td>\n",
       "      <td>-0.687988</td>\n",
       "      <td>-0.416260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.440430</td>\n",
       "      <td>0.083313</td>\n",
       "      <td>0.200317</td>\n",
       "      <td>-0.754883</td>\n",
       "      <td>0.169189</td>\n",
       "      <td>-0.265625</td>\n",
       "      <td>-0.528809</td>\n",
       "      <td>0.175781</td>\n",
       "      <td>1.065430</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49225</th>\n",
       "      <td>-0.127930</td>\n",
       "      <td>0.394287</td>\n",
       "      <td>0.441895</td>\n",
       "      <td>-0.243408</td>\n",
       "      <td>-0.411377</td>\n",
       "      <td>-0.300293</td>\n",
       "      <td>0.028076</td>\n",
       "      <td>-0.054688</td>\n",
       "      <td>-0.478516</td>\n",
       "      <td>-0.224731</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.283691</td>\n",
       "      <td>0.092102</td>\n",
       "      <td>-0.050598</td>\n",
       "      <td>-0.695801</td>\n",
       "      <td>0.098633</td>\n",
       "      <td>0.142578</td>\n",
       "      <td>-0.408447</td>\n",
       "      <td>0.257324</td>\n",
       "      <td>0.741699</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158265</th>\n",
       "      <td>0.449951</td>\n",
       "      <td>0.435547</td>\n",
       "      <td>0.288086</td>\n",
       "      <td>-0.087097</td>\n",
       "      <td>-0.048950</td>\n",
       "      <td>0.714844</td>\n",
       "      <td>-0.627441</td>\n",
       "      <td>-0.139526</td>\n",
       "      <td>0.037781</td>\n",
       "      <td>-0.392822</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.330322</td>\n",
       "      <td>0.201050</td>\n",
       "      <td>0.036438</td>\n",
       "      <td>-0.448730</td>\n",
       "      <td>-0.454346</td>\n",
       "      <td>-0.360840</td>\n",
       "      <td>-0.517578</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.399902</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>0.040314</td>\n",
       "      <td>0.040253</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.346191</td>\n",
       "      <td>-0.253418</td>\n",
       "      <td>0.219482</td>\n",
       "      <td>-0.346924</td>\n",
       "      <td>-0.459229</td>\n",
       "      <td>-0.234009</td>\n",
       "      <td>-0.016251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179688</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>-0.037048</td>\n",
       "      <td>-0.417480</td>\n",
       "      <td>-0.152466</td>\n",
       "      <td>0.052826</td>\n",
       "      <td>-0.471924</td>\n",
       "      <td>0.008827</td>\n",
       "      <td>0.480225</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>0.158936</td>\n",
       "      <td>-0.016418</td>\n",
       "      <td>0.793945</td>\n",
       "      <td>-0.266113</td>\n",
       "      <td>-0.669434</td>\n",
       "      <td>-0.004871</td>\n",
       "      <td>-0.398193</td>\n",
       "      <td>-0.331055</td>\n",
       "      <td>0.040588</td>\n",
       "      <td>-0.129272</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127441</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>-0.206665</td>\n",
       "      <td>-0.333496</td>\n",
       "      <td>0.056488</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.239014</td>\n",
       "      <td>0.317383</td>\n",
       "      <td>-0.004375</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>0.162231</td>\n",
       "      <td>0.439941</td>\n",
       "      <td>0.299561</td>\n",
       "      <td>-0.111328</td>\n",
       "      <td>-0.139404</td>\n",
       "      <td>0.153687</td>\n",
       "      <td>-0.460449</td>\n",
       "      <td>-0.149048</td>\n",
       "      <td>0.320801</td>\n",
       "      <td>-0.473389</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045044</td>\n",
       "      <td>0.411377</td>\n",
       "      <td>-0.177979</td>\n",
       "      <td>-0.549805</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>-0.155273</td>\n",
       "      <td>-0.111450</td>\n",
       "      <td>0.056763</td>\n",
       "      <td>0.045990</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200048</th>\n",
       "      <td>-0.154175</td>\n",
       "      <td>0.127686</td>\n",
       "      <td>0.332520</td>\n",
       "      <td>-0.328125</td>\n",
       "      <td>-0.582031</td>\n",
       "      <td>0.210815</td>\n",
       "      <td>0.188110</td>\n",
       "      <td>0.594238</td>\n",
       "      <td>0.397949</td>\n",
       "      <td>0.190674</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.284912</td>\n",
       "      <td>-0.298096</td>\n",
       "      <td>-0.150879</td>\n",
       "      <td>0.051758</td>\n",
       "      <td>0.336914</td>\n",
       "      <td>0.061096</td>\n",
       "      <td>0.478516</td>\n",
       "      <td>-0.390137</td>\n",
       "      <td>0.254883</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60933</th>\n",
       "      <td>-0.006786</td>\n",
       "      <td>0.071411</td>\n",
       "      <td>0.483154</td>\n",
       "      <td>-0.466309</td>\n",
       "      <td>-0.287598</td>\n",
       "      <td>0.137451</td>\n",
       "      <td>-0.036652</td>\n",
       "      <td>0.012230</td>\n",
       "      <td>-0.068115</td>\n",
       "      <td>-0.241211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056396</td>\n",
       "      <td>0.058685</td>\n",
       "      <td>0.099060</td>\n",
       "      <td>-0.135742</td>\n",
       "      <td>-0.311035</td>\n",
       "      <td>-0.453125</td>\n",
       "      <td>-0.349121</td>\n",
       "      <td>0.174072</td>\n",
       "      <td>0.460205</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "208138 -0.059204  0.191895  0.372070  0.126587 -0.102600 -0.024734 -0.340088   \n",
       "157010 -0.064270  0.268555  0.395020 -0.094360 -0.176514  0.011276 -0.220093   \n",
       "101657 -0.030762  0.119934  0.539062 -0.437012 -0.739258 -0.153442  0.081116   \n",
       "49225  -0.127930  0.394287  0.441895 -0.243408 -0.411377 -0.300293  0.028076   \n",
       "158265  0.449951  0.435547  0.288086 -0.087097 -0.048950  0.714844 -0.627441   \n",
       "204215  0.040314  0.040253  0.532227 -0.346191 -0.253418  0.219482 -0.346924   \n",
       "274316  0.158936 -0.016418  0.793945 -0.266113 -0.669434 -0.004871 -0.398193   \n",
       "57708   0.162231  0.439941  0.299561 -0.111328 -0.139404  0.153687 -0.460449   \n",
       "200048 -0.154175  0.127686  0.332520 -0.328125 -0.582031  0.210815  0.188110   \n",
       "60933  -0.006786  0.071411  0.483154 -0.466309 -0.287598  0.137451 -0.036652   \n",
       "\n",
       "               7         8         9  ...        91        92        93  \\\n",
       "208138  0.083252 -0.264160 -0.224731  ... -0.258545  0.219971 -0.457031   \n",
       "157010 -0.191528 -0.177979 -0.433350  ... -0.135132  0.084656 -0.274170   \n",
       "101657 -0.385498 -0.687988 -0.416260  ... -0.440430  0.083313  0.200317   \n",
       "49225  -0.054688 -0.478516 -0.224731  ... -0.283691  0.092102 -0.050598   \n",
       "158265 -0.139526  0.037781 -0.392822  ... -0.330322  0.201050  0.036438   \n",
       "204215 -0.459229 -0.234009 -0.016251  ... -0.179688  0.093750 -0.037048   \n",
       "274316 -0.331055  0.040588 -0.129272  ... -0.127441 -0.073242 -0.206665   \n",
       "57708  -0.149048  0.320801 -0.473389  ... -0.045044  0.411377 -0.177979   \n",
       "200048  0.594238  0.397949  0.190674  ... -0.284912 -0.298096 -0.150879   \n",
       "60933   0.012230 -0.068115 -0.241211  ...  0.056396  0.058685  0.099060   \n",
       "\n",
       "              94        95        96        97        98        99     class  \n",
       "208138 -0.332764 -0.326172 -0.164795 -0.160400  0.378906  0.456543  positive  \n",
       "157010 -0.524414 -0.061218 -0.264160 -0.521973  0.185547  0.541992  positive  \n",
       "101657 -0.754883  0.169189 -0.265625 -0.528809  0.175781  1.065430  positive  \n",
       "49225  -0.695801  0.098633  0.142578 -0.408447  0.257324  0.741699  positive  \n",
       "158265 -0.448730 -0.454346 -0.360840 -0.517578  0.019989  0.399902  positive  \n",
       "204215 -0.417480 -0.152466  0.052826 -0.471924  0.008827  0.480225  negative  \n",
       "274316 -0.333496  0.056488  0.071777  0.239014  0.317383 -0.004375  negative  \n",
       "57708  -0.549805 -0.000109 -0.155273 -0.111450  0.056763  0.045990  negative  \n",
       "200048  0.051758  0.336914  0.061096  0.478516 -0.390137  0.254883  negative  \n",
       "60933  -0.135742 -0.311035 -0.453125 -0.349121  0.174072  0.460205  negative  \n",
       "\n",
       "[10 rows x 101 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#obtem as stopwords\n",
    "stop_words = set()\n",
    "with open(\"datasets/stopwords.txt\") as stop_file:\n",
    "    stop_words = set(stop_word[:-1] for stop_word in stop_file)\n",
    "\n",
    "from embeddings.textual_representation import AggregateEmbeddings,InstanceWisePreprocess\n",
    "dict_embedding = get_embedding(\"glove.en.100.txt\")\n",
    "aggregate_keywords_exp = AggregateEmbeddings(dict_embedding, aggregate_method=\"avg\", \n",
    "                                            words_to_filter=stop_words, words_to_consider=vocabulary_expanded)\n",
    "emb_keywords_exp = InstanceWisePreprocess(\"emb_keywords_exp\",aggregate_keywords_exp)\n",
    "emb_keywords_exp.preprocess_train_dataset(df_amazon_mini, \"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação por meio de um método de aprendizado de máquina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os embeddings podem oferecer uma informação de proximidade de conceitos que o uso de Bag of Words não seria capaz. Mesmo assim, cada representação e preprocessamento tem sua vantagem e desvantagem e não existe um método que será sempre o melhor. Assim, para sabermos qual representação é melhor para uma tarefa, é importante avaliarmos em quais delas são maiores para a tarefa em questão. Como o foco desta prática não é a avaliação, iremos apenas apresentar o resultado, caso queira, você pode [assistir a video aula](https://www.youtube.com/watch?v=Ag06UuWTsr4&list=PLwIaU1DGYV6tUx10fCTw5aPnqypbbK_GJ&index=12) e [fazer a prática sobre avaliação](https://github.com/daniel-hasan/ap-de-maquina-cefetmg-avaliacao/archive/master.zip). Nesta parte, iremos apenas usar a avaliação para verificar qual método é melhor.  \n",
    "\n",
    "Para que esta seção seja auto contida, iremos fazer toda a preparação que fizemos nas seções anteriores\n",
    "\n",
    "**Criação da lista de stopwords e de vocabulário:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: the\n",
      "10000: persecution\n",
      "20000: baths\n",
      "30000: mortally\n",
      "40000: 1667\n",
      "50000: bec\n",
      "60000: baek\n",
      "70000: b/w\n",
      "80000: klinghoffer\n",
      "90000: azarov\n",
      "100000: capron\n",
      "110000: perpetua\n",
      "120000: biratnagar\n",
      "130000: 12.74\n",
      "140000: yaffa\n",
      "150000: cryogenics\n",
      "160000: ef1\n",
      "170000: franchetti\n",
      "180000: blintzes\n",
      "190000: birthstones\n",
      "200000: naadam\n",
      "210000: concertation\n",
      "220000: lesticus\n",
      "230000: containerboard\n",
      "240000: boydston\n",
      "250000: afterellen.com\n",
      "260000: acuff-rose\n",
      "270000: close-fitting\n",
      "280000: packbot\n",
      "290000: comptel\n",
      "300000: tanke\n",
      "310000: saraju\n",
      "320000: rouiba\n",
      "330000: discomfit\n",
      "340000: numurkah\n",
      "350000: hla-a\n",
      "360000: 90125\n",
      "370000: zipkin\n",
      "380000: lombarde\n",
      "390000: 1.137\n",
      "Palavras ignoradas: 0\n"
     ]
    }
   ],
   "source": [
    "from embeddings.utils import get_embedding, KDTreeEmbedding\n",
    "\n",
    "emotion_words = {\n",
    "                    \"pride\":{\"proud\"},\n",
    "                    \"elation\":{\"ecstatic\", \"euphoria\", \"exaltation\", \"exhilarating\"},#vs boredom\n",
    "                    \"happiness\":{\"joy\",\"cheer\", \"bliss\", \"delight\", \"enjoy\", \"happy\"},#vs sad\n",
    "                    \"satisfaction\":{\"comfortable\",\"contentment\"},#\n",
    "                    \"relief\":{},\n",
    "                    \"hope\":{\"buoyancy\", \"confident\", \"faith\", \"optimistic\"},\n",
    "                    \"interest\":{\"alert\", \"animation\", \"ardor\", \"curious\",\"enthusiasm\"},\n",
    "                    \"surprise\":{\"amazed\", \"astonishing\", \"dumbfounded\",\"thunderstruck\"},\n",
    "                    \"anxiety\":{\"anguish\",\"anxiety\",\"apprehensive\",\"jittery\",\"nervous\",\"worry\"},\n",
    "                    \"sadness\":{\"chagrin\", \"dejected\", \"gloom\", \"hopeless\", \"melancholy\", \"sad\", \"tear\"},\n",
    "                    \"boredom\":{\"ennui\",\"indifference\",\"tedious\"},\n",
    "                    \"shame\":{\"abashed\", \"ashamed\", \"embarrassing\", \"humiliating\"},\n",
    "                    \"guilt\":{\"blame\", \"contrition\", \"remorse\"},\n",
    "                    \"disgust\":{\"abhor\", \"aversion\", \"dislike\", \"disrelish\", \"nausea\",\"sick\"},\n",
    "                    \"contempt\":{\"denigration\",\"depreciate\",\"derision\",\"disdain\",\"scorn\"},\n",
    "                    \"hostile\":{},\n",
    "                    \"anger\":{\"anger\",\"angry\",\"furious\",\"fury\",\"incense\",\"infuriating\",\n",
    "                                \"mad\",\"rage\",\"resent\",\"temper\",\"wrath\"},\n",
    "                    \"recognition\":{\"respect\",\"acknowledgement\"}\n",
    "            }\n",
    "dict_embedding = get_embedding(\"glove.en.100.txt\") \n",
    "kdtree_embedding = KDTreeEmbedding(dict_embedding, \"kdt_en.p\")\n",
    "\n",
    "#obtem as stopwords\n",
    "stop_words = set()\n",
    "with open(\"datasets/stopwords.txt\") as stop_file:\n",
    "    stop_words = set(stop_word[:-1] for stop_word in stop_file)\n",
    "\n",
    "\n",
    "#palavras chaves a serem consideradas\n",
    "set_vocabulary = set()\n",
    "for key_word, arr_related_words in emotion_words.items():\n",
    "    set_vocabulary.add(key_word)\n",
    "    set_vocabulary = set_vocabulary | set(arr_related_words)\n",
    "\n",
    "#kdtree - para gerar o conjunto com palavras chaves e suas similares\n",
    "vocabulary_expanded = []\n",
    "for word in set_vocabulary:\n",
    "    _, words = kdtree_embedding.get_most_similar_embedding(word,60)\n",
    "    vocabulary_expanded.extend(words)\n",
    "vocabulary_expanded = set(vocabulary_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Representações usadas**: Iremos avaliar a filtragem de stopwords e usando um vocabulário restrito da representação bag of words e também da representação usando a média de embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.textual_representation import BagOfWords, AggregateEmbeddings,InstanceWisePreprocess\n",
    "\n",
    "#gera as representações\n",
    "aggregate = AggregateEmbeddings(dict_embedding, \"avg\")\n",
    "embedding = InstanceWisePreprocess(\"embbeding\",aggregate)\n",
    "\n",
    "aggregate_stop = AggregateEmbeddings(dict_embedding, \"avg\",words_to_filter=stop_words)\n",
    "emb_nostop = InstanceWisePreprocess(\"emb_nostop\",aggregate_stop)\n",
    "\n",
    "\n",
    "aggregate_keywords_exp = AggregateEmbeddings(dict_embedding, \"avg\",words_to_consider=vocabulary_expanded)\n",
    "emb_keywords_exp = InstanceWisePreprocess(\"emb_keywords_exp\",aggregate_keywords_exp)\n",
    "\n",
    "bow_keywords = BagOfWords(\"bow_keywords_exp\", words_to_consider=vocabulary_expanded)\n",
    "bow = BagOfWords(\"bow\", stop_words=stop_words)\n",
    "\n",
    "arr_representations = [embedding,emb_nostop, emb_keywords_exp, bow,bow_keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>Do NOT WASTE Your Time: This book, to put it n...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>Peels the paint off the walls: I first heard t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>History With Modern Appeal: This is a must rea...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>Worse Music cd ever: I tried putting this in a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>Deliberately Obtuse Nonsense: I don't know wha...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29215</th>\n",
       "      <td>Better than the movie?: YES! This book gets be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256457</th>\n",
       "      <td>The Best RE yet: This is the best in the RE se...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210215</th>\n",
       "      <td>What are they waiting for?: This has got to be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200693</th>\n",
       "      <td>Hollywood - promoting the Antichrist again?: C...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169551</th>\n",
       "      <td>Best American TV Series Ever: With its combina...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     class\n",
       "id                                                                 \n",
       "204215  Do NOT WASTE Your Time: This book, to put it n...  negative\n",
       "208138  Peels the paint off the walls: I first heard t...  positive\n",
       "157010  History With Modern Appeal: This is a must rea...  positive\n",
       "274316  Worse Music cd ever: I tried putting this in a...  negative\n",
       "57708   Deliberately Obtuse Nonsense: I don't know wha...  negative\n",
       "...                                                   ...       ...\n",
       "29215   Better than the movie?: YES! This book gets be...  positive\n",
       "256457  The Best RE yet: This is the best in the RE se...  positive\n",
       "210215  What are they waiting for?: This has got to be...  positive\n",
       "200693  Hollywood - promoting the Antichrist again?: C...  negative\n",
       "169551  Best American TV Series Ever: With its combina...  positive\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"datasets/amazon_reviews_mini.txt\",index_col=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, é executado um método de aprendizado  para cada representação. Esse processo pode demorar um pouco pois é feito a procura do melhor parametro do algoritmo. Algumas otimizações que talvez, você precise fazer é no arquivo `embedding/avaliacao_embedding.py` alterar o parametro `n_jobs` no método `obtem_metodo` da classe `OtimizacaoObjetivoRandomForest`. Esse parametro é responsável por utiizar mais threads ao executar o Random Forests.  O valor pode ser levemente inferior a quantidades de núcleos que seu computador tem, caso ele tenha mais de 2, caso contrário, o ideal é colocarmos `n_jobs=1`. Caso queira visualizar resultados mais rapidamente, diminua o valor da variável `num_trials` e `num_folds` abaixo. Atenção que `num_folds` deve ser um valor maior que um."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Representação: embbeding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-08 00:10:26,814] Using an existing study with name 'random_forest_embbeding_fold_0' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:26,872] Using an existing study with name 'random_forest_embbeding_fold_1' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:26,916] Using an existing study with name 'random_forest_embbeding_fold_2' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:26,961] Using an existing study with name 'random_forest_embbeding_fold_3' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:27,005] Using an existing study with name 'random_forest_embbeding_fold_4' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:27,099] Using an existing study with name 'random_forest_emb_nostop_fold_0' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:27,156] Using an existing study with name 'random_forest_emb_nostop_fold_1' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representação: embbeding concluida\n",
      "===== Representação: emb_nostop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-08 00:10:27,206] Using an existing study with name 'random_forest_emb_nostop_fold_2' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:27,266] Using an existing study with name 'random_forest_emb_nostop_fold_3' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:27,320] Using an existing study with name 'random_forest_emb_nostop_fold_4' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:27,428] Using an existing study with name 'random_forest_emb_keywords_exp_fold_0' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:27,477] Using an existing study with name 'random_forest_emb_keywords_exp_fold_1' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representação: emb_nostop concluida\n",
      "===== Representação: emb_keywords_exp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-08 00:10:27,526] Using an existing study with name 'random_forest_emb_keywords_exp_fold_2' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:27,585] Using an existing study with name 'random_forest_emb_keywords_exp_fold_3' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:27,640] Using an existing study with name 'random_forest_emb_keywords_exp_fold_4' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representação: emb_keywords_exp concluida\n",
      "===== Representação: bow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-08 00:10:28,197] A new study created in RDB with name: random_forest_bow_fold_0\n",
      "[W 2023-12-08 00:10:29,050] Trial 0 failed with parameters: {'min_samples_split': 9, 'max_features': 95, 'num_arvores': 30} because of the following error: InvalidParameterError('The \\'stop_words\\' parameter of TfidfVectorizer must be a str among {\\'english\\'}, an instance of \\'list\\' or None. Got {\\'took\\', \\'are\\', \\'un\\', \\'down\\', \\'gets\\', \\'wasn\\', \\'yet\\', \\'hence\\', \\'according\\', \\'corresponding\\', \\'zero\\', \\'others\\', \\'yourselves\\', \\'was\\', \\'three\\', \\'whose\\', \\'lest\\', \\'know\\', \\'one\\', \"there\\'s\", \\'plus\\', \\'seeming\\', \\'like\\', \\'whereby\\', \\'possible\\', \\'why\\', \\'went\\', \\'whence\\', \\'appreciate\\', \\'hereby\\', \\'using\\', \\'but\\', \\'moreover\\', \\'of\\', \\'keeps\\', \\'at\\', \\'getting\\', \\'forth\\', \\'such\\', \\'said\\', \\'oh\\', \\'sometime\\', \\'ok\\', \"we\\'ve\", \\'often\\', \\'indicated\\', \\'hi\\', \\'thorough\\', \\'whatever\\', \\'among\\', \\'up\\', \\'came\\', \\'indeed\\', \\'et\\', \\'under\\', \\'those\\', \\'don\\', \\'neither\\', \"\\'ll\", \\'edu\\', \\'afterwards\\', \\'as\\', \\'causes\\', \\'whither\\', \\'seen\\', \\'downwards\\', \\'even\\', \\'normally\\', \\'anything\\', \\'hither\\', \\'would\\', \"here\\'s\", \\'que\\', \\'some\\', \\'anyone\\', \\'none\\', \\'taken\\', \\'everybody\\', \\'fifth\\', \\'take\\', \\'presumably\\', \\'sure\\', \\'with\\', \\'anyway\\', \\'somewhere\\', \\'let\\', \\'seriously\\', \\'him\\', \\'indicate\\', \\'merely\\', \\'accordingly\\', \\'looks\\', \\'gone\\', \\'you\\', \"who\\'s\", \\'therefore\\', \\'through\\', \\'goes\\', \\'say\\', \\'apart\\', \\'within\\', \\'nine\\', \\'until\\', \\'besides\\', \\'unfortunately\\', \\'awfully\\', \\'sub\\', \\'ie\\', \\'co\\', \\'overall\\', \\'whenever\\', \\'most\\', \\'into\\', \\'onto\\', \\'thus\\', \\'likely\\', \\'looking\\', \\'them\\', \\'an\\', \\'herein\\', \\'mean\\', \\'uses\\', \\'sup\\', \\'old\\', \\'after\\', \\'gives\\', \\'serious\\', \\'wouldn\\', \\'have\\', \\'both\\', \\'therein\\', \\'somebody\\', \\'hereupon\\', \\'in\\', \\'just\\', \\'going\\', \\'several\\', \\'other\\', \\'thereupon\\', \\'okay\\', \\'whereupon\\', \\'anybody\\', \\'kept\\', \\'shouldn\\', \\'may\\', \\'again\\', \\'want\\', \\'always\\', \\'clearly\\', \\'also\\', \\'nevertheless\\', \\'following\\', \\'inward\\', \\'she\\', \\'weren\\', \\'latterly\\', \\'might\\', \\'truly\\', \\'yourself\\', \"they\\'d\", \\'nd\\', \\'welcome\\', \\'does\\', \\'across\\', \\'necessary\\', \\'cannot\\', \\'definitely\\', \\'seeing\\', \\'what\\', \\'ever\\', \\'were\\', \\'all\\', \\'thereafter\\', \"it\\'ll\", \\'then\\', \"\\'t\", \\'becomes\\', \\'six\\', \\'its\\', \\'regarding\\', \\'and\\', \\'however\\', \\'unless\\', \\'wish\\', \\'ve\\', \\'second\\', \\'everywhere\\', \\'four\\', \\'whole\\', \"you\\'d\", \\'regards\\', \\'more\\', \\'quite\\', \\'next\\', \\'please\\', \\'between\\', \\'got\\', \\'though\\', \\'consider\\', \\'since\\', \\'wherever\\', \\'alone\\', \\'saw\\', \\'available\\', \\'viz\\', \\'can\\', \\'allow\\', \\'concerning\\', \\'eight\\', \\'nowhere\\', \\'whom\\', \\'being\\', \\'aside\\', \\'happens\\', \\'specified\\', \\'aren\\', \\'wants\\', \\'ones\\', \\'something\\', \\'thats\\', \\'anyways\\', \\'placed\\', \\'mostly\\', \\'twice\\', \\'every\\', \\'been\\', \\'towards\\', \\'become\\', \\'novel\\', \\'else\\', \\'outside\\', \\'i\\', \\'yours\\', \\'gotten\\', \\'last\\', \\'hasn\\', \\'our\\', \\'already\\', \\'having\\', \\'somehow\\', \\'otherwise\\', \\'need\\', \\'wonder\\', \\'another\\', \\'regardless\\', \\'away\\', \\'near\\', \\'exactly\\', \\'me\\', \\'ain\\', \\'while\\', \\'before\\', \\'ignored\\', \\'now\\', \\'best\\', \\'knows\\', \\'saying\\', \\'usually\\', \\'became\\', \\'too\\', \\'value\\', \\'below\\', \"that\\'s\", \\'obviously\\', \\'contain\\', \\'containing\\', \\'latter\\', \\'third\\', \\'appropriate\\', \\'around\\', \\'beyond\\', \\'lately\\', \\'anywhere\\', \\'whoever\\', \\'keep\\', \\'whereas\\', \\'nothing\\', \\'specifying\\', \\'tell\\', \\'anyhow\\', \\'yes\\', \"a\\'s\", \\'tried\\', \\'thru\\', \"we\\'d\", \\'think\\', \\'without\\', \\'whereafter\\', \\'willing\\', \\'former\\', \\'each\\', \\'somewhat\\', \\'liked\\', \\'howbeit\\', \\'first\\', \\'nobody\\', \\'respectively\\', \\'tends\\', \\'little\\', \\'believe\\', \\'theres\\', \\'so\\', \\'look\\', \\'inasmuch\\', \\'we\\', \\'only\\', \\'course\\', \\'doing\\', \\'that\\', \\'someone\\', \\'very\\', \\'see\\', \\'meanwhile\\', \\'is\\', \\'for\\', \\'did\\', \\'least\\', \\'do\\', \"what\\'s\", \\'behind\\', \\'hello\\', \\'own\\', \\'must\\', \\'later\\', \\'off\\', \\'able\\', \\'nor\\', \\'many\\', \\'us\\', \\'formerly\\', \\'way\\', \\'contains\\', \\'new\\', \\'haven\\', \\'wherein\\', \\'seemed\\', \\'sometimes\\', \"we\\'re\", \\'shall\\', \\'etc\\', \\'how\\', \\'their\\', \\'could\\', \\'probably\\', \\'really\\', \\'together\\', \\'upon\\', \\'everything\\', \\'it\\', \\'sorry\\', \"let\\'s\", \\'various\\', \"it\\'s\", \\'myself\\', \\'doesn\\', \\'different\\', \\'these\\', \\'per\\', \\'despite\\', \\'not\\', \\'except\\', \\'mainly\\', \\'soon\\', \\'which\\', \\'from\\', \\'itself\\', \\'if\\', \\'hereafter\\', \\'they\\', \\'isn\\', \\'along\\', \\'better\\', \\'your\\', \\'followed\\', \\'use\\', \"it\\'d\", \\'inner\\', \\'be\\', \\'either\\', \\'furthermore\\', \\'or\\', \\'thereby\\', \\'currently\\', \\'indicates\\', \\'nearly\\', \\'via\\', \\'reasonably\\', \"they\\'ll\", \\'ours\\', \\'go\\', \\'tries\\', \\'self\\', \\'no\\', \\'comes\\', \\'because\\', \\'eg\\', \\'themselves\\', \\'thanks\\', \\'whether\\', \\'insofar\\', \"he\\'s\", \\'over\\', \\'never\\', \\'get\\', \\'qv\\', \\'about\\', \\'against\\', \\'theirs\\', \\'the\\', \\'had\\', \"we\\'ll\", \\'right\\', \\'didn\\', \\'non\\', \\'am\\', \\'come\\', \\'needs\\', \\'by\\', \\'perhaps\\', \\'certainly\\', \\'actually\\', \\'once\\', \\'sensible\\', \\'won\\', \\'thoroughly\\', \\'try\\', \\'c\\', \\'provides\\', \\'seems\\', \\'five\\', \\'vs\\', \"you\\'ve\", \"where\\'s\", \\'amongst\\', \\'than\\', \\'hadn\\', \\'done\\', \\'almost\\', \\'on\\', \\'during\\', \\'same\\', \\'out\\', \\'much\\', \"they\\'ve\", \\'allows\\', \\'hers\\', \\'ought\\', \\'cant\\', \\'maybe\\', \"\\'s\", \\'relatively\\', \\'entirely\\', \\'throughout\\', \\'few\\', \\'elsewhere\\', \\'thank\\', \\'thence\\', \\'ltd\\', \"t\\'s\", \\'hardly\\', \\'asking\\', \\'unlikely\\', \\'noone\\', \\'unto\\', \\'follows\\', \\'greetings\\', \\'namely\\', \\'less\\', \\'immediate\\', \\'rd\\', \\'associated\\', \\'two\\', \\'sent\\', \\'couldn\\', \\'useful\\', \\'specify\\', \"\\'d\", \\'himself\\', \\'well\\', \\'my\\', \\'secondly\\', \\'far\\', \\'everyone\\', \\'given\\', \\'this\\', \\'trying\\', \\'above\\', \\'instead\\', \\'when\\', \\'enough\\', \\'certain\\', \\'brief\\', \\'ourselves\\', \\'especially\\', \\'re\\', \\'ll\\', \\'her\\', \\'cause\\', \\'says\\', \\'com\\', \\'where\\', \\'t\\', \\'any\\', \\'ask\\', \\'there\\', \\'who\\', \\'described\\', \\'he\\', \\'particular\\', \\'although\\', \\'still\\', \\'beforehand\\', \\'particularly\\', \\'consequently\\', \\'appear\\', \\'should\\', \\'seven\\', \"you\\'re\", \\'considering\\', \\'rather\\', \\'to\\', \\'selves\\', \\'ex\\', \\'has\\', \\'th\\', \\'help\\', \\'thanx\\', \"they\\'re\", \\'used\\', \\'further\\', \"you\\'ll\", \\'hopefully\\', \\'seem\\', \\'his\\', \\'herself\\', \\'known\\', \\'becoming\\', \\'name\\', \\'beside\\', \\'example\\', \\'inc\\', \\'changes\\', \\'here\\', \\'toward\\', \\'will\\'} instead.').\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Projetos\\Python\\15 - Embeddings\\ap-de-maquina-embedding\\embeddings\\avaliacao_embedding.py\", line 26, in calcula_experimento_representacao\n",
      "    experimento.carrega_resultados_existentes()\n",
      "  File \"d:\\Projetos\\Python\\15 - Embeddings\\ap-de-maquina-embedding\\base_am\\avaliacao.py\", line 120, in carrega_resultados_existentes\n",
      "    resultados = pd.read_csv(\n",
      "  File \"C:\\Users\\Pablo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\", line 912, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"C:\\Users\\Pablo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\", line 577, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"C:\\Users\\Pablo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1407, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "  File \"C:\\Users\\Pablo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1661, in _make_engine\n",
      "    self.handles = get_handle(\n",
      "  File \"C:\\Users\\Pablo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\common.py\", line 859, in get_handle\n",
      "    handle = open(\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'resultados/random_forest_bow.csv'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Pablo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"d:\\Projetos\\Python\\15 - Embeddings\\ap-de-maquina-embedding\\base_am\\avaliacao.py\", line 176, in __call__\n",
      "    resultado = metodo.eval(\n",
      "  File \"d:\\Projetos\\Python\\15 - Embeddings\\ap-de-maquina-embedding\\base_am\\metodo.py\", line 24, in eval\n",
      "    df_preproc_treino = preproc_method.preprocess_train_dataset(df_treino, col_classe)\n",
      "  File \"d:\\Projetos\\Python\\15 - Embeddings\\ap-de-maquina-embedding\\base_am\\preprocessamento_atributos.py\", line 13, in preprocess_train_dataset\n",
      "    df_preprocessed_data = self.generate_preproc_train(df_data, class_col)\n",
      "  File \"d:\\Projetos\\Python\\15 - Embeddings\\ap-de-maquina-embedding\\embeddings\\textual_representation.py\", line 148, in generate_preproc_train\n",
      "    mat_bow = self.vectorizer.fit_transform(df_data[self.text_col])\n",
      "  File \"C:\\Users\\Pablo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\feature_extraction\\text.py\", line 2126, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\Pablo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Pablo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Pablo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'took', 'are', 'un', 'down', 'gets', 'wasn', 'yet', 'hence', 'according', 'corresponding', 'zero', 'others', 'yourselves', 'was', 'three', 'whose', 'lest', 'know', 'one', \"there's\", 'plus', 'seeming', 'like', 'whereby', 'possible', 'why', 'went', 'whence', 'appreciate', 'hereby', 'using', 'but', 'moreover', 'of', 'keeps', 'at', 'getting', 'forth', 'such', 'said', 'oh', 'sometime', 'ok', \"we've\", 'often', 'indicated', 'hi', 'thorough', 'whatever', 'among', 'up', 'came', 'indeed', 'et', 'under', 'those', 'don', 'neither', \"'ll\", 'edu', 'afterwards', 'as', 'causes', 'whither', 'seen', 'downwards', 'even', 'normally', 'anything', 'hither', 'would', \"here's\", 'que', 'some', 'anyone', 'none', 'taken', 'everybody', 'fifth', 'take', 'presumably', 'sure', 'with', 'anyway', 'somewhere', 'let', 'seriously', 'him', 'indicate', 'merely', 'accordingly', 'looks', 'gone', 'you', \"who's\", 'therefore', 'through', 'goes', 'say', 'apart', 'within', 'nine', 'until', 'besides', 'unfortunately', 'awfully', 'sub', 'ie', 'co', 'overall', 'whenever', 'most', 'into', 'onto', 'thus', 'likely', 'looking', 'them', 'an', 'herein', 'mean', 'uses', 'sup', 'old', 'after', 'gives', 'serious', 'wouldn', 'have', 'both', 'therein', 'somebody', 'hereupon', 'in', 'just', 'going', 'several', 'other', 'thereupon', 'okay', 'whereupon', 'anybody', 'kept', 'shouldn', 'may', 'again', 'want', 'always', 'clearly', 'also', 'nevertheless', 'following', 'inward', 'she', 'weren', 'latterly', 'might', 'truly', 'yourself', \"they'd\", 'nd', 'welcome', 'does', 'across', 'necessary', 'cannot', 'definitely', 'seeing', 'what', 'ever', 'were', 'all', 'thereafter', \"it'll\", 'then', \"'t\", 'becomes', 'six', 'its', 'regarding', 'and', 'however', 'unless', 'wish', 've', 'second', 'everywhere', 'four', 'whole', \"you'd\", 'regards', 'more', 'quite', 'next', 'please', 'between', 'got', 'though', 'consider', 'since', 'wherever', 'alone', 'saw', 'available', 'viz', 'can', 'allow', 'concerning', 'eight', 'nowhere', 'whom', 'being', 'aside', 'happens', 'specified', 'aren', 'wants', 'ones', 'something', 'thats', 'anyways', 'placed', 'mostly', 'twice', 'every', 'been', 'towards', 'become', 'novel', 'else', 'outside', 'i', 'yours', 'gotten', 'last', 'hasn', 'our', 'already', 'having', 'somehow', 'otherwise', 'need', 'wonder', 'another', 'regardless', 'away', 'near', 'exactly', 'me', 'ain', 'while', 'before', 'ignored', 'now', 'best', 'knows', 'saying', 'usually', 'became', 'too', 'value', 'below', \"that's\", 'obviously', 'contain', 'containing', 'latter', 'third', 'appropriate', 'around', 'beyond', 'lately', 'anywhere', 'whoever', 'keep', 'whereas', 'nothing', 'specifying', 'tell', 'anyhow', 'yes', \"a's\", 'tried', 'thru', \"we'd\", 'think', 'without', 'whereafter', 'willing', 'former', 'each', 'somewhat', 'liked', 'howbeit', 'first', 'nobody', 'respectively', 'tends', 'little', 'believe', 'theres', 'so', 'look', 'inasmuch', 'we', 'only', 'course', 'doing', 'that', 'someone', 'very', 'see', 'meanwhile', 'is', 'for', 'did', 'least', 'do', \"what's\", 'behind', 'hello', 'own', 'must', 'later', 'off', 'able', 'nor', 'many', 'us', 'formerly', 'way', 'contains', 'new', 'haven', 'wherein', 'seemed', 'sometimes', \"we're\", 'shall', 'etc', 'how', 'their', 'could', 'probably', 'really', 'together', 'upon', 'everything', 'it', 'sorry', \"let's\", 'various', \"it's\", 'myself', 'doesn', 'different', 'these', 'per', 'despite', 'not', 'except', 'mainly', 'soon', 'which', 'from', 'itself', 'if', 'hereafter', 'they', 'isn', 'along', 'better', 'your', 'followed', 'use', \"it'd\", 'inner', 'be', 'either', 'furthermore', 'or', 'thereby', 'currently', 'indicates', 'nearly', 'via', 'reasonably', \"they'll\", 'ours', 'go', 'tries', 'self', 'no', 'comes', 'because', 'eg', 'themselves', 'thanks', 'whether', 'insofar', \"he's\", 'over', 'never', 'get', 'qv', 'about', 'against', 'theirs', 'the', 'had', \"we'll\", 'right', 'didn', 'non', 'am', 'come', 'needs', 'by', 'perhaps', 'certainly', 'actually', 'once', 'sensible', 'won', 'thoroughly', 'try', 'c', 'provides', 'seems', 'five', 'vs', \"you've\", \"where's\", 'amongst', 'than', 'hadn', 'done', 'almost', 'on', 'during', 'same', 'out', 'much', \"they've\", 'allows', 'hers', 'ought', 'cant', 'maybe', \"'s\", 'relatively', 'entirely', 'throughout', 'few', 'elsewhere', 'thank', 'thence', 'ltd', \"t's\", 'hardly', 'asking', 'unlikely', 'noone', 'unto', 'follows', 'greetings', 'namely', 'less', 'immediate', 'rd', 'associated', 'two', 'sent', 'couldn', 'useful', 'specify', \"'d\", 'himself', 'well', 'my', 'secondly', 'far', 'everyone', 'given', 'this', 'trying', 'above', 'instead', 'when', 'enough', 'certain', 'brief', 'ourselves', 'especially', 're', 'll', 'her', 'cause', 'says', 'com', 'where', 't', 'any', 'ask', 'there', 'who', 'described', 'he', 'particular', 'although', 'still', 'beforehand', 'particularly', 'consequently', 'appear', 'should', 'seven', \"you're\", 'considering', 'rather', 'to', 'selves', 'ex', 'has', 'th', 'help', 'thanx', \"they're\", 'used', 'further', \"you'll\", 'hopefully', 'seem', 'his', 'herself', 'known', 'becoming', 'name', 'beside', 'example', 'inc', 'changes', 'here', 'toward', 'will'} instead.\n",
      "[W 2023-12-08 00:10:29,138] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'took', 'are', 'un', 'down', 'gets', 'wasn', 'yet', 'hence', 'according', 'corresponding', 'zero', 'others', 'yourselves', 'was', 'three', 'whose', 'lest', 'know', 'one', \"there's\", 'plus', 'seeming', 'like', 'whereby', 'possible', 'why', 'went', 'whence', 'appreciate', 'hereby', 'using', 'but', 'moreover', 'of', 'keeps', 'at', 'getting', 'forth', 'such', 'said', 'oh', 'sometime', 'ok', \"we've\", 'often', 'indicated', 'hi', 'thorough', 'whatever', 'among', 'up', 'came', 'indeed', 'et', 'under', 'those', 'don', 'neither', \"'ll\", 'edu', 'afterwards', 'as', 'causes', 'whither', 'seen', 'downwards', 'even', 'normally', 'anything', 'hither', 'would', \"here's\", 'que', 'some', 'anyone', 'none', 'taken', 'everybody', 'fifth', 'take', 'presumably', 'sure', 'with', 'anyway', 'somewhere', 'let', 'seriously', 'him', 'indicate', 'merely', 'accordingly', 'looks', 'gone', 'you', \"who's\", 'therefore', 'through', 'goes', 'say', 'apart', 'within', 'nine', 'until', 'besides', 'unfortunately', 'awfully', 'sub', 'ie', 'co', 'overall', 'whenever', 'most', 'into', 'onto', 'thus', 'likely', 'looking', 'them', 'an', 'herein', 'mean', 'uses', 'sup', 'old', 'after', 'gives', 'serious', 'wouldn', 'have', 'both', 'therein', 'somebody', 'hereupon', 'in', 'just', 'going', 'several', 'other', 'thereupon', 'okay', 'whereupon', 'anybody', 'kept', 'shouldn', 'may', 'again', 'want', 'always', 'clearly', 'also', 'nevertheless', 'following', 'inward', 'she', 'weren', 'latterly', 'might', 'truly', 'yourself', \"they'd\", 'nd', 'welcome', 'does', 'across', 'necessary', 'cannot', 'definitely', 'seeing', 'what', 'ever', 'were', 'all', 'thereafter', \"it'll\", 'then', \"'t\", 'becomes', 'six', 'its', 'regarding', 'and', 'however', 'unless', 'wish', 've', 'second', 'everywhere', 'four', 'whole', \"you'd\", 'regards', 'more', 'quite', 'next', 'please', 'between', 'got', 'though', 'consider', 'since', 'wherever', 'alone', 'saw', 'available', 'viz', 'can', 'allow', 'concerning', 'eight', 'nowhere', 'whom', 'being', 'aside', 'happens', 'specified', 'aren', 'wants', 'ones', 'something', 'thats', 'anyways', 'placed', 'mostly', 'twice', 'every', 'been', 'towards', 'become', 'novel', 'else', 'outside', 'i', 'yours', 'gotten', 'last', 'hasn', 'our', 'already', 'having', 'somehow', 'otherwise', 'need', 'wonder', 'another', 'regardless', 'away', 'near', 'exactly', 'me', 'ain', 'while', 'before', 'ignored', 'now', 'best', 'knows', 'saying', 'usually', 'became', 'too', 'value', 'below', \"that's\", 'obviously', 'contain', 'containing', 'latter', 'third', 'appropriate', 'around', 'beyond', 'lately', 'anywhere', 'whoever', 'keep', 'whereas', 'nothing', 'specifying', 'tell', 'anyhow', 'yes', \"a's\", 'tried', 'thru', \"we'd\", 'think', 'without', 'whereafter', 'willing', 'former', 'each', 'somewhat', 'liked', 'howbeit', 'first', 'nobody', 'respectively', 'tends', 'little', 'believe', 'theres', 'so', 'look', 'inasmuch', 'we', 'only', 'course', 'doing', 'that', 'someone', 'very', 'see', 'meanwhile', 'is', 'for', 'did', 'least', 'do', \"what's\", 'behind', 'hello', 'own', 'must', 'later', 'off', 'able', 'nor', 'many', 'us', 'formerly', 'way', 'contains', 'new', 'haven', 'wherein', 'seemed', 'sometimes', \"we're\", 'shall', 'etc', 'how', 'their', 'could', 'probably', 'really', 'together', 'upon', 'everything', 'it', 'sorry', \"let's\", 'various', \"it's\", 'myself', 'doesn', 'different', 'these', 'per', 'despite', 'not', 'except', 'mainly', 'soon', 'which', 'from', 'itself', 'if', 'hereafter', 'they', 'isn', 'along', 'better', 'your', 'followed', 'use', \"it'd\", 'inner', 'be', 'either', 'furthermore', 'or', 'thereby', 'currently', 'indicates', 'nearly', 'via', 'reasonably', \"they'll\", 'ours', 'go', 'tries', 'self', 'no', 'comes', 'because', 'eg', 'themselves', 'thanks', 'whether', 'insofar', \"he's\", 'over', 'never', 'get', 'qv', 'about', 'against', 'theirs', 'the', 'had', \"we'll\", 'right', 'didn', 'non', 'am', 'come', 'needs', 'by', 'perhaps', 'certainly', 'actually', 'once', 'sensible', 'won', 'thoroughly', 'try', 'c', 'provides', 'seems', 'five', 'vs', \"you've\", \"where's\", 'amongst', 'than', 'hadn', 'done', 'almost', 'on', 'during', 'same', 'out', 'much', \"they've\", 'allows', 'hers', 'ought', 'cant', 'maybe', \"'s\", 'relatively', 'entirely', 'throughout', 'few', 'elsewhere', 'thank', 'thence', 'ltd', \"t's\", 'hardly', 'asking', 'unlikely', 'noone', 'unto', 'follows', 'greetings', 'namely', 'less', 'immediate', 'rd', 'associated', 'two', 'sent', 'couldn', 'useful', 'specify', \"'d\", 'himself', 'well', 'my', 'secondly', 'far', 'everyone', 'given', 'this', 'trying', 'above', 'instead', 'when', 'enough', 'certain', 'brief', 'ourselves', 'especially', 're', 'll', 'her', 'cause', 'says', 'com', 'where', 't', 'any', 'ask', 'there', 'who', 'described', 'he', 'particular', 'although', 'still', 'beforehand', 'particularly', 'consequently', 'appear', 'should', 'seven', \"you're\", 'considering', 'rather', 'to', 'selves', 'ex', 'has', 'th', 'help', 'thanx', \"they're\", 'used', 'further', \"you'll\", 'hopefully', 'seem', 'his', 'herself', 'known', 'becoming', 'name', 'beside', 'example', 'inc', 'changes', 'here', 'toward', 'will'} instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32md:\\Projetos\\Python\\15 - Embeddings\\ap-de-maquina-embedding\\embeddings\\avaliacao_embedding.py:26\u001b[0m, in \u001b[0;36mcalcula_experimento_representacao\u001b[1;34m(nom_experimento, preproc_method, df_amostra, col_classe, num_folds, num_folds_validacao, num_trials, ClasseObjetivoOtimizacao, sampler)\u001b[0m\n\u001b[0;32m     22\u001b[0m     experimento \u001b[39m=\u001b[39m Experimento(nom_experimento, arr_folds, \n\u001b[0;32m     23\u001b[0m                         ClasseObjetivoOtimizacao\u001b[39m=\u001b[39mClasseObjetivoOtimizacao,\n\u001b[0;32m     24\u001b[0m                         num_trials\u001b[39m=\u001b[39mnum_trials, preproc_method\u001b[39m=\u001b[39mpreproc_method,\n\u001b[0;32m     25\u001b[0m                         sampler\u001b[39m=\u001b[39msampler, load_if_exists\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> 26\u001b[0m     experimento\u001b[39m.\u001b[39;49mcarrega_resultados_existentes()\n\u001b[0;32m     27\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIOError\u001b[39;00m: \n",
      "File \u001b[1;32md:\\Projetos\\Python\\15 - Embeddings\\ap-de-maquina-embedding\\base_am\\avaliacao.py:120\u001b[0m, in \u001b[0;36mExperimento.carrega_resultados_existentes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     resultados \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\n\u001b[0;32m    121\u001b[0m         \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mresultados/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnom_experimento\u001b[39m}\u001b[39;49;00m\u001b[39m.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, index_col\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m    122\u001b[0m     )\n\u001b[0;32m    123\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    857\u001b[0m \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m     \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m     handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    860\u001b[0m         handle,\n\u001b[0;32m    861\u001b[0m         ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    862\u001b[0m         encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    863\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    864\u001b[0m         newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    865\u001b[0m     )\n\u001b[0;32m    866\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m     \u001b[39m# Binary mode\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'resultados/random_forest_bow.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32md:\\Projetos\\Python\\15 - Embeddings\\ap-de-maquina-embedding\\Especificacao - Representacoes Textuais e Ap de Maquina.ipynb Célula 72\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/Python/15%20-%20Embeddings/ap-de-maquina-embedding/Especificacao%20-%20Representacoes%20Textuais%20e%20Ap%20de%20Maquina.ipynb#Y131sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m num_trials \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m  \u001b[39m# Com 100 estava demorando 30 minutos para 1 representação\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/Python/15%20-%20Embeddings/ap-de-maquina-embedding/Especificacao%20-%20Representacoes%20Textuais%20e%20Ap%20de%20Maquina.ipynb#Y131sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m nom_experimento \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetodo\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m representation\u001b[39m.\u001b[39mnome\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projetos/Python/15%20-%20Embeddings/ap-de-maquina-embedding/Especificacao%20-%20Representacoes%20Textuais%20e%20Ap%20de%20Maquina.ipynb#Y131sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m experimento \u001b[39m=\u001b[39m calcula_experimento_representacao(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/Python/15%20-%20Embeddings/ap-de-maquina-embedding/Especificacao%20-%20Representacoes%20Textuais%20e%20Ap%20de%20Maquina.ipynb#Y131sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     nom_experimento,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/Python/15%20-%20Embeddings/ap-de-maquina-embedding/Especificacao%20-%20Representacoes%20Textuais%20e%20Ap%20de%20Maquina.ipynb#Y131sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     representation,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/Python/15%20-%20Embeddings/ap-de-maquina-embedding/Especificacao%20-%20Representacoes%20Textuais%20e%20Ap%20de%20Maquina.ipynb#Y131sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     df_amazon_reviews,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/Python/15%20-%20Embeddings/ap-de-maquina-embedding/Especificacao%20-%20Representacoes%20Textuais%20e%20Ap%20de%20Maquina.ipynb#Y131sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     col_classe,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/Python/15%20-%20Embeddings/ap-de-maquina-embedding/Especificacao%20-%20Representacoes%20Textuais%20e%20Ap%20de%20Maquina.ipynb#Y131sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     num_folds,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/Python/15%20-%20Embeddings/ap-de-maquina-embedding/Especificacao%20-%20Representacoes%20Textuais%20e%20Ap%20de%20Maquina.ipynb#Y131sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     num_folds_validacao,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/Python/15%20-%20Embeddings/ap-de-maquina-embedding/Especificacao%20-%20Representacoes%20Textuais%20e%20Ap%20de%20Maquina.ipynb#Y131sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     num_trials,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/Python/15%20-%20Embeddings/ap-de-maquina-embedding/Especificacao%20-%20Representacoes%20Textuais%20e%20Ap%20de%20Maquina.ipynb#Y131sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     ClasseObjetivoOtimizacao\u001b[39m=\u001b[39;49mparam_metodo[\u001b[39m\"\u001b[39;49m\u001b[39mclasse_otimizacao\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/Python/15%20-%20Embeddings/ap-de-maquina-embedding/Especificacao%20-%20Representacoes%20Textuais%20e%20Ap%20de%20Maquina.ipynb#Y131sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     sampler\u001b[39m=\u001b[39;49mparam_metodo[\u001b[39m\"\u001b[39;49m\u001b[39msampler\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/Python/15%20-%20Embeddings/ap-de-maquina-embedding/Especificacao%20-%20Representacoes%20Textuais%20e%20Ap%20de%20Maquina.ipynb#Y131sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/Python/15%20-%20Embeddings/ap-de-maquina-embedding/Especificacao%20-%20Representacoes%20Textuais%20e%20Ap%20de%20Maquina.ipynb#Y131sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRepresentação: \u001b[39m\u001b[39m{\u001b[39;00mrepresentation\u001b[39m.\u001b[39mnome\u001b[39m}\u001b[39;00m\u001b[39m concluida\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Projetos\\Python\\15 - Embeddings\\ap-de-maquina-embedding\\embeddings\\avaliacao_embedding.py:28\u001b[0m, in \u001b[0;36mcalcula_experimento_representacao\u001b[1;34m(nom_experimento, preproc_method, df_amostra, col_classe, num_folds, num_folds_validacao, num_trials, ClasseObjetivoOtimizacao, sampler)\u001b[0m\n\u001b[0;32m     26\u001b[0m     experimento\u001b[39m.\u001b[39mcarrega_resultados_existentes()\n\u001b[0;32m     27\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIOError\u001b[39;00m: \n\u001b[1;32m---> 28\u001b[0m     experimento\u001b[39m.\u001b[39;49mcalcula_resultados()\n\u001b[0;32m     29\u001b[0m     experimento\u001b[39m.\u001b[39msalva_resultados()\n\u001b[0;32m     31\u001b[0m \u001b[39mreturn\u001b[39;00m experimento\n",
      "File \u001b[1;32md:\\Projetos\\Python\\15 - Embeddings\\ap-de-maquina-embedding\\base_am\\avaliacao.py:78\u001b[0m, in \u001b[0;36mExperimento.calcula_resultados\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     68\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(\n\u001b[0;32m     69\u001b[0m     study_name\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnom_experimento\u001b[39m}\u001b[39;00m\u001b[39m_fold_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     70\u001b[0m     sampler\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampler,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m     load_if_exists\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_if_exists,\n\u001b[0;32m     74\u001b[0m )\n\u001b[0;32m     75\u001b[0m objetivo_otimizacao \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mClasseObjetivoOtimizacao(\n\u001b[0;32m     76\u001b[0m     fold, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreproc_method\n\u001b[0;32m     77\u001b[0m )\n\u001b[1;32m---> 78\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objetivo_otimizacao, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_trials)\n\u001b[0;32m     79\u001b[0m \u001b[39m# obtem o melhor metodo da otimizacao\u001b[39;00m\n\u001b[0;32m     80\u001b[0m best_method \u001b[39m=\u001b[39m objetivo_otimizacao\u001b[39m.\u001b[39marr_evaluated_methods[\n\u001b[0;32m     81\u001b[0m     study\u001b[39m.\u001b[39mbest_trial\u001b[39m.\u001b[39mnumber\n\u001b[0;32m     82\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\optuna\\study\\study.py:442\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    349\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    350\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \n\u001b[0;32m    352\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     _optimize(\n\u001b[0;32m    443\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    444\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    445\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    446\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    447\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    448\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    449\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    450\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    451\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    452\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "File \u001b[1;32md:\\Projetos\\Python\\15 - Embeddings\\ap-de-maquina-embedding\\base_am\\avaliacao.py:176\u001b[0m, in \u001b[0;36mOtimizacaoObjetivo.__call__\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39marr_evaluated_methods\u001b[39m.\u001b[39mappend(metodo)\n\u001b[0;32m    175\u001b[0m \u001b[39mfor\u001b[39;00m fold_validacao \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfold\u001b[39m.\u001b[39marr_folds_validacao:\n\u001b[1;32m--> 176\u001b[0m     resultado \u001b[39m=\u001b[39m metodo\u001b[39m.\u001b[39;49meval(\n\u001b[0;32m    177\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreproc_method,\n\u001b[0;32m    178\u001b[0m         fold_validacao\u001b[39m.\u001b[39;49mdf_treino,\n\u001b[0;32m    179\u001b[0m         fold_validacao\u001b[39m.\u001b[39;49mdf_data_to_predict,\n\u001b[0;32m    180\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfold\u001b[39m.\u001b[39;49mcol_classe,\n\u001b[0;32m    181\u001b[0m     )\n\u001b[0;32m    182\u001b[0m     \u001b[39msum\u001b[39m \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresultado_metrica_otimizacao(resultado)\n\u001b[0;32m    184\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39m \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfold\u001b[39m.\u001b[39marr_folds_validacao)\n",
      "File \u001b[1;32md:\\Projetos\\Python\\15 - Embeddings\\ap-de-maquina-embedding\\base_am\\metodo.py:24\u001b[0m, in \u001b[0;36mScikitLearnAprendizadoDeMaquina.eval\u001b[1;34m(self, preproc_method, df_treino, df_data_to_predict, col_classe, seed)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meval\u001b[39m(\u001b[39mself\u001b[39m, preproc_method:PreprocessDataset, df_treino:pd\u001b[39m.\u001b[39mDataFrame, df_data_to_predict:pd\u001b[39m.\u001b[39mDataFrame, col_classe:\u001b[39mstr\u001b[39m, seed:\u001b[39mint\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Resultado:\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m     \u001b[39m#faz o preprocessamento\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     df_preproc_treino \u001b[39m=\u001b[39m preproc_method\u001b[39m.\u001b[39;49mpreprocess_train_dataset(df_treino, col_classe)\n\u001b[0;32m     25\u001b[0m     df_preproc_to_predict \u001b[39m=\u001b[39m preproc_method\u001b[39m.\u001b[39mpreprocess_test_dataset(df_data_to_predict, col_classe)\n\u001b[0;32m     27\u001b[0m     \u001b[39m#a partir de df_preproc_treino, separe os atributos  da classe\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projetos\\Python\\15 - Embeddings\\ap-de-maquina-embedding\\base_am\\preprocessamento_atributos.py:13\u001b[0m, in \u001b[0;36mPreprocessDataset.preprocess_train_dataset\u001b[1;34m(self, df_data, class_col)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_train_dataset\u001b[39m(\u001b[39mself\u001b[39m,df_data, class_col):\n\u001b[1;32m---> 13\u001b[0m     df_preprocessed_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_preproc_train(df_data, class_col)\n\u001b[0;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocessed_to_dataframe(df_data, df_preprocessed_data,class_col)\n",
      "File \u001b[1;32md:\\Projetos\\Python\\15 - Embeddings\\ap-de-maquina-embedding\\embeddings\\textual_representation.py:148\u001b[0m, in \u001b[0;36mBagOfWords.generate_preproc_train\u001b[1;34m(self, df_data, class_col)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_preproc_train\u001b[39m(\n\u001b[0;32m    146\u001b[0m     \u001b[39mself\u001b[39m, df_data: pd\u001b[39m.\u001b[39mDataFrame, class_col: \u001b[39mstr\u001b[39m\n\u001b[0;32m    147\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pd\u001b[39m.\u001b[39mDataFrame:\n\u001b[1;32m--> 148\u001b[0m     mat_bow \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvectorizer\u001b[39m.\u001b[39;49mfit_transform(df_data[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_col])\n\u001b[0;32m    149\u001b[0m     \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mDataFrame(\n\u001b[0;32m    150\u001b[0m         mat_bow\u001b[39m.\u001b[39mtoarray(),\n\u001b[0;32m    151\u001b[0m         columns\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectorizer\u001b[39m.\u001b[39mget_feature_names_out(),\n\u001b[0;32m    152\u001b[0m         index\u001b[39m=\u001b[39mdf_data\u001b[39m.\u001b[39mindex,\n\u001b[0;32m    153\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\feature_extraction\\text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2119\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2120\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2121\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2122\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2123\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2124\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2125\u001b[0m )\n\u001b[1;32m-> 2126\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2127\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2128\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py:1144\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m partial_fit_and_fitted \u001b[39m=\u001b[39m (\n\u001b[0;32m   1140\u001b[0m     fit_method\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpartial_fit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1141\u001b[0m )\n\u001b[0;32m   1143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m global_skip_validation \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1144\u001b[0m     estimator\u001b[39m.\u001b[39;49m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[0;32m   1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py:637\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_params\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    630\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \n\u001b[0;32m    632\u001b[0m \u001b[39m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[39m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 637\u001b[0m     validate_parameter_constraints(\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parameter_constraints,\n\u001b[0;32m    639\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_params(deep\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m    640\u001b[0m         caller_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m,\n\u001b[0;32m    641\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[39m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mstr\u001b[39m(c)\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39mconstraints[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\u001b[39m}\u001b[39;00m\u001b[39m or\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[39mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m{\u001b[39;00mparam_name\u001b[39m!r}\u001b[39;00m\u001b[39m parameter of \u001b[39m\u001b[39m{\u001b[39;00mcaller_name\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints_str\u001b[39m}\u001b[39;00m\u001b[39m. Got \u001b[39m\u001b[39m{\u001b[39;00mparam_val\u001b[39m!r}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'took', 'are', 'un', 'down', 'gets', 'wasn', 'yet', 'hence', 'according', 'corresponding', 'zero', 'others', 'yourselves', 'was', 'three', 'whose', 'lest', 'know', 'one', \"there's\", 'plus', 'seeming', 'like', 'whereby', 'possible', 'why', 'went', 'whence', 'appreciate', 'hereby', 'using', 'but', 'moreover', 'of', 'keeps', 'at', 'getting', 'forth', 'such', 'said', 'oh', 'sometime', 'ok', \"we've\", 'often', 'indicated', 'hi', 'thorough', 'whatever', 'among', 'up', 'came', 'indeed', 'et', 'under', 'those', 'don', 'neither', \"'ll\", 'edu', 'afterwards', 'as', 'causes', 'whither', 'seen', 'downwards', 'even', 'normally', 'anything', 'hither', 'would', \"here's\", 'que', 'some', 'anyone', 'none', 'taken', 'everybody', 'fifth', 'take', 'presumably', 'sure', 'with', 'anyway', 'somewhere', 'let', 'seriously', 'him', 'indicate', 'merely', 'accordingly', 'looks', 'gone', 'you', \"who's\", 'therefore', 'through', 'goes', 'say', 'apart', 'within', 'nine', 'until', 'besides', 'unfortunately', 'awfully', 'sub', 'ie', 'co', 'overall', 'whenever', 'most', 'into', 'onto', 'thus', 'likely', 'looking', 'them', 'an', 'herein', 'mean', 'uses', 'sup', 'old', 'after', 'gives', 'serious', 'wouldn', 'have', 'both', 'therein', 'somebody', 'hereupon', 'in', 'just', 'going', 'several', 'other', 'thereupon', 'okay', 'whereupon', 'anybody', 'kept', 'shouldn', 'may', 'again', 'want', 'always', 'clearly', 'also', 'nevertheless', 'following', 'inward', 'she', 'weren', 'latterly', 'might', 'truly', 'yourself', \"they'd\", 'nd', 'welcome', 'does', 'across', 'necessary', 'cannot', 'definitely', 'seeing', 'what', 'ever', 'were', 'all', 'thereafter', \"it'll\", 'then', \"'t\", 'becomes', 'six', 'its', 'regarding', 'and', 'however', 'unless', 'wish', 've', 'second', 'everywhere', 'four', 'whole', \"you'd\", 'regards', 'more', 'quite', 'next', 'please', 'between', 'got', 'though', 'consider', 'since', 'wherever', 'alone', 'saw', 'available', 'viz', 'can', 'allow', 'concerning', 'eight', 'nowhere', 'whom', 'being', 'aside', 'happens', 'specified', 'aren', 'wants', 'ones', 'something', 'thats', 'anyways', 'placed', 'mostly', 'twice', 'every', 'been', 'towards', 'become', 'novel', 'else', 'outside', 'i', 'yours', 'gotten', 'last', 'hasn', 'our', 'already', 'having', 'somehow', 'otherwise', 'need', 'wonder', 'another', 'regardless', 'away', 'near', 'exactly', 'me', 'ain', 'while', 'before', 'ignored', 'now', 'best', 'knows', 'saying', 'usually', 'became', 'too', 'value', 'below', \"that's\", 'obviously', 'contain', 'containing', 'latter', 'third', 'appropriate', 'around', 'beyond', 'lately', 'anywhere', 'whoever', 'keep', 'whereas', 'nothing', 'specifying', 'tell', 'anyhow', 'yes', \"a's\", 'tried', 'thru', \"we'd\", 'think', 'without', 'whereafter', 'willing', 'former', 'each', 'somewhat', 'liked', 'howbeit', 'first', 'nobody', 'respectively', 'tends', 'little', 'believe', 'theres', 'so', 'look', 'inasmuch', 'we', 'only', 'course', 'doing', 'that', 'someone', 'very', 'see', 'meanwhile', 'is', 'for', 'did', 'least', 'do', \"what's\", 'behind', 'hello', 'own', 'must', 'later', 'off', 'able', 'nor', 'many', 'us', 'formerly', 'way', 'contains', 'new', 'haven', 'wherein', 'seemed', 'sometimes', \"we're\", 'shall', 'etc', 'how', 'their', 'could', 'probably', 'really', 'together', 'upon', 'everything', 'it', 'sorry', \"let's\", 'various', \"it's\", 'myself', 'doesn', 'different', 'these', 'per', 'despite', 'not', 'except', 'mainly', 'soon', 'which', 'from', 'itself', 'if', 'hereafter', 'they', 'isn', 'along', 'better', 'your', 'followed', 'use', \"it'd\", 'inner', 'be', 'either', 'furthermore', 'or', 'thereby', 'currently', 'indicates', 'nearly', 'via', 'reasonably', \"they'll\", 'ours', 'go', 'tries', 'self', 'no', 'comes', 'because', 'eg', 'themselves', 'thanks', 'whether', 'insofar', \"he's\", 'over', 'never', 'get', 'qv', 'about', 'against', 'theirs', 'the', 'had', \"we'll\", 'right', 'didn', 'non', 'am', 'come', 'needs', 'by', 'perhaps', 'certainly', 'actually', 'once', 'sensible', 'won', 'thoroughly', 'try', 'c', 'provides', 'seems', 'five', 'vs', \"you've\", \"where's\", 'amongst', 'than', 'hadn', 'done', 'almost', 'on', 'during', 'same', 'out', 'much', \"they've\", 'allows', 'hers', 'ought', 'cant', 'maybe', \"'s\", 'relatively', 'entirely', 'throughout', 'few', 'elsewhere', 'thank', 'thence', 'ltd', \"t's\", 'hardly', 'asking', 'unlikely', 'noone', 'unto', 'follows', 'greetings', 'namely', 'less', 'immediate', 'rd', 'associated', 'two', 'sent', 'couldn', 'useful', 'specify', \"'d\", 'himself', 'well', 'my', 'secondly', 'far', 'everyone', 'given', 'this', 'trying', 'above', 'instead', 'when', 'enough', 'certain', 'brief', 'ourselves', 'especially', 're', 'll', 'her', 'cause', 'says', 'com', 'where', 't', 'any', 'ask', 'there', 'who', 'described', 'he', 'particular', 'although', 'still', 'beforehand', 'particularly', 'consequently', 'appear', 'should', 'seven', \"you're\", 'considering', 'rather', 'to', 'selves', 'ex', 'has', 'th', 'help', 'thanx', \"they're\", 'used', 'further', \"you'll\", 'hopefully', 'seem', 'his', 'herself', 'known', 'becoming', 'name', 'beside', 'example', 'inc', 'changes', 'here', 'toward', 'will'} instead."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "from embeddings.avaliacao_embedding import (\n",
    "    calcula_experimento_representacao,\n",
    "    OtimizacaoObjetivoRandomForest,\n",
    ")\n",
    "\n",
    "# Método de aprendizado de máquina a ser usado\n",
    "dict_metodo = {\n",
    "    \"random_forest\": {\n",
    "        \"classe_otimizacao\": OtimizacaoObjetivoRandomForest,\n",
    "        \"sampler\": optuna.samplers.TPESampler(seed=1, n_startup_trials=10),\n",
    "    },\n",
    "}\n",
    "df_amazon_reviews = pd.read_csv(\"datasets/amazon_reviews_mini.txt\", index_col=\"id\")\n",
    "\n",
    "# executa experimento com a representacao determinada e o método\n",
    "for metodo, param_metodo in dict_metodo.items():\n",
    "    for representation in arr_representations:\n",
    "        print(f\"===== Representação: {representation.nome}\")\n",
    "        col_classe = \"class\"\n",
    "        num_folds = 5\n",
    "        num_folds_validacao = 3\n",
    "        num_trials = 5  # Com 100 estava demorando 30 minutos para 1 representação\n",
    "\n",
    "        nom_experimento = f\"{metodo}_\" + representation.nome\n",
    "        experimento = calcula_experimento_representacao(\n",
    "            nom_experimento,\n",
    "            representation,\n",
    "            df_amazon_reviews,\n",
    "            col_classe,\n",
    "            num_folds,\n",
    "            num_folds_validacao,\n",
    "            num_trials,\n",
    "            ClasseObjetivoOtimizacao=param_metodo[\"classe_otimizacao\"],\n",
    "            sampler=param_metodo[\"sampler\"],\n",
    "        )\n",
    "        print(f\"Representação: {representation.nome} concluida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como a experimentação é uma tarefa custosa, todos os resultados são salvos na pasta \"resultados\" - inclusive os valores dos parametros na classe optuna (a prática de avaliação apresenta mais detalhes da biblioteca Optuna). A macro f1 é uma métrica relacionada a taxa de acerto (se necessário, [veja a explicação neste video - tópico 2 e 3)](https://www.youtube.com/watch?v=u7o7CSeXaNs&list=PLwIaU1DGYV6tUx10fCTw5aPnqypbbK_GJ&index=13). Analise os resultados abaixo: qual representação foi melhor? A restrição de vocabulário ou eliminação de stopwords auxiliou? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-08 00:10:37,628] Using an existing study with name 'random_forest_embbeding_fold_0' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:37,691] Using an existing study with name 'random_forest_embbeding_fold_1' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:37,756] Using an existing study with name 'random_forest_embbeding_fold_2' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:37,803] Using an existing study with name 'random_forest_embbeding_fold_3' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:37,851] Using an existing study with name 'random_forest_embbeding_fold_4' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:37,901] Using an existing study with name 'random_forest_emb_keywords_exp_fold_0' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:37,950] Using an existing study with name 'random_forest_emb_keywords_exp_fold_1' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:38,005] Using an existing study with name 'random_forest_emb_keywords_exp_fold_2' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:38,066] Using an existing study with name 'random_forest_emb_keywords_exp_fold_3' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:38,121] Using an existing study with name 'random_forest_emb_keywords_exp_fold_4' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:38,176] Using an existing study with name 'random_forest_emb_nostop_fold_0' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:38,223] Using an existing study with name 'random_forest_emb_nostop_fold_1' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:38,270] Using an existing study with name 'random_forest_emb_nostop_fold_2' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:38,324] Using an existing study with name 'random_forest_emb_nostop_fold_3' instead of creating a new one.\n",
      "[I 2023-12-08 00:10:38,385] Using an existing study with name 'random_forest_emb_nostop_fold_4' instead of creating a new one.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nom_experimento</th>\n",
       "      <th>macro-f1</th>\n",
       "      <th>f1-positive</th>\n",
       "      <th>precision-positive</th>\n",
       "      <th>recall-positive</th>\n",
       "      <th>f1-negative</th>\n",
       "      <th>precision-negative</th>\n",
       "      <th>recall-negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>random_forest_embbeding</td>\n",
       "      <td>0.722678</td>\n",
       "      <td>0.718133</td>\n",
       "      <td>0.725953</td>\n",
       "      <td>0.710629</td>\n",
       "      <td>0.727223</td>\n",
       "      <td>0.71986</td>\n",
       "      <td>0.734882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>random_forest_emb_keywords_exp</td>\n",
       "      <td>0.686080</td>\n",
       "      <td>0.677361</td>\n",
       "      <td>0.694003</td>\n",
       "      <td>0.662761</td>\n",
       "      <td>0.694799</td>\n",
       "      <td>0.68059</td>\n",
       "      <td>0.710886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest_emb_nostop</td>\n",
       "      <td>0.716860</td>\n",
       "      <td>0.712571</td>\n",
       "      <td>0.719894</td>\n",
       "      <td>0.706562</td>\n",
       "      <td>0.721150</td>\n",
       "      <td>0.71572</td>\n",
       "      <td>0.727707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  nom_experimento  macro-f1  f1-positive  precision-positive  \\\n",
       "0         random_forest_embbeding  0.722678     0.718133            0.725953   \n",
       "1  random_forest_emb_keywords_exp  0.686080     0.677361            0.694003   \n",
       "2        random_forest_emb_nostop  0.716860     0.712571            0.719894   \n",
       "\n",
       "   recall-positive  f1-negative  precision-negative  recall-negative  \n",
       "0         0.710629     0.727223             0.71986         0.734882  \n",
       "1         0.662761     0.694799             0.68059         0.710886  \n",
       "2         0.706562     0.721150             0.71572         0.727707  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from base_am.avaliacao import Experimento\n",
    "\n",
    "arr_resultado = []\n",
    "results_folder = \"resultados\"\n",
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)\n",
    "for resultado_csv in os.listdir(\"resultados\"):\n",
    "    if resultado_csv.endswith(\"csv\"):\n",
    "        nom_experimento = resultado_csv.split(\".\")[0]\n",
    "        \n",
    "        #carrega resultados previamente realizados\n",
    "        experimento = Experimento(nom_experimento,[])\n",
    "        experimento.carrega_resultados_existentes()\n",
    "        \n",
    "        #adiciona experimento\n",
    "        num_folds = len(experimento.resultados)\n",
    "        dict_resultados = {\"nom_experimento\":nom_experimento, \n",
    "                            \"macro-f1\":sum([r.macro_f1 for r in experimento.resultados])/num_folds}\n",
    "        #resultados por classe\n",
    "        for classe in experimento.resultados[0].mat_confusao.keys():\n",
    "\n",
    "            dict_resultados[f\"f1-{classe}\"] = sum([r.f1_por_classe[classe] for r in experimento.resultados])/num_folds\n",
    "            dict_resultados[f\"precision-{classe}\"] = sum([r.precisao[classe] for r in experimento.resultados])/num_folds\n",
    "            dict_resultados[f\"recall-{classe}\"] = sum([r.revocacao[classe] for r in experimento.resultados])/num_folds\n",
    "\n",
    "        arr_resultado.append(dict_resultados)\n",
    "\n",
    "pd.DataFrame.from_dict(arr_resultado) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTADO\n",
    "\n",
    "Ao examinar as métricas, especialmente a macro-F1, observamos que as mudanças realizadas não fizeram muita diferença. Por fim, infelizmente não foi possível saber os resultados do ramdom_forest_bow, portanto não saberemos os resultados que iriamos obter ao remover instancias de palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bolukbasi, T., Chang, K. W., Zou, J., Saligrama, V., & Kalai, A. (2016). **[Man is to computer programmer as woman is to homemaker? Debiasing word embeddings](https://arxiv.org/abs/1607.06520)**. \n",
    "\n",
    "Hartmann, N., Fonseca, E., Shulby, C., Treviso, M., Rodrigues, J., & Aluisio, S. (2017). [**Portuguese word embeddings: Evaluating on word analogies and natural language tasks.**](https://arxiv.org/abs/1708.06025)\n",
    "\n",
    "\n",
    "Pennington, J., Socher, R., & Manning, C. D. (2014, October).**[GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)**. In EMNLP 2015 \n",
    "\n",
    "\n",
    "Scherer, Klaus R. **[What are emotions? And how can they be measured?](https://journals.sagepub.com/doi/pdf/10.1177/0539018405058216)**. Social science information, v. 44, n. 4, p. 695-729, 2005.\n",
    "\n",
    "Shen, D., Wang, G., Wang, W., Min, M. R., Su, Q., Zhang, Y., Carin, L. (2018). [Baseline needs more love: On simple word-embedding-based models and associated pooling mechanisms](https://arxiv.org/pdf/1805.09843.pdf).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Licença Creative Commons\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br />Este obra está licenciado com uma Licença <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Atribuição-CompartilhaIgual 4.0 Internacional</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
